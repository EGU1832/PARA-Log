기초머신러닝

# 0. Syllabus

AI를 전공하기 위해 반드시 알아야 할 Math Concept들은 다음과 같다.
- Linear Algebra ★
- Analytic Geometry ★
- Matrix Decomposition ★
- Vector Calculus
- Probability and Distributions
- Optimization

그리고 기초적인 ML Problem들은 다음과 같다.
- When Models Meet Data `Linear Mapping`
- Dimensionality Reductds3fion with Principal Component Analysis `차원축소 with PCA`
- Density Estimation with Gaussian Mixture Models `분포 예측 with GMM`
- Claasification with Support Vector Machines `분류 with SVM`

본 강의에서 배우는 ML의 기본 토대 4가지는 다음과 같다.
- Regression
- Dimensionality Reduction
- Density Estimation
- Classification

따라서 강의자료로만 공부하면 부족할수도 있으니, 교재도 보기를 바란다.
`Mathematics for Machine Learning1, Cambridge University Press, Marc Peter`

# 1. Introduction

Notation
- Sets: $\mathcal{A, B, C}$

머신러닝의 알고리즘은 일반 알고리즘들과 접근방식이 다소 다르다.
머신러닝의 알고리즘은 주어진 Data로부터 Valuable Information을 추출하는 것이 주요 목적이다.

ML의 주요 3가지 Concept는 다음과 같다.
1. Data: Data를 생성해내는 어떤 Process에 관한것
2. A Model: ML에 필요한 중요한 특징 중 하나는 내가 가지고 있던 Data Set 뿐만이 아니라 Unseened Data로부터 유의미한 정보값을 추출해낼 수 있는 것이다. Application에 따라 이 Unseen Data의 범위를 정하는 것 또한 중요하다고 할 수 있겠다.
3. Learning: Data에 있는 패턴과 구조를 자동적으로 이해할 수 있게끔 모델의 Parameter를 최적화하는 것이다.
항상 이 3가지가 동등하게 중요하다는 것을 앞으로 ML을 공부하며 잊으면 안될 것이다.

어느 학문이나 그렇겠지만, 근본적인 이해를 위해선 수학적인 Background가 중요하다.

ML에서 알고리즘이란 무엇이냐?
- 예를들어 "ML Algorithm"이라 얘기할 때 Input Data를 기반으로 예측값을 뽑아내는 시스템을 가리킬 때가 있다. 즉, 우리가 생각하는 알고리즘과 조금 상이하다.
- Training Proces를 의미할 때도 있다.
그럼 뭐가 뭔지 어떻게 판단하냐? Context상으로 이해할 수 밖에 없다.

#### Data

본 강의에서, 우리는 Data를 컴퓨터에 쉬이 학습될 수 있도록 이미 수치적으로 표현된 **벡터**라고 생각할 것이다.
```
Data == vector
```

그렇다면 **벡터**란 무엇이라고 생각하면 되느냐?
- CSE View: 숫자의 배열
- PHY View: 방향과 크기를 가지는 화살표
- MAT View: 덧셈과 Scaling을 따르는 객체
그때 그때 Context에 따라 알아서 잘 이해하면된다.

#### Model

좋은 모델이란,
- 실제 사고방식을 얼마나 잘 단순화하여 모방했느냐에 따라 좌우된다.
- 또한 사전학습 없이 Real-World Experiments를 얼마나 잘 예측하는지

#### Learning

Training이란, Model의 Parameter들을 최적화하는 것이다.
- 이는 모델이 Training Data에 대하여 얼마나 잘 예측하는지를 평가하는 Utility 함수를 기반으로 이루어진다.
- 계속 반복해서 말하지만, **Training Data가 아닌 Unseened Data에 대하여 좋은 성능을 내는 것**이 중요하다.

# Part I: Math

# 2. Linear Algebra

## 2-1. Systems of Linear Equations

## 2-2. Matrices

$a_{\text{ Row Index | Column Index }}$

$\mathbf{Ax} = \mathbf{b}$
Column Vector: $\begin{pmatrix} a_{11}\\\vdots\\a_{m1} \end{pmatrix} \mathbf{x_1}$

Column - 세로, Row - 가로

Reshape: $\mathbb{R}^{m\times n} \rightarrow \mathbb{R}^{mn}$

$\begin{pmatrix}1 & 2 & 3 \\ 3 & 2 & 1\end{pmatrix} \times \begin{pmatrix}0 & 2 \\ 1 & -1 \\ 0 & 1\end{pmatrix} = \begin{pmatrix}2&2\\3&5\end{pmatrix}$

Identity Matrix: $\mathbf{I}_n$, 문맥 상 크기가 확실하면 굳이 n을 표기할 필요는 없다.

Matrix Properties
- 결합법칙: $\mathbf{(AB)C} = \mathbf{A(BC)}$
- 분배법칙: $\mathbf{(A+B)C} = \mathbf{AC + BC} \neq \mathbf{CA + CB}$
- 항등원: $\mathbf{I}_m\mathbf{A} = \mathbf{A}\mathbf{I}_n = \mathbf{A}$

Inverse $\mathbf{AA}^{-1} = \mathbf{I}_n = \mathbf{A}^{-1}\mathbf{A}$: **Regular**/Inbertible/**Nonsingular**
- Unique ★
- Matrix $\mathbf{A}$가 $2\times 2$면 $\mathbf{A}^{-1} = \frac{1}{a_{11}a_{22} - a_{12}a_{21}}\begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11}\end{pmatrix} = \frac{1}{\det}\begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11}\end{pmatrix}$
Transpose
- $\mathbf{A} = \mathbf{A}^{\top}$이면 **Symmetric**이라 한다.

More Matrix Properties `막 그렇게 당연하지는 않은 것들`
- $(\mathbf{A} + \mathbf{B})^{-1} \neq \mathbf{A}^{-1} + \mathbf{B}^{-1}$
- $\mathbf{A}^{-\top} = (\mathbf{A}^{-1})^{\top} = (\mathbf{A}^{\top})^{-1}$

Matrix의 Scalar Multiplication을 할 때 $\lambda \times \mathbf{A}$처럼 $\times$를 쓰기도 한다.
이건 그냥 직관적으로 결합법칙이나 분배법칙을 사용해도 무관하다.

## 2-3. Solving Systems of Linear Equations

식을 세우고, Gaussian Elimination등을 통해 해를 구하면 된다.

General Solution
![250](../../z.%20Docs/img/Pasted%20image%2020250313093530.png)
- z를 어떻게 Sampling 했느냐에 따라 결과가 다양해진다.

General Solution = Particular Solution + Homogeneous Solution
![350](../../z.%20Docs/img/Pasted%20image%2020250313093709.png)

ML은 진짜로 $\mathbf{Ax = b}$를 푸는 게 다이다.
- 이 Equation을 푸는 것을 컴퓨터 알고리즘으로 만들기 위해 Gauss-Jordan Method등을 정의하는 것이다.

Row-Echelon Form, Reduced Row-Echelon Form
![300](https://dmn92m25mtw4z.cloudfront.net/img_set/la-1-4-x-eq-8/v1/la-1-4-x-eq-8-460w.jpg)
- Pivot은 언제나 그 아래 행의 Pivot의 왼쪽에 있어야 한다.

Calculating Inverse

$$
\begin{pmatrix}
1 & 0 & 2 & 0 & | & 1 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & | & 0 & 1 & 0 & 0\\
1 & 2 & 0 & 1 & | & 0 & 0 & 1 & 0\\
1 & 1 & 1 & 1 & | & 0 & 0 & 0 & 1
\end{pmatrix}
$$
$$
\begin{pmatrix}
-1 & 2 & -2 & 2 \\
? & ? & ? & ? \\
? & ? & ? & ? \\
? & ? & ? & ? \\
\end{pmatrix}
$$

Algorithms for Solving System of Linear Equations

(1) **Pseudo-Inverse** $(\mathbf{A}^{\top}\mathbf{A})^{-1}\mathbf{A}^{\top}$
이게 문제가 뭘까?
Inverse가 들어가있기 때문에 연산량이 어마어마해서 실제로 쓰기가 너무 어렵다는 점이다.
따라서 좋아보이긴 하지만, 실제로 쓰기는 어렵다는 점.

(2) Gaussian Elimination
- 바로 한 번에 구해지지 않기 때문에 Intuitibe and Constructive 방법이라고 한다.
- Cuic Complexity

(3) Optimization
- Gradient se

## 3-4. Vector Spaces

**Group**
- Set 안에서 임의로 뽑은 $x, y$에 대해서 $x \otimes y$가 $\mathcal{G}$ 에 속해야 한다.
- Set 안에서 임의로 뽑은 $x, y$에 대해서 연산 순서가 결과에 영향을 미치면 안된다.
- 항등원이 존재해야한다.
- 역원이 존재해야한다.

**Abelian Group**
- Set $\mathcal{G}$에서 연산자 $\otimes$에 대해서 교환법칙이 성립하면 Abelian Group이라고 한다.

어떤 게 Group, Abelian Group을 만족하는지 판단할 수 있어야 한다.

**Vector Spaces**
- $Def.\ \mathbf{V} = (\mathcal{V}, +, \cdot)$
- $(\mathcal{V}, +)$은 Abelian Group이다.
- 분배법칙, 결합법칙, 항등원이 성립한다.

**Vector Subspaces** `= Linear Subspace = Subspace`
- Vector Space $\mathbf{V}$에 대해서 $\mathcal{U} \subset \mathcal{V}$일때, $\mathbf{U} = (\mathcal{U}, +, \cdot)$ 즉 $\mathbf{U}$ 자체도 Vector Space여야 한다.
- e.g.
	- Vector Spave $\mathbf{V}$에 대하여 $\mathbf{V}$와 $\{0\}$도 Subspace이다.
	- $\mathbf{Ax} = 0$ ($\mathbf{Ax = b}$는 아니다!)
- 임의의 많은 Subspaces의 교차점은 Subspace 자체이다.

## 2-5. Linear Independence

**Linear Independence**
- Linear Combination $\mathbf{v} = \lambda_1\mathbf{x}_1 + \cdots + \lambda_k\mathbf{x}_k$
- $Def.$ $\sum^{k}_{i = 1}\lambda_i\mathbf{x}_i = 0$의 Solution이 $\lambda_1 = \cdots = \lambda_k = 0$ 밖에 없을 때
- Vectors가 Lineary dependent할 때, 적어도 하나의 vector는 다른 vector들의 Linear Combination이다.
- Linear Independence 검사: Gaussian Elimination으로 정리 했을 때 모든 Column Vector들이 Pivot Column이면

(Q) k개의 Linearly Independent Vectors $\mathbf{b_k}$로 m개의 Linear Combination Vectors $\mathbf{x_m}$를 만들었을 때, 이는 Linearly Independent한가?
	$\mathbf{x}_j = \mathbf{B}\lambda_j$로 표현 가능하다. `자세한 것은 강의자료 40p. 참조`
	(A) Yes.

(Q) $\mathbf{x}_1$과 $\mathbf{x}_4$는 Linearly Dependent한가?
$$
\begin{pmatrix}
1 & -4 & 2 & 17 \\
-2 & -2 & 3 & -10 \\
1 & 0 & -1 & 11 \\
-1 & 4 & -3 & 1
\end{pmatrix}
\rightarrow
\begin{pmatrix}
1 & 0 & 0 & -7 \\
0 & 1 & 0 & -15 \\
0 & 0 & 1 & 18 \\
0 & 0 & 0 & 0
\end{pmatrix}
\therefore \text{dependence}
$$
## 3-6. Basis and Rank

**Generating Set**
- $Def.$ $\mathbf{v}\in \mathcal{V}$의 모든 $\mathbf{v}$를 $\mathcal{A} = \{\mathbf{x}_1 , \cdots, \mathbf{x}_k\}\subset \mathcal{V}$로 표현 가능할 때, $\mathcal{A}$를 $\mathbf{V}$의 Generating Set이라고 한다.
- **Span**: $\mathcal{A}$의 모든 Linear Combination의 Set. Notation은 $\mathbf{V} = \text{span}[\mathcal{A}]$이다.

**Basis**
- $Def.$ Minimal Generating Set $\mathcal{B}$
- **Dimension**: Basis Vector의 개수
- $\mathbf{x}\in\mathbf{V}$인 모든 Vector는 Unique한 $\mathcal{B}$의 Linear Combination으로 나타낼 수 있다.
- Standard Basis e.g. $(0\ 1),\ (1\ 0)$

`정의되는 순서: Generating Set -> Span -> Basis -> Dimension`

(Q) `45p.` 두 번째
	$\mathbb{R}^4$를 위한 Basis를 나타내고 있기 때문에 3개의 Vector로는 4차원 공간을 만들 수 없기 때문에 Basis가 아니다.

Determining a Basis
1. Matrix $\mathbf{A} = (\mathbf{x}_1\ \mathbf{x}_2\ \cdots\ \mathbf{x}_m)$을 만든다.
2. $\mathbf{A}$의 Row-Echelon Form을 찾는다.
3. Pivot Column들을 모든다.
`생각하면 쉽다.`

Example 2.17


**Rank**
- $Def.$ $rk(\mathbf{A})$Linearly Independent한 Column의 개수
- e.g.의 $\begin{pmatrix}1 \\ 3 \\ 0\end{pmatrix}$은 쓸모가 없다는 것이다. 있으나 없으나 Column들로 Span할 수 있는 Space는 같다. `이는 ML 모델의 경량화로 이어진다.`
- $rk(\mathbf{A}) = rk(\mathbf{A}^{\top})$
- $\mathbf{A}\in \mathbb{R}^{n\times n},\ rk(\mathbf{A}) = n \iff$ $\mathbf{A}$가 invertible하다.
- $\mathbf{Ax} = \mathbf{b}$가 solvable $\iff\ rk(\mathbf{A} = rk(\mathbf{A|b}))$
- `필기 필요`
- Full Rank $= \min(\text{\# of cols}, \text{\# of rows})$

## 2-7. Linear Mappings

어떤 Vector Spave의 구조가 유지되는 Mapping을 알아보자.

**Linear Mapping**
- $Def.$ $\Phi: \mathbf{V}\mapsto\mathbf{W}$
- 다음과 같은 성질을 만족하면 Linear Transformation이라고 얘기할 수 있다.
	- $\Phi(\mathbf{x + y}) = \Phi(\mathbf{x}) + \Phi(\mathbf{y})$
	- $\Phi(\lambda\mathbf{x}) = \lambda\Phi(\mathbf{x})$
- 어떻게 Mapping 되느냐에 따라 다음과 같이 정의 할 수 있다.
	![300](https://media.geeksforgeeks.org/wp-content/uploads/20231016183533/Injective-Function-5.png)
	- Injective(단사): $if\ \forall \mathbf{x, y}\in \mathcal{V},\ \Phi(\mathbf{x}) = \Phi(\mathbf{y}) \Rightarrow \mathbf{x = y}$
	- Surgective(전사): $if\ \Phi(\mathcal{V}) = \mathcal{W}$
	- Bijective(전단사): Injective and Surjective

Example 2.19 (Homomorphism)
	위에서 언급한 성질을 이용해 Linear Transformation인지 확인
	`자세한 것은 Silent Camera 참고`

- Bijective Mapping에서는 Inverse Mapping $\Phi^{-1}$이 성립한다.
- Isomorphism: $if \Psi$가 Linear 하고 Bijective 할때

- $Theorem.$ Dimension이 같은 Vector Space는 같은 공간이다.
★
> Linear Mapping $\Phi$과 $\Psi$에 대하여, $\Phi \odot \Psi?$또한 Linear Mapping이다.
> 즉, Linear Mapping을 중첩시킬 수 있다는 뜻이며 이는 ML의 근간을 이루는 개념이다.

`추가 정리 필요`

**Coordinates**
- Cartesian Coordinates(데카르트 좌표계): 흔히들 우리가 아는 좌표계
	![150](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Cartesian-coordinate-system.svg/1200px-Cartesian-coordinate-system.svg.png)
- Coordinate는 Basis에 따라서 바뀐다. 무슨 이야기냐면 Basis가 바뀌면 다른 Unique한 Linear Combination으로 표현된다는 것이다.
	- $\mathbf{x} = \alpha_1\mathbf{b}_1 + \cdots + \alpha_n\mathbf{b}_n,\ Basis\ B = (\mathbf{b}_1, \cdots, \mathbf{b}_n)$
	- 이때 $\alpha = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix}$가 바로 Coordinate이다.
- `추가 정리 필요 (빠..빨라요 교수님)`

**Basis Change**
- $\mathbf{y} = \mathbf{B}'^{-1}\mathbf{bx}$
- `추가 정리 필요`

Example
$\mathbf{y} = \mathbf{B}'^{-1}\mathbf{bx}$를 이용해 풀면 된다. 자세한 것은 pdf 참고

**Transformation Matrix**
- 모든 Basis Vector에 대해서 Transformation을 나타내면 Matrix Multiplication의 형태로 나타낼 수 있다.
- $\hat{x}$는 $\mathbf{b}$로 표현된 좌표고, $\hat{y}$는 $\mathbf{c}$로 표현된 좌표라고 생각하면된다.

그럼 Matrix $\mathbf{A}_\Phi$의 Element의 각 요소는 어떻게 찾으면 될까?

# 3. Analytic Geometry

