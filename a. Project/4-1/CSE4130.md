기초머신러닝

# 0. Syllabus

AI를 전공하기 위해 반드시 알아야 할 Math Concept들은 다음과 같다.
- Linear Algebra ★
- Analytic Geometry ★
- Matrix Decomposition ★
- Vector Calculus
- Probability and Distributions
- Optimization

그리고 기초적인 ML Problem들은 다음과 같다.
- When Models Meet Data `Linear Mapping`
- Dimensionality Reductds3fion with Principal Component Analysis `차원축소 with PCA`
- Density Estimation with Gaussian Mixture Models `분포 예측 with GMM`
- Claasification with Support Vector Machines `분류 with SVM`

본 강의에서 배우는 ML의 기본 토대 4가지는 다음과 같다.
- Regression
- Dimensionality Reduction
- Density Estimation
- Classification

따라서 강의자료로만 공부하면 부족할수도 있으니, 교재도 보기를 바란다.
`Mathematics for Machine Learning1, Cambridge University Press, Marc Peter`

# 1. Introduction

Notation
- Sets: $\mathcal{A, B, C}$

머신러닝의 알고리즘은 일반 알고리즘들과 접근방식이 다소 다르다.
머신러닝의 알고리즘은 주어진 Data로부터 Valuable Information을 추출하는 것이 주요 목적이다.

ML의 주요 3가지 Concept는 다음과 같다.
1. Data: Data를 생성해내는 어떤 Process에 관한것
2. A Model: ML에 필요한 중요한 특징 중 하나는 내가 가지고 있던 Data Set 뿐만이 아니라 Unseened Data로부터 유의미한 정보값을 추출해낼 수 있는 것이다. Application에 따라 이 Unseen Data의 범위를 정하는 것 또한 중요하다고 할 수 있겠다.
3. Learning: Data에 있는 패턴과 구조를 자동적으로 이해할 수 있게끔 모델의 Parameter를 최적화하는 것이다.
항상 이 3가지가 동등하게 중요하다는 것을 앞으로 ML을 공부하며 잊으면 안될 것이다.

어느 학문이나 그렇겠지만, 근본적인 이해를 위해선 수학적인 Background가 중요하다.

ML에서 알고리즘이란 무엇이냐?
- 예를들어 "ML Algorithm"이라 얘기할 때 Input Data를 기반으로 예측값을 뽑아내는 시스템을 가리킬 때가 있다. 즉, 우리가 생각하는 알고리즘과 조금 상이하다.
- Training Proces를 의미할 때도 있다.
그럼 뭐가 뭔지 어떻게 판단하냐? Context상으로 이해할 수 밖에 없다.

#### Data

본 강의에서, 우리는 Data를 컴퓨터에 쉬이 학습될 수 있도록 이미 수치적으로 표현된 **벡터**라고 생각할 것이다.
```
Data == vector
```

그렇다면 **벡터**란 무엇이라고 생각하면 되느냐?
- CSE View: 숫자의 배열
- PHY View: 방향과 크기를 가지는 화살표
- MAT View: 덧셈과 Scaling을 따르는 객체
그때 그때 Context에 따라 알아서 잘 이해하면된다.

#### Model

좋은 모델이란,
- 실제 사고방식을 얼마나 잘 단순화하여 모방했느냐에 따라 좌우된다.
- 또한 사전학습 없이 Real-World Experiments를 얼마나 잘 예측하는지

#### Learning

Training이란, Model의 Parameter들을 최적화하는 것이다.
- 이는 모델이 Training Data에 대하여 얼마나 잘 예측하는지를 평가하는 Utility 함수를 기반으로 이루어진다.
- 계속 반복해서 말하지만, **Training Data가 아닌 Unseened Data에 대하여 좋은 성능을 내는 것**이 중요하다.


# 2. Linear Algebra

## 2-1. Systems of Linear Equations

## 2-2. Matrices

$a_{\text{ Row Index | Column Index }}$

$\mathbf{Ax} = \mathbf{b}$
Column Vector: $\begin{pmatrix} a_{11}\\\vdots\\a_{m1} \end{pmatrix} \mathbf{x_1}$

Column - 세로, Row - 가로

Reshape: $\mathbb{R}^{m\times n} \rightarrow \mathbb{R}^{mn}$

$\begin{pmatrix}1 & 2 & 3 \\ 3 & 2 & 1\end{pmatrix} \times \begin{pmatrix}0 & 2 \\ 1 & -1 \\ 0 & 1\end{pmatrix} = \begin{pmatrix}2&2\\3&5\end{pmatrix}$

Identity Matrix: $\mathbf{I}_n$, 문맥 상 크기가 확실하면 굳이 n을 표기할 필요는 없다.

Matrix Properties
- 결합법칙: $\mathbf{(AB)C} = \mathbf{A(BC)}$
- 분배법칙: $\mathbf{(A+B)C} = \mathbf{AC + BC} \neq \mathbf{CA + CB}$
- 항등원: $\mathbf{I}_m\mathbf{A} = \mathbf{A}\mathbf{I}_n = \mathbf{A}$

Inverse $\mathbf{AA}^{-1} = \mathbf{I}_n = \mathbf{A}^{-1}\mathbf{A}$: **Regular**/Inbertible/**Nonsingular**
- Unique ★
- Matrix $\mathbf{A}$가 $2\times 2$면 $\mathbf{A}^{-1} = \frac{1}{a_{11}a_{22} - a_{12}a_{21}}\begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11}\end{pmatrix} = \frac{1}{\det}\begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11}\end{pmatrix}$
Transpose
- $\mathbf{A} = \mathbf{A}^{\top}$이면 **Symmetric**이라 한다.

More Matrix Properties `막 그렇게 당연하지는 않은 것들`
- $(\mathbf{A} + \mathbf{B})^{-1} \neq \mathbf{A}^{-1} + \mathbf{B}^{-1}$
- $\mathbf{A}^{-\top} = (\mathbf{A}^{-1})^{\top} = (\mathbf{A}^{\top})^{-1}$

Matrix의 Scalar Multiplication을 할 때 $\lambda \times \mathbf{A}$처럼 $\times$를 쓰기도 한다.
이건 그냥 직관적으로 결합법칙이나 분배법칙을 사용해도 무관하다.

## 2-3. Solving Systems of Linear Equations

식을 세우고, Gaussian Elimination등을 통해 해를 구하면 된다.

General Solution
![250](../../z.%20Docs/img/Pasted%20image%2020250313093530.png)
- z를 어떻게 Sampling 했느냐에 따라 결과가 다양해진다.

General Solution = Particular Solution + Homogeneous Solution
![350](../../z.%20Docs/img/Pasted%20image%2020250313093709.png)

ML은 진짜로 $\mathbf{Ax = b}$를 푸는 게 다이다.
- 이 Equation을 푸는 것을 컴퓨터 알고리즘으로 만들기 위해 Gauss-Jordan Method등을 정의하는 것이다.

Row-Echelon Form, Reduced Row-Echelon Form
![300](https://dmn92m25mtw4z.cloudfront.net/img_set/la-1-4-x-eq-8/v1/la-1-4-x-eq-8-460w.jpg)
- Pivot은 언제나 그 아래 행의 Pivot의 왼쪽에 있어야 한다.

Calculating Inverse

$$
\begin{pmatrix}
1 & 0 & 2 & 0 & | & 1 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & | & 0 & 1 & 0 & 0\\
1 & 2 & 0 & 1 & | & 0 & 0 & 1 & 0\\
1 & 1 & 1 & 1 & | & 0 & 0 & 0 & 1
\end{pmatrix}
$$
$$
\begin{pmatrix}
-1 & 2 & -2 & 2 \\
? & ? & ? & ? \\
? & ? & ? & ? \\
? & ? & ? & ? \\
\end{pmatrix}
$$

Algorithms for Solving System of Linear Equations

(1) **Pseudo-Inverse** $(\mathbf{A}^{\top}\mathbf{A})^{-1}\mathbf{A}^{\top}$
이게 문제가 뭘까?
Inverse가 들어가있기 때문에 연산량이 어마어마해서 실제로 쓰기가 너무 어렵다는 점이다.
따라서 좋아보이긴 하지만, 실제로 쓰기는 어렵다는 점.

(2) Gaussian Elimination
- 바로 한 번에 구해지지 않기 때문에 Intuitibe and Constructive 방법이라고 한다.
- Cuic Complexity

(3) Optimization
- Gradient se

## 2-4. Vector Spaces

**Group**
- Set 안에서 임의로 뽑은 $x, y$에 대해서 $x \otimes y$가 $\mathcal{G}$ 에 속해야 한다.
- Set 안에서 임의로 뽑은 $x, y$에 대해서 연산 순서가 결과에 영향을 미치면 안된다.
- 항등원이 존재해야한다.
- 역원이 존재해야한다.

**Abelian Group**
- Set $\mathcal{G}$에서 연산자 $\otimes$에 대해서 교환법칙이 성립하면 Abelian Group이라고 한다.

어떤 게 Group, Abelian Group을 만족하는지 판단할 수 있어야 한다.

**Vector Spaces**
- $Def.\ \mathbf{V} = (\mathcal{V}, +, \cdot)$
- $(\mathcal{V}, +)$은 Abelian Group이다.
- 분배법칙, 결합법칙, 항등원이 성립한다.

**Vector Subspaces** `= Linear Subspace = Subspace`
- Vector Space $\mathbf{V}$에 대해서 $\mathcal{U} \subset \mathcal{V}$일때, $\mathbf{U} = (\mathcal{U}, +, \cdot)$ 즉 $\mathbf{U}$ 자체도 Vector Space여야 한다.
- e.g.
	- Vector Spave $\mathbf{V}$에 대하여 $\mathbf{V}$와 $\{0\}$도 Subspace이다.
	- $\mathbf{Ax} = 0$ ($\mathbf{Ax = b}$는 아니다!)
- 임의의 많은 Subspaces의 교차점은 Subspace 자체이다.

## 2-5. Linear Independence

**Linear Independence**
- Linear Combination $\mathbf{v} = \lambda_1\mathbf{x}_1 + \cdots + \lambda_k\mathbf{x}_k$
- $Def.$ $\sum^{k}_{i = 1}\lambda_i\mathbf{x}_i = 0$의 Solution이 $\lambda_1 = \cdots = \lambda_k = 0$ 밖에 없을 때
- Vectors가 Lineary dependent할 때, 적어도 하나의 vector는 다른 vector들의 Linear Combination이다.
- Linear Independence 검사: Gaussian Elimination으로 정리 했을 때 모든 Column Vector들이 Pivot Column이면

(Q) k개의 Linearly Independent Vectors $\mathbf{b_k}$로 m개의 Linear Combination Vectors $\mathbf{x_m}$를 만들었을 때, 이는 Linearly Independent한가?
	$\mathbf{x}_j = \mathbf{B}\lambda_j$로 표현 가능하다. `자세한 것은 강의자료 40p. 참조`
	(A) Yes.

(Q) $\mathbf{x}_1$과 $\mathbf{x}_4$는 Linearly Dependent한가?
$$
\begin{pmatrix}
1 & -4 & 2 & 17 \\
-2 & -2 & 3 & -10 \\
1 & 0 & -1 & 11 \\
-1 & 4 & -3 & 1
\end{pmatrix}
\rightarrow
\begin{pmatrix}
1 & 0 & 0 & -7 \\
0 & 1 & 0 & -15 \\
0 & 0 & 1 & 18 \\
0 & 0 & 0 & 0
\end{pmatrix}
\therefore \text{dependence}
$$
## 2-6. Basis and Rank

**Generating Set**
- $Def.$ $\mathbf{v}\in \mathcal{V}$의 모든 $\mathbf{v}$를 $\mathcal{A} = \{\mathbf{x}_1 , \cdots, \mathbf{x}_k\}\subset \mathcal{V}$로 표현 가능할 때, $\mathcal{A}$를 $\mathbf{V}$의 Generating Set이라고 한다.
- **Span**: $\mathcal{A}$의 모든 Linear Combination의 Set. Notation은 $\mathbf{V} = \text{span}[\mathcal{A}]$이다.

**Basis**
- $Def.$ Minimal Generating Set $\mathcal{B}$
- **Dimension**: Basis Vector의 개수
- $\mathbf{x}\in\mathbf{V}$인 모든 Vector는 Unique한 $\mathcal{B}$의 Linear Combination으로 나타낼 수 있다.
- Standard Basis e.g. $(0\ 1),\ (1\ 0)$

`정의되는 순서: Generating Set -> Span -> Basis -> Dimension`

(Q) `45p.` 두 번째
	$\mathbb{R}^4$를 위한 Basis를 나타내고 있기 때문에 3개의 Vector로는 4차원 공간을 만들 수 없기 때문에 Basis가 아니다.

Determining a Basis
1. Matrix $\mathbf{A} = (\mathbf{x}_1\ \mathbf{x}_2\ \cdots\ \mathbf{x}_m)$을 만든다.
2. $\mathbf{A}$의 Row-Echelon Form을 찾는다.
3. Pivot Column들을 모든다.
`생각하면 쉽다.`

Example 2.17


**Rank**
- $Def.$ $rk(\mathbf{A})$Linearly Independent한 Column의 개수
- e.g.의 $\begin{pmatrix}1 \\ 3 \\ 0\end{pmatrix}$은 쓸모가 없다는 것이다. 있으나 없으나 Column들로 Span할 수 있는 Space는 같다. `이는 ML 모델의 경량화로 이어진다.`
- $rk(\mathbf{A}) = rk(\mathbf{A}^{\top})$
- $\mathbf{A}\in \mathbb{R}^{n\times n},\ rk(\mathbf{A}) = n \iff$ $\mathbf{A}$가 invertible하다.
- $\mathbf{Ax} = \mathbf{b}$가 solvable $\iff\ rk(\mathbf{A} = rk(\mathbf{A|b}))$
- `필기 필요`
- Full Rank $= \min(\text{\# of cols}, \text{\# of rows})$

## 2-7. Linear Mappings

어떤 Vector Spave의 구조가 유지되는 Mapping을 알아보자.

**Linear Mapping**
- $Def.$ $\Phi: \mathbf{V}\mapsto\mathbf{W}$
- 다음과 같은 성질을 만족하면 Linear Transformation이라고 얘기할 수 있다.
	- $\Phi(\mathbf{x + y}) = \Phi(\mathbf{x}) + \Phi(\mathbf{y})$
	- $\Phi(\lambda\mathbf{x}) = \lambda\Phi(\mathbf{x})$
- 어떻게 Mapping 되느냐에 따라 다음과 같이 정의 할 수 있다.
	![300](https://media.geeksforgeeks.org/wp-content/uploads/20231016183533/Injective-Function-5.png)
	- Injective(단사): $if\ \forall \mathbf{x, y}\in \mathcal{V},\ \Phi(\mathbf{x}) = \Phi(\mathbf{y}) \Rightarrow \mathbf{x = y}$
	- Surgective(전사): $if\ \Phi(\mathcal{V}) = \mathcal{W}$
	- Bijective(전단사): Injective and Surjective

Example 2.19 (Homomorphism)
	위에서 언급한 성질을 이용해 Linear Transformation인지 확인
	`자세한 것은 Silent Camera 참고`

- Bijective Mapping에서는 Inverse Mapping $\Phi^{-1}$이 성립한다.
- Isomorphism: $if \Psi$가 Linear 하고 Bijective 할때
	- $\mathbf{V}$와 $\mathbf{W}$이 Isomorphic하다는 것은 일대일 대응이라는 것이고, $\dim(\mathbf{V} = \dim(\mathbf{W}))$이다.
	- $Theorem.$ Dimension이 같은 Vector Space는 같은 공간이다.

★
> Linear Mapping $\Phi$과 $\Psi$에 대하여, $\Phi \odot \Psi$ 또한 Linear Mapping이다.
> 즉, Linear Mapping을 중첩시킬 수 있다는 뜻이며 이는 ML의 근간을 이루는 개념이다.

`추가 정리 필요`

**Coordinates**
- Cartesian Coordinates(데카르트 좌표계): 흔히들 우리가 아는 좌표계
	![150](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Cartesian-coordinate-system.svg/1200px-Cartesian-coordinate-system.svg.png)
- Coordinate는 Basis에 따라서 바뀐다. 무슨 이야기냐면 Basis가 바뀌면 다른 Unique한 Linear Combination으로 표현된다는 것이다.
	- $\mathbf{x} = \alpha_1\mathbf{b}_1 + \cdots + \alpha_n\mathbf{b}_n,\ Basis\ B = (\mathbf{b}_1, \cdots, \mathbf{b}_n)$
	- 이때 $\alpha = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix}$가 바로 Coordinate이다.
- `추가 정리 필요 (빠..빨라요 교수님)`

**Basis Change**
- $\mathbf{y} = \mathbf{B}'^{-1}\mathbf{bx}$
- `추가 정리 필요`
- Coordinate System이 Unique하다는 것은 Basis는 다양할 수 있으나, 각각의 Basis에 해당하는 Coordinate는 하나라는 것에 주의하자.

Example
$\mathbf{y} = \mathbf{B}'^{-1}\mathbf{bx}$를 이용해 풀면 된다. 자세한 것은 pdf 참고

**Transformation Matrix**
- 모든 Basis Vector에 대해서 Transformation을 나타내면 Matrix Multiplication의 형태로 나타낼 수 있다.
- $\hat{x}$는 $\mathbf{b}$로 표현된 좌표고, $\hat{y}$는 $\mathbf{c}$로 표현된 좌표라고 생각하면된다.
- 우리는 $\mathbb{R}\in n\times m$ 짜리 Transformation Matrix $\mathbf{A}_{n\times m}$를 만들 수 있다.

그럼 Matrix $\mathbf{A}_\Phi$의 Element의 각 요소는 어떻게 찾으면 될까?

Example 2.21 (Transformation Matrix)
`문제는 pdf 58p. 참고`
$$\mathbf{Ab_i} = \mathbf{c_i}$$

**Basis Change: Gerenal Case**
- Intra: 같은 공간에서 이동하는 것 $\mathbf{B} \rightarrow \mathbf{C}$, $\mathbf{B'} \rightarrow \mathbf{C'}$
- Inter: 다른 Vector Space로 이동하는 것 $\mathbf{B'} \rightarrow \mathbf{B}$, $\mathbf{C'} \rightarrow \mathbf{C}$
- $Theorem.$ $\mathbf{A'}_{\Phi} = \mathbf{T}^{-1}\mathbf{A}_{\Phi}\mathbf{S}$ (Matrix 순서 주의)
중요한 점은, 바로 변환하기 어려울 때 Theorem을 통해 우회하는 것이 가능하다는 것이다.

- Equivalence: $\mathbf{A'} = \mathbf{T}^{-1}\mathbf{AS}$
- Similarity: $\mathbf{A'} = \mathbf{S^{-1}}\mathbf{AS}$ `이 개념은 계속 나오니 잘 기억해두자.`
- Remark. Simillar $\subset$ Equivalance

Example 2.24 (Basis Change)
$$\tilde{A}_{\Phi} = \mathbf{T}^{-1}\mathbf{A}_{\Phi}\mathbf{S}$$

**Image and Kernel**
![300](../../z.%20Docs/img/Pasted%20image%2020250325094842.png)
- $\mathbf{v}$ 안에 있는 vectore들을 $\Phi$에 태워서 $\mathbf{W}$로 보내버렸는데 0 vector가 되었을 때, 이를 Kernel이라고 부른다.
- $Def.$ $\ker(\Phi) := \Phi^{-1}(0_{W}) = \{ \mathbf{v}\in\mathbf{V}: \Phi(\mathbf{v}) = 0_{W} \}$
- Image/range: 그림에서 $\mathbf{w}\in\mathbf{W}$에 해당하는 벡터들의 set
- $\mathbf{V}$: Domain, $\mathbf{W}$: Codomain

**Properties**
- `추가 정리 필요`
- ★ 어떠한 Linear Transformation이던 Matrix $\mathbf{A}$로 표현할 수 있다.
- $\ker(\Phi)$는 $\mathbf{Ax} = 0$으로 표현되는 Homoheneous System의 Solution이다.

Example 2.25 (Image and Kernel of a Linear Mapping)
`문제 풀이 및 자세한 설명은 pdf 65p. 참고`

**Rank-Nullity Theorem**
`정리 필요`

## 2-8. Affine Spaces

**Affine Subspace**
- Affine Subspace는 Vector Subspace가 아니다!
- translation vector = support vector
- linear function: $f(x) = ax$
- affine function: $f(x) = ax + b$ (원점을 지나지 않는다.)

# 3. Analytic Geometry

## 3-1. Norms

**Norm**
$Def.$벡터의 길이 개념
$||\cdot||: \mathbf{V} \rightarrow \mathbb{R}$, 벡터에서 Real Number로 간다.
![100](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/TriangleInequality.svg/220px-TriangleInequality.svg.png)
- Absolutely Homogeneous: $||\lambda x || = |\lambda|\||x||$
- Triangle Inequality: $||x + y|| \le||x|| + ||y||$
- Positive Definite: $||x||\ge0 \text{ and } || x || = 0 \Longleftrightarrow x = 0$ `강의자료 오타`

![300](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLSz8eqkjnBK8fb4frSx-t1vviAIR7qjpC-A&s)
1. **Manhattan Norm ($\ell_1$ norm)**: $||x||_1 :==== \Sigma^{n}_{i = 1}|x_i|$
	- Matnhatten Norm이라고는 거의 안 한다.
2. **Euclidean Norm ($\ell_2$ norm)**: $||x||_z :==== \sqrt{\Sigma^{n}_{i = 1}{x_i^2}} = \sqrt{\mathbf{x}^{\top}\mathbf{x}}$
	- 원점으로부터 얼마나 Distance가 떨어져 있느냐를 얘기하는 것

## 3-2. Inner Projects ☆

추상 벡터 공간에서 정의된 벡터의 길이와 두 벡터 사이의 각도 또는 거리에 대하여 이야기 할 필요가 있다.

**Bilinear Mapping $\ohm$**
$Def.$ `정리 필요`
- Symmetric: 입력 순서를 반대로 바꿔도 결과가 같다.
- Positive Definite: Bilinear Mapping은 항상 0보다 커야하고 $\ohm(0, 0) = 0$이어야 한다.

**Inner Product**
다음 조건을 만족하는 어떤 Mapping을 우리는 그것을 Inner Product라고 한다.
1. $<\mathbf{u + v, w> = <u, w> + <v, w>}$
2. $\mathbf{<\lambda v, w> = \lambda<v, w>}$
3. $\mathbf{<v, w> = <w, v>}$
4. $\mathbf{<v, v>\ge0}$ and equal iff $\mathbf{v} = 0$
$(\mathbf{V}, <\cdot, \cdot>)$를 Innter Product Space라고 한다.

Example의 두 번째 예시를 보며 실제 조건을 적용해보며 왜 Inner Product인지 한 번 알아보자.
- Euclidean 공간
- $\mathbf{R}^2$
- 함수

**Symmetric, Positive Definite Matrix**
다음 조건을 만족하면 우리는 그것을 Symmetric하고 Potisive Definite인 Square Matrix라고 한다.
$$\forall x \in \mathbf{V} \backslash \{0\}: \mathbf{x}^{\top} \mathbf{Ax} >0$$
여기서 $\ge$면 Symmetrix, Positive Semidefinite이라고 한다.

e.g. 3.4 `66p.`
$\mathbf{x}^{\top} \mathbf{Ax}$로 정리하여 완전제곱식이 들어간 수식의 형태로 나타내어 양수 여부를 따져보면 된다.

`12p.`
$\mathbf{\hat{x}}^{\top} \mathbf{A\hat{y}}$
- x, y를 b의 Linear Combination으로 표현 했을 때 Inner Product 성질 상 덧셈인 Sigma와 스칼라 값은 밖으로 뺄 수있다. 결론은 x와 y의 좌표를 가져와서 A의 양옆에 곱해주면 된다. 이때 A는 Basis Vector로 구성된 Matrix이다. A가 Identity Matrix이면 우리가 아는 Dot Product의 정의가 된다.
- 앞에서는 Inner Product를 매우 추상적으로 정리했는데, Basis Vector를 기반으로 명확하게 정의할 수 있다는 것이다.
- 이 A를 어떻게 찾을 것이냐? 예를 들어 A를 Identity로 가정하고 Euclidean 공간이니 Dot Product를 하며 연구를 진행했는데 나중에 잘 관찰해보니 다른 공간인 것 같으면 A의 값을 바꿔야 한다는 소리이다.

`13p.`
- 내적의 결과값은 딱 하나의 숫자이다.
- Properties
	- 만약 $= 0$이면 A가 Positive Definite이 아니기 때문에 모순이 발생한다.
	- A의 대각 성분은 모두 양수이다. 아니면 A는 Positive Definite이 아닌 것이다.

## 3-3. Lenghts and Distances

**Length**
Inner Product는 Norm을 자연스럽게 유도한다.
$$||x|| := \sqrt{\mathbf{\langle x, x\rangle}}$$
하지만 모든 Norm이 이렇게 정의될 수 있는 건 아니다.
$\ell 2$ Norm은 다음과 같이 정의 가능하나, $\ell 1$은 정의될 수 없다.
$||x||_2 = \sqrt{\mathbf{x^{\top}x}}$

코시슈바르츠 부등식
$|\mathbf{<x, y>}| \le ||\mathbf{x}|| \ ||\mathbf{y} ||$

e.g. 3.5 (Lenghts of Vectors Using Inner Products) `pdf 81p.`
- different inner product에선 길이가 1이다. 즉, 다른 공간에서 잰 Norm은 달라질 수 있다는 것이다.

**Distance**
두 벡터의 차이의 Length
$$d(\mathbf{x, y}) := ||\mathbf{x - y}||$$
- $\sqrt{\mathbf{\langle x - y, x - y\rangle}}$로도 표현할 수는 있는데, 이는 Norm을 가끔씩 Inner Product로도 표현 가능하기 때문이다.
- 추가로 $\ell_1$ Norm으로로 $\ell_1$ Space에서 Distance를 정의할 수 있다.
- 일반적으로 다음을 만족하면 Distance의 적절한 개념인 Metric이라고 한다.
	- Positive Definite: $\forall x, y\ d(\mathbf{x, y})\ge 0$ and $d(\mathbf{x, y}) = 0\Longleftrightarrow \mathbf{x = y}$
	- Symmetrix: 이쪽에서 재든 저쪽에서 재든 똑같다는 뜻이다.
	- Triangle Inequality `위에서 한 거랑 거의 비슷한 것`
생각보다 Metric 같은데 Metric 같지 않은 경우도 많고, Distance도 마찬가지이다. 개념을 확실히 알고 가 헷갈리지 않도록 하자.

## 3-4. Angles and Orthogonality

**Angle, Orthogonal, and Otthonormal**
코시슈바르츠 부등식 $|\mathbf{<x, y>}| \le ||\mathbf{x}|| \ ||\mathbf{y} ||$ 으로부터 다음 수식을 끌어낼 수 있다.
$$-1\le \frac{\mathbf{\langle x, x\rangle>}}{\mathbf{||x||\ ||y||}} \le 1$$
이때 우리는 Unique한 $\omega \in [0, \pi]$가 존재한다는 것을 알 수 있다.
$$\cos{\omega} = \frac{\mathbf{\langle x, y\rangle}}{||\mathbf{x} ||\ ||\mathbf{y} ||}$$
$if\ \mathbf{\langle x, y\rangle>} = 0$, in other word $\omega = \frac\pi2$
- **Orthogonal** $\mathbf{x\bot y}$: 따라서 두 개의 벡터가 Orthogonal하다는 것은 두 개의 벡터를 내적했을 때 0이 나온다는 것이다.
- **Orthonormal**: 추가로 $||\mathbf{x}|| = || \mathbf{y}|| = 1$이면 Orthonormal이라고 한다.

e.g. 강의자료 참고

**Orthogonal Matrix**

$Def.$ A라는 Square Matrix의 Column(또는 Row)끼리 Orthonormal할 때 이 Matrix를 Orthogonal Matrix라고 한다.
$$\mathbf{AA^{\top}} = I = \mathbf{A^{\top}A}, \text{ implying }\mathbf{A}^{-1} = \mathbf{A}^{\top}$$
Orthogonal Matrix는 다음과 같은 성질을 만족한다.
- $\mathbf{A, B}$가 Orthogonal이면 $\mathbf{AB}$도 Orthogonal이다.
- $\mathbf{A}$가 Orthogonal이면 $\det(\mathbf{A}) = \pm 1$이다.
Orthogonal Matrix에 의한 Linear Mappig $\Phi$는 Length와 Angle이 보존된다.
`증명은 강의자료 참고`

## 3-5. Orthonormal Basis

**Orthonormal Basis**

## 3-6. Orthogonal Complement

**Orthogonal Complement (직교 여공간)**

$\mathbf{U}^\bot$

Normal Vector $\mathbf{w}$는 다음과 같은 벡터이다.
$\mathbf{||w||} = 1$이고 $\mathbf{U}$에 Orthogonal하여 $\mathbf{U}^\bot$의 Basis를 이룬다.

## 3-7. Inner Product of Functions

**Inner Product of Functions**
함수의 Inner Product는 다음과 같다.
$\langle u, v \rangle := \int^b_a u(x)v(x)dx$

## 3-8. Orthogonal Projections

큰 Data는 굉장히 High Dimensional하다.
우리는 중요한 정보는 Low Dimension에 있다고 가정하고 고차원에서 저차원으로 Mapping하고 싶은 것이다.
- *정보 누수를 최소한으로 하여 저차원으로 Mapping하는 것이 목표이다.*
딥러닝으로 저차원으로 보내는 방식을 학습하는 것이 지금의 주류 해결책이다.
그 전에는 낮추는 규칙을 사람들이 일일이 정했는데, 이를 Hand-crafted Feature라고 한다.
`Projection = Feature = Enbedding`

**Definition of Projection**

$Def.$ $\mathbf{U}\in\mathbf{V}, \pi:\mathbf{V\rightarrow U} \text{ and } \pi^2 = \pi \circ \pi = \pi$
- 두 번 Mapping 했을 때와 한 번 Mapping 했을 때의 결과가 같아야 한다.
- 즉, 투영된 점은 이미 그 공간 위에 있으니까 다시 투영해도 같은 점이 나온다는 의미이다.

**Projection onto Lines(1D Subspaces)**

$x$를 line에 Orthogonal하게 Projection해야 정보 손실이 가장 적다!
$$\mathbf{P}_\pi = \frac{\mathbf{bb^\top}}{||\mathbf{b}||^2}$$
우리가 아는 그 Projection 식이다.

**Inner Product and Projection**

$\langle\mathbf{x, b}\rangle = ||\pi_b(x)||\times ||b||$는 다음을 생각해보면 어쩌면 당연한 것이다.
$\vec{a}\times\vec{b} = ||a|| \ ||b||\cos\theta$

e.g.
`강의자료 31p. 참고`

**Projection onto General Subspaces**

이제 n차원에서 m차원으로 보내보자.
$$\mathbf{P}_\pi = \mathbf{B(B^\top B)^{-1}B^\top}$$
유도 과정은 1D에서의 유도 과정과 비슷하며, 책 `71p.`에 나와있다.
- 여기서 $\mathbf{(B^\top B)^{-1}B^\top}$를 **Pseudo-Inverse**라고 한다. 왜냐하면 $\mathbf{B}$가 Inverse가 나오는 Square Matrix가 아닐 수도 있기 때문에 먼저 $\mathbf{B^\top B}$로 Square Matrix로 변환해주는 것이다.

e.g.
`강의자료 33p. 참고`

**Gram-Schmidt Otrhogonalization Method (G-S method)**



e.g.
`강의자료 35p. 참고`

## 3-9. Rotations

Remind. Orthogonal Matrix는 길이와 각도가 보존된다.

**Rotation**
Rotation이라는 특수한 Mapping은 Orthogonal Matrix로 나타낼 수 있다.
$$R(\theta) = \begin{pmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{pmatrix}$$

# 4. Matrix Decompositions

☑️ Matrix를 어떻게 요약하느냐: Determinant와 Eigenvalues

☑️ Matrix를 어떻게 Decomposition 하느냐: Cholesky Decomposition, Diagonalization, Singular Value Decomposition

☑️ Matrix를 어떻게 Approzimate 하느냐

Flow Chart
![300](../../z.%20Docs/img/Pasted%20image%2020250403093038.png)

## 4-1. Determinant and Trace

**Determinant**
$$\begin{vmatrix}a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}\end{vmatrix}$$
Laplace Expansion
- $n\times n$ 행렬의 행렬식을 특정 행 또는 열을 기준으로 전개해서 계산하는 방법
- 다시 말해, 어떤 행이나 열을 선택해서, 그 안의 각 원소에 대해 소행렬식(minor)과 부호(cofactor)를 곱해 전체 행렬식으로 만드는 방식
$$det⁡(A)= \sum_{j=1}^{n} (-1)^{i+j} \cdot a_{ij} \cdot \det(M_{ij})$$
e.g. 4.3 (Laplace Expansion)
`교재 108p. 참고`

이렇듯 행렬이 커지면 Determinant 계산하는 것은 어려워진다. `단순히 계산할게 많아진다.`

**Determinant: Properties**

Determinant의 성질들은 다음과 같다:
`강의자료 10p. 참고`
- Triangular Matrix에서 Determinant는 대각성분을 모두 곱한 것과 같다.
- 어떤 행 또는 열에 대해, 그 행(열)에 다른 행(열)의 배수를 더해도 Determinant는 변하지 않는다.

**Trace**
$$tr(\mathbf{A}):= \sum^n_{i = 1} a_{ii}$$
- $tr(\mathbf{AB}) = tr(\mathbf{BA}), tr(\mathbf{AKL}) = tr(\mathbf{KLA})$ `Cyclic Turmination`
- $tr(\mathbf{xy^{\top}}) = tr(\mathbf{y^{\top}x}) = y^{\top}x$ `Dot Product`
`나머지 특징은 강의자료 12p. 참고`

**Characteristic Polynomial**
$$\begin{align}
p_{\mathbf{A}}(\lambda) &:= \det(\mathbf{A - \lambda I}) \\
&= c_0 + c_1\lambda + c_2\lambda^2 + \cdots + c_{n-1}\lambda^{n - 1} + (-1)^n\lambda^n \\
\text{where } c_0 &= \det(\mathbf{A}) \text{ and } c_{n-1}{ = (-1)^{n-1}}tr(\mathbf{A})
\end{align}$$
- 이게 왜 필요하느냐? Eigenvalue와 Eigenvector를 구할 때 필요하기 때문이다.

## 4-2. Eigevalues and Eigenvectors

**Eigenvalue and Eigenvector**
$$\mathbf{Ax} = \lambda\mathbf{x}$$
- Non-trivially 하게 푼다는 것은, 명확한 해인 $\mathbf{x} = 0$을 보지 않고 풀겠다는 것이다.
- Invertable := det != 0
- $\det(\mathbf{A - \lambda I_n}) = 0 \Leftrightarrow p_{\mathbf{A}}(\lambda) = 0$

e.g.
-> Eigenvector는 Unique하지 않고 무한하게 있다.

**Eigenvalue and Eigenvector: Properties**

다음과 같은 특징들을 가지고 있다.
- Collinear: 같은 직선 위에 있는 벡터들
- Eigenspace는 $\mathbf{A - \lambda I}$의 Kernel이다.
- 기하적으로 해석하면 0이 아닌 고윳값에 해당하는 고유벡터는 선형 변환에 의해 늘어나는 방향을 가리킨다. 여기서 고윳값은 길이가 얼마나 바뀌느냐를 나타내는 Factor이다.
- Eigencalue는 Transformation Matrix에 무관하다.

>> Basis가 바뀌어도 Determinant, Trace, Eigenvalue들은 안 바뀐다.

**Examples for Geometric Interpretation**
`강의자료 19-20p. 참고`

## 4-3. Cholesky Decomposition

## 4-4. Eigendecomposition and Diagonalization

## 4-5. Singular Value Decomposition