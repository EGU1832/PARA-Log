# 기초머신러닝

# 0. Syllabus

AI를 전공하기 위해 반드시 알아야 할 Math Concept들은 다음과 같다.
- Linear Algebra ★
- Analytic Geometry ★
- Matrix Decomposition ★
- Vector Calculus
- Probability and Distributions
- Optimization

그리고 기초적인 ML Problem들은 다음과 같다.
- When Models Meet Data `Linear Mapping`
- Dimensionality Reductds3fion with Principal Component Analysis `차원축소 with PCA`
- Density Estimation with Gaussian Mixture Models `분포 예측 with GMM`
- Claasification with Support Vector Machines `분류 with SVM`

본 강의에서 배우는 ML의 기본 토대 4가지는 다음과 같다.
- Regression
- Dimensionality Reduction
- Density Estimation
- Classification

따라서 강의자료로만 공부하면 부족할수도 있으니, 교재도 보기를 바란다.
`Mathematics for Machine Learning1, Cambridge University Press, Marc Peter`

# 1. Introduction

Notation
- Sets: $\mathcal{A, B, C}$

머신러닝의 알고리즘은 일반 알고리즘들과 접근방식이 다소 다르다.
머신러닝의 알고리즘은 주어진 Data로부터 Valuable Information을 추출하는 것이 주요 목적이다.

ML의 주요 3가지 Concept는 다음과 같다.
1. Data: Data를 생성해내는 어떤 Process에 관한것
2. A Model: ML에 필요한 중요한 특징 중 하나는 내가 가지고 있던 Data Set 뿐만이 아니라 Unseened Data로부터 유의미한 정보값을 추출해낼 수 있는 것이다. Application에 따라 이 Unseen Data의 범위를 정하는 것 또한 중요하다고 할 수 있겠다.
3. Learning: Data에 있는 패턴과 구조를 자동적으로 이해할 수 있게끔 모델의 Parameter를 최적화하는 것이다.
항상 이 3가지가 동등하게 중요하다는 것을 앞으로 ML을 공부하며 잊으면 안될 것이다.

어느 학문이나 그렇겠지만, 근본적인 이해를 위해선 수학적인 Background가 중요하다.

ML에서 알고리즘이란 무엇이냐?
- 예를들어 "ML Algorithm"이라 얘기할 때 Input Data를 기반으로 예측값을 뽑아내는 시스템을 가리킬 때가 있다. 즉, 우리가 생각하는 알고리즘과 조금 상이하다.
- Training Proces를 의미할 때도 있다.
그럼 뭐가 뭔지 어떻게 판단하냐? Context상으로 이해할 수 밖에 없다.

#### Data

본 강의에서, 우리는 Data를 컴퓨터에 쉬이 학습될 수 있도록 이미 수치적으로 표현된 **벡터**라고 생각할 것이다.
```
Data == vector
```

그렇다면 **벡터**란 무엇이라고 생각하면 되느냐?
- CSE View: 숫자의 배열
- PHY View: 방향과 크기를 가지는 화살표
- MAT View: 덧셈과 Scaling을 따르는 객체
그때 그때 Context에 따라 알아서 잘 이해하면된다.

#### Model

좋은 모델이란,
- 실제 사고방식을 얼마나 잘 단순화하여 모방했느냐에 따라 좌우된다.
- 또한 사전학습 없이 Real-World Experiments를 얼마나 잘 예측하는지

#### Learning

Training이란, Model의 Parameter들을 최적화하는 것이다.
- 이는 모델이 Training Data에 대하여 얼마나 잘 예측하는지를 평가하는 Utility 함수를 기반으로 이루어진다.
- 계속 반복해서 말하지만, **Training Data가 아닌 Unseened Data에 대하여 좋은 성능을 내는 것**이 중요하다.


# 2. Linear Algebra

## 2-1. Systems of Linear Equations

## 2-2. Matrices


$a_{\text{ Row Index | Column Index }}$

$\mathbf{Ax} = \mathbf{b}$
Column Vector: $\begin{pmatrix} a_{11}\\\vdots\\a_{m1} \end{pmatrix} \mathbf{x_1}$

Column - 세로, Row - 가로

Reshape: $\mathbb{R}^{m\times n} \rightarrow \mathbb{R}^{mn}$

$\begin{pmatrix}1 & 2 & 3 \\ 3 & 2 & 1\end{pmatrix} \times \begin{pmatrix}0 & 2 \\ 1 & -1 \\ 0 & 1\end{pmatrix} = \begin{pmatrix}2&2\\3&5\end{pmatrix}$

Identity Matrix: $\mathbf{I}_n$, 문맥 상 크기가 확실하면 굳이 n을 표기할 필요는 없다.

Matrix Properties
- 결합법칙: $\mathbf{(AB)C} = \mathbf{A(BC)}$
- 분배법칙: $\mathbf{(A+B)C} = \mathbf{AC + BC} \neq \mathbf{CA + CB}$
- 항등원: $\mathbf{I}_m\mathbf{A} = \mathbf{A}\mathbf{I}_n = \mathbf{A}$

Inverse $\mathbf{AA}^{-1} = \mathbf{I}_n = \mathbf{A}^{-1}\mathbf{A}$: **Regular**/Inbertible/**Nonsingular**
- Unique ★
- Matrix $\mathbf{A}$가 $2\times 2$면 $\mathbf{A}^{-1} = \frac{1}{a_{11}a_{22} - a_{12}a_{21}}\begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11}\end{pmatrix} = \frac{1}{\det}\begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11}\end{pmatrix}$
Transpose
- $\mathbf{A} = \mathbf{A}^{\top}$이면 **Symmetric**이라 한다.

More Matrix Properties `막 그렇게 당연하지는 않은 것들`
- $(\mathbf{A} + \mathbf{B})^{-1} \neq \mathbf{A}^{-1} + \mathbf{B}^{-1}$
- $\mathbf{A}^{-\top} = (\mathbf{A}^{-1})^{\top} = (\mathbf{A}^{\top})^{-1}$

Matrix의 Scalar Multiplication을 할 때 $\lambda \times \mathbf{A}$처럼 $\times$를 쓰기도 한다.
이건 그냥 직관적으로 결합법칙이나 분배법칙을 사용해도 무관하다.

## 2-3. Solving Systems of Linear Equations

식을 세우고, Gaussian Elimination등을 통해 해를 구하면 된다.

General Solution
![250](../../Z.%20Docs/img/Pasted%20image%2020250313093530.png)
- z를 어떻게 Sampling 했느냐에 따라 결과가 다양해진다.

General Solution = Particular Solution + Homogeneous Solution
![350](../../Z.%20Docs/img/Pasted%20image%2020250313093709.png)

ML은 진짜로 $\mathbf{Ax = b}$를 푸는 게 다이다.
- 이 Equation을 푸는 것을 컴퓨터 알고리즘으로 만들기 위해 Gauss-Jordan Method등을 정의하는 것이다.

Row-Echelon Form, Reduced Row-Echelon Form
![300](https://dmn92m25mtw4z.cloudfront.net/img_set/la-1-4-x-eq-8/v1/la-1-4-x-eq-8-460w.jpg)
- Pivot은 언제나 그 아래 행의 Pivot의 왼쪽에 있어야 한다.

Calculating Inverse

$$
\begin{pmatrix}
1 & 0 & 2 & 0 & | & 1 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & | & 0 & 1 & 0 & 0\\
1 & 2 & 0 & 1 & | & 0 & 0 & 1 & 0\\
1 & 1 & 1 & 1 & | & 0 & 0 & 0 & 1
\end{pmatrix}
$$
$$
\begin{pmatrix}
-1 & 2 & -2 & 2 \\
? & ? & ? & ? \\
? & ? & ? & ? \\
? & ? & ? & ? \\
\end{pmatrix}
$$

Algorithms for Solving System of Linear Equations

(1) **Pseudo-Inverse** $(\mathbf{A}^{\top}\mathbf{A})^{-1}\mathbf{A}^{\top}$
이게 문제가 뭘까?
Inverse가 들어가있기 때문에 연산량이 어마어마해서 실제로 쓰기가 너무 어렵다는 점이다.
따라서 좋아보이긴 하지만, 실제로 쓰기는 어렵다는 점.

(2) Gaussian Elimination
- 바로 한 번에 구해지지 않기 때문에 Intuitibe and Constructive 방법이라고 한다.
- Cuic Complexity

(3) Iterative Methods `Optimization`
- 직접 해를 계산하기보다 점진적으로 근사해에 수렴하는 방법
- Stationary Iterative Methods: 초기 추정값 $x^{(0)}$에서 출발 → 점진적으로 업데이트
	- Richardson method
	- Jacobi method
	- Gauss–Seidel method
	- SOR (Successive Over-Relaxation)
- Krylov Subspace Methods: 반복 과정에서 생성되는 Krylov 부분공간을 이용해 효율적으로 해를 근사
	- Conjugate Gradient (CG): 대칭 양의 정부호(SPD) 행렬에 적합
	- GMRES (Generalized Minimal Residual): 비대칭 행렬에도 사용 가능
	- BiCG (Bi-Conjugate Gradient): 비정규 행렬도 처리 가능

## 2-4. Vector Spaces

**Group**
- Set 안에서 임의로 뽑은 $x, y$에 대해서 $x \otimes y$가 $\mathcal{G}$ 에 속해야 한다.
- Set 안에서 임의로 뽑은 $x, y$에 대해서 연산 순서가 결과에 영향을 미치면 안된다.
- 항등원이 존재해야한다.
- 역원이 존재해야한다.

**Abelian Group**
- Set $\mathcal{G}$에서 연산자 $\otimes$에 대해서 교환법칙이 성립하면 Abelian Group이라고 한다.

어떤 게 Group, Abelian Group을 만족하는지 판단할 수 있어야 한다.

**Vector Spaces**
- $Def.\ \mathbf{V} = (\mathcal{V}, +, \cdot)$
- $(\mathcal{V}, +)$은 Abelian Group이다.
- 분배법칙, 결합법칙, 항등원이 성립한다.

**Vector Subspaces** `= Linear Subspace = Subspace`
- Vector Space $\mathbf{V}$에 대해서 $\mathcal{U} \subset \mathcal{V}$일때, $\mathbf{U} = (\mathcal{U}, +, \cdot)$ 즉 $\mathbf{U}$ 자체도 Vector Space여야 한다.
- e.g.
	- Vector Spave $\mathbf{V}$에 대하여 $\mathbf{V}$와 $\{0\}$도 Subspace이다.
	- $\mathbf{Ax} = 0$ ($\mathbf{Ax = b}$는 아니다!)
- 임의의 많은 Subspaces의 교차점은 Subspace 자체이다.

## 2-5. Linear Independence

**Linear Independence**
- Linear Combination $\mathbf{v} = \lambda_1\mathbf{x}_1 + \cdots + \lambda_k\mathbf{x}_k$
- $Def.$ $\sum^{k}_{i = 1}\lambda_i\mathbf{x}_i = 0$의 Solution이 $\lambda_1 = \cdots = \lambda_k = 0$ 밖에 없을 때
- Vectors가 Lineary dependent할 때, 적어도 하나의 vector는 다른 vector들의 Linear Combination이다.
- Linear Independence 검사: Gaussian Elimination으로 정리 했을 때 모든 Column Vector들이 Pivot Column이면

(Q) k개의 Linearly Independent Vectors $\mathbf{b_k}$로 m개의 Linear Combination Vectors $\mathbf{x_m}$를 만들었을 때, 이는 Linearly Independent한가?
	$\mathbf{x}_j = \mathbf{B}\lambda_j$로 표현 가능하다. `자세한 것은 강의자료 40p. 참조`
	(A) Yes.

(Q) $\mathbf{x}_1$과 $\mathbf{x}_4$는 Linearly Dependent한가?
$$
\begin{pmatrix}
1 & -4 & 2 & 17 \\
-2 & -2 & 3 & -10 \\
1 & 0 & -1 & 11 \\
-1 & 4 & -3 & 1
\end{pmatrix}
\rightarrow
\begin{pmatrix}
1 & 0 & 0 & -7 \\
0 & 1 & 0 & -15 \\
0 & 0 & 1 & 18 \\
0 & 0 & 0 & 0
\end{pmatrix}
\therefore \text{dependence}
$$
## 2-6. Basis and Rank

**Generating Set**
- $Def.$ $\mathbf{v}\in \mathcal{V}$의 모든 $\mathbf{v}$를 $\mathcal{A} = \{\mathbf{x}_1 , \cdots, \mathbf{x}_k\}\subset \mathcal{V}$로 표현 가능할 때, $\mathcal{A}$를 $\mathbf{V}$의 Generating Set이라고 한다.
- **Span**: $\mathcal{A}$의 모든 Linear Combination의 Set. Notation은 $\mathbf{V} = \text{span}[\mathcal{A}]$이다.

**Basis**
- $Def.$ Minimal Generating Set $\mathcal{B}$
- **Dimension**: Basis Vector의 개수
- $\mathbf{x}\in\mathbf{V}$인 모든 Vector는 Unique한 $\mathcal{B}$의 Linear Combination으로 나타낼 수 있다.
- Standard Basis e.g. $(0\ 1),\ (1\ 0)$

`정의되는 순서: Generating Set -> Span -> Basis -> Dimension`

(Q) `45p.` 두 번째
	$\mathbb{R}^4$를 위한 Basis를 나타내고 있기 때문에 3개의 Vector로는 4차원 공간을 만들 수 없기 때문에 Basis가 아니다.

Determining a Basis
1. Matrix $\mathbf{A} = (\mathbf{x}_1\ \mathbf{x}_2\ \cdots\ \mathbf{x}_m)$을 만든다.
2. $\mathbf{A}$의 Row-Echelon Form을 찾는다.
3. Pivot Column들을 모든다.
`생각하면 쉽다.`

Example 2.17


**Rank**
- $Def.$ $rk(\mathbf{A})$Linearly Independent한 Column의 개수
- e.g.의 $\begin{pmatrix}1 \\ 3 \\ 0\end{pmatrix}$은 쓸모가 없다는 것이다. 있으나 없으나 Column들로 Span할 수 있는 Space는 같다. `이는 ML 모델의 경량화로 이어진다.`
- $rk(\mathbf{A}) = rk(\mathbf{A}^{\top})$
- $\mathbf{A}\in \mathbb{R}^{n\times n},\ rk(\mathbf{A}) = n \iff$ $\mathbf{A}$가 invertible하다.
- $\mathbf{Ax} = \mathbf{b}$가 solvable $\iff\ rk(\mathbf{A} = rk(\mathbf{A|b}))$
- `필기 필요`
- Full Rank $= \min(\text{\# of cols}, \text{\# of rows})$

## 2-7. Linear Mappings

어떤 Vector Spave의 구조가 유지되는 Mapping을 알아보자.

**Linear Mapping**
- $Def.$ $\Phi: \mathbf{V}\mapsto\mathbf{W}$
- 다음과 같은 성질을 만족하면 Linear Transformation이라고 얘기할 수 있다.
	- $\Phi(\mathbf{x + y}) = \Phi(\mathbf{x}) + \Phi(\mathbf{y})$
	- $\Phi(\lambda\mathbf{x}) = \lambda\Phi(\mathbf{x})$
- 어떻게 Mapping 되느냐에 따라 다음과 같이 정의 할 수 있다.
	![300](https://media.geeksforgeeks.org/wp-content/uploads/20231016183533/Injective-Function-5.png)
	- Injective(단사): $if\ \forall \mathbf{x, y}\in \mathcal{V},\ \Phi(\mathbf{x}) = \Phi(\mathbf{y}) \Rightarrow \mathbf{x = y}$
	- Surgective(전사): $if\ \Phi(\mathcal{V}) = \mathcal{W}$
	- Bijective(전단사): Injective and Surjective

Example 2.19 (Homomorphism)
	위에서 언급한 성질을 이용해 Linear Transformation인지 확인
	`자세한 것은 Silent Camera 참고`

- Bijective Mapping에서는 Inverse Mapping $\Phi^{-1}$이 성립한다.
- Isomorphism: $if \Psi$가 Linear 하고 Bijective 할때
	- $\mathbf{V}$와 $\mathbf{W}$이 Isomorphic하다는 것은 일대일 대응이라는 것이고, $\dim(\mathbf{V} = \dim(\mathbf{W}))$이다.
	- $Theorem.$ Dimension이 같은 Vector Space는 같은 공간이다.

★
> Linear Mapping $\Phi$과 $\Psi$에 대하여, $\Phi \circ \Psi$ 또한 Linear Mapping이다.
> 즉, Linear Mapping을 중첩시킬 수 있다는 뜻이며 이는 ML의 근간을 이루는 개념이다.

`추가 정리 필요`

**Coordinates**
- Cartesian Coordinates(데카르트 좌표계): 흔히들 우리가 아는 좌표계
	![150](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Cartesian-coordinate-system.svg/1200px-Cartesian-coordinate-system.svg.png)
- Coordinate는 Basis에 따라서 바뀐다. 무슨 이야기냐면 Basis가 바뀌면 다른 Unique한 Linear Combination으로 표현된다는 것이다.
	- $\mathbf{x} = \alpha_1\mathbf{b}_1 + \cdots + \alpha_n\mathbf{b}_n,\ Basis\ B = (\mathbf{b}_1, \cdots, \mathbf{b}_n)$
	- 이때 $\alpha = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix}$가 바로 Coordinate이다.
- `추가 정리 필요 (빠..빨라요 교수님)`

**Basis Change**
- $\mathbf{y} = \mathbf{B}'^{-1}\mathbf{bx}$
- `추가 정리 필요`
- Coordinate System이 Unique하다는 것은 Basis는 다양할 수 있으나, 각각의 Basis에 해당하는 Coordinate는 하나라는 것에 주의하자.

Example
$\mathbf{y} = \mathbf{B}'^{-1}\mathbf{bx}$를 이용해 풀면 된다. 자세한 것은 pdf 참고

**Transformation Matrix**
- 모든 Basis Vector에 대해서 Transformation을 나타내면 Matrix Multiplication의 형태로 나타낼 수 있다.
- $\hat{x}$는 $\mathbf{b}$로 표현된 좌표고, $\hat{y}$는 $\mathbf{c}$로 표현된 좌표라고 생각하면된다.
- 우리는 $\mathbb{R}\in n\times m$ 짜리 Transformation Matrix $\mathbf{A}_{n\times m}$를 만들 수 있다.

그럼 Matrix $\mathbf{A}_\Phi$의 Element의 각 요소는 어떻게 찾으면 될까?

Example 2.21 (Transformation Matrix)
`문제는 pdf 58p. 참고`
$$\mathbf{Ab_i} = \mathbf{c_i}$$

**Basis Change: Gerenal Case**
- Inter: 서로 다른 벡터공간 사이에서의 기저 대응 변화 $\mathbf{B} \rightarrow \mathbf{C}$, $\mathbf{B'} \rightarrow \mathbf{C'}$
- Intra: 같은 벡터공간 내에서의 기저변화 $\mathbf{B'} \rightarrow \mathbf{B}$, $\mathbf{C'} \rightarrow \mathbf{C}$
- $Theorem.$ $\mathbf{A'}_{\Phi} = \mathbf{T}^{-1}\mathbf{A}_{\Phi}\mathbf{S}$ (Matrix 순서 주의)
중요한 점은, 바로 변환하기 어려울 때 Theorem을 통해 우회하는 것이 가능하다는 것이다.

- Equivalence: $\mathbf{A'} = \mathbf{T}^{-1}\mathbf{AS}$
- Similarity: $\mathbf{A'} = \mathbf{S^{-1}}\mathbf{AS}$ `이 개념은 계속 나오니 잘 기억해두자.`
- Remark. Simillar $\subset$ Equivalance

Example 2.24 (Basis Change)
$$\tilde{A}_{\Phi} = \mathbf{T}^{-1}\mathbf{A}_{\Phi}\mathbf{S}$$

**Image and Kernel**
![300](../../Z.%20Docs/img/Pasted%20image%2020250325094842.png)
- $\mathbf{v}$ 안에 있는 vectore들을 $\Phi$에 태워서 $\mathbf{W}$로 보내버렸는데 0 vector가 되었을 때, 이를 Kernel이라고 부른다.
- $Def.$ $\ker(\Phi) := \Phi^{-1}(0_{W}) = \{ \mathbf{v}\in\mathbf{V}: \Phi(\mathbf{v}) = 0_{W} \}$
- Image/range: 그림에서 $\mathbf{w}\in\mathbf{W}$에 해당하는 벡터들의 set
- $\mathbf{V}$: Domain, $\mathbf{W}$: Codomain

**Properties**
- `추가 정리 필요`
- ★ 어떠한 Linear Transformation이던 Matrix $\mathbf{A}$로 표현할 수 있다.
- $\ker(\Phi)$는 $\mathbf{Ax} = 0$으로 표현되는 Homoheneous System의 Solution이다.

Example 2.25 (Image and Kernel of a Linear Mapping)
`문제 풀이 및 자세한 설명은 pdf 65p. 참고`

**Rank-Nullity Theorem**
`정리 필요`

## 2-8. Affine Spaces

**Affine Subspace**
- Affine Subspace는 Vector Subspace가 아니다!
- translation vector = support vector
- linear function: $f(x) = ax$
- affine function: $f(x) = ax + b$ (원점을 지나지 않는다.)

# 3. Analytic Geometry

## 3-1. Norms

**Norm**
$Def.$벡터의 길이 개념
$||\cdot||: \mathbf{V} \rightarrow \mathbb{R}$, 벡터에서 Real Number로 간다.
![100](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/TriangleInequality.svg/220px-TriangleInequality.svg.png)
- Absolutely Homogeneous: $||\lambda x || = |\lambda|\||x||$
- Triangle Inequality: $||x + y|| \le||x|| + ||y||$
- Positive Definite: $||x||\ge0 \text{ and } || x || = 0 \Longleftrightarrow x = 0$ `강의자료 오타`

![300](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLSz8eqkjnBK8fb4frSx-t1vviAIR7qjpC-A&s)
1. **Manhattan Norm ($\ell_1$ norm)**: $||x||_1 :==== \Sigma^{n}_{i = 1}|x_i|$
	- Matnhatten Norm이라고는 거의 안 한다.
2. **Euclidean Norm ($\ell_2$ norm)**: $||x||_z :==== \sqrt{\Sigma^{n}_{i = 1}{x_i^2}} = \sqrt{\mathbf{x}^{\top}\mathbf{x}}$
	- 원점으로부터 얼마나 Distance가 떨어져 있느냐를 얘기하는 것

## 3-2. Inner Projects ☆

추상 벡터 공간에서 정의된 벡터의 길이와 두 벡터 사이의 각도 또는 거리에 대하여 이야기 할 필요가 있다.

**Bilinear Mapping $\ohm$**
$Def.$ `정리 필요`
- Symmetric: 입력 순서를 반대로 바꿔도 결과가 같다.
- Positive Definite: Bilinear Mapping은 항상 0보다 커야하고 $\ohm(0, 0) = 0$이어야 한다.

**Inner Product**
다음 조건을 만족하는 어떤 Mapping을 우리는 그것을 Inner Product라고 한다.
1. $<\mathbf{u + v, w> = <u, w> + <v, w>}$
2. $\mathbf{<\lambda v, w> = \lambda<v, w>}$
3. $\mathbf{<v, w> = <w, v>}$
4. $\mathbf{<v, v>\ge0}$ and equal iff $\mathbf{v} = 0$
$(\mathbf{V}, <\cdot, \cdot>)$를 Innter Product Space라고 한다.

Example의 두 번째 예시를 보며 실제 조건을 적용해보며 왜 Inner Product인지 한 번 알아보자.
- Euclidean 공간
- $\mathbf{R}^2$
- 함수

**Symmetric, Positive Definite Matrix**
다음 조건을 만족하면 우리는 그것을 Symmetric하고 Potisive Definite인 Square Matrix라고 한다.
$$\forall x \in \mathbf{V} \backslash \{0\}: \mathbf{x}^{\top} \mathbf{Ax} >0$$
여기서 $\ge$면 Symmetrix, Positive Semidefinite이라고 한다.

e.g. 3.4 `66p.`
$\mathbf{x}^{\top} \mathbf{Ax}$로 정리하여 완전제곱식이 들어간 수식의 형태로 나타내어 양수 여부를 따져보면 된다.

`12p.`
$\mathbf{\hat{x}}^{\top} \mathbf{A\hat{y}}$
- x, y를 b의 Linear Combination으로 표현 했을 때 Inner Product 성질 상 덧셈인 Sigma와 스칼라 값은 밖으로 뺄 수있다. 결론은 x와 y의 좌표를 가져와서 A의 양옆에 곱해주면 된다. 이때 A는 Basis Vector로 구성된 Matrix이다. A가 Identity Matrix이면 우리가 아는 Dot Product의 정의가 된다.
- 앞에서는 Inner Product를 매우 추상적으로 정리했는데, Basis Vector를 기반으로 명확하게 정의할 수 있다는 것이다.
- 이 A를 어떻게 찾을 것이냐? 예를 들어 A를 Identity로 가정하고 Euclidean 공간이니 Dot Product를 하며 연구를 진행했는데 나중에 잘 관찰해보니 다른 공간인 것 같으면 A의 값을 바꿔야 한다는 소리이다.

`13p.`
- 내적의 결과값은 딱 하나의 숫자이다.
- Properties
	- 만약 $= 0$이면 A가 Positive Definite이 아니기 때문에 모순이 발생한다.
	- A의 대각 성분은 모두 양수이다. 아니면 A는 Positive Definite이 아닌 것이다.

## 3-3. Lenghts and Distances

**Length**
Inner Product는 Norm을 자연스럽게 유도한다.
$$||x|| := \sqrt{\mathbf{\langle x, x\rangle}}$$
하지만 모든 Norm이 이렇게 정의될 수 있는 건 아니다.
$\ell 2$ Norm은 다음과 같이 정의 가능하나, $\ell 1$은 정의될 수 없다.
$||x||_2 = \sqrt{\mathbf{x^{\top}x}}$

코시슈바르츠 부등식
$|\mathbf{<x, y>}| \le ||\mathbf{x}|| \ ||\mathbf{y} ||$

e.g. 3.5 (Lenghts of Vectors Using Inner Products) `pdf 81p.`
- different inner product에선 길이가 1이다. 즉, 다른 공간에서 잰 Norm은 달라질 수 있다는 것이다.

**Distance**
두 벡터의 차이의 Length
$$d(\mathbf{x, y}) := ||\mathbf{x - y}||$$
- $\sqrt{\mathbf{\langle x - y, x - y\rangle}}$로도 표현할 수는 있는데, 이는 Norm을 가끔씩 Inner Product로도 표현 가능하기 때문이다.
- 추가로 $\ell_1$ Norm으로로 $\ell_1$ Space에서 Distance를 정의할 수 있다.
- 일반적으로 다음을 만족하면 Distance의 적절한 개념인 Metric이라고 한다.
	- Positive Definite: $\forall x, y\ d(\mathbf{x, y})\ge 0$ and $d(\mathbf{x, y}) = 0\Longleftrightarrow \mathbf{x = y}$
	- Symmetrix: 이쪽에서 재든 저쪽에서 재든 똑같다는 뜻이다.
	- Triangle Inequality `위에서 한 거랑 거의 비슷한 것`
생각보다 Metric 같은데 Metric 같지 않은 경우도 많고, Distance도 마찬가지이다. 개념을 확실히 알고 가 헷갈리지 않도록 하자.

## 3-4. Angles and Orthogonality

**Angle, Orthogonal, and Otthonormal**
코시슈바르츠 부등식 $|\mathbf{<x, y>}| \le ||\mathbf{x}|| \ ||\mathbf{y} ||$ 으로부터 다음 수식을 끌어낼 수 있다.
$$-1\le \frac{\mathbf{\langle x, x\rangle>}}{\mathbf{||x||\ ||y||}} \le 1$$
이때 우리는 Unique한 $\omega \in [0, \pi]$가 존재한다는 것을 알 수 있다.
$$\cos{\omega} = \frac{\mathbf{\langle x, y\rangle}}{||\mathbf{x} ||\ ||\mathbf{y} ||}$$
$if\ \mathbf{\langle x, y\rangle>} = 0$, in other word $\omega = \frac\pi2$
- **Orthogonal** $\mathbf{x\bot y}$: 따라서 두 개의 벡터가 Orthogonal하다는 것은 두 개의 벡터를 내적했을 때 0이 나온다는 것이다.
- **Orthonormal**: 추가로 $||\mathbf{x}|| = || \mathbf{y}|| = 1$이면 Orthonormal이라고 한다.

e.g. 강의자료 참고

**Orthogonal Matrix**

$Def.$ A라는 Square Matrix의 Column(또는 Row)끼리 Orthonormal할 때 이 Matrix를 Orthogonal Matrix라고 한다.
$$\mathbf{AA^{\top}} = I = \mathbf{A^{\top}A}, \text{ implying }\mathbf{A}^{-1} = \mathbf{A}^{\top}$$
Orthogonal Matrix는 다음과 같은 성질을 만족한다.
- $\mathbf{A, B}$가 Orthogonal이면 $\mathbf{AB}$도 Orthogonal이다.
- $\mathbf{A}$가 Orthogonal이면 $\det(\mathbf{A}) = \pm 1$이다.
Orthogonal Matrix에 의한 Linear Mappig $\Phi$는 Length와 Angle이 보존된다.
`증명은 강의자료 참고`

## 3-5. Orthonormal Basis

**Orthonormal Basis**

## 3-6. Orthogonal Complement

**Orthogonal Complement (직교 여공간)**

$\mathbf{U}^\bot$

Normal Vector $\mathbf{w}$는 다음과 같은 벡터이다.
$\mathbf{||w||} = 1$이고 $\mathbf{U}$에 Orthogonal하여 $\mathbf{U}^\bot$의 Basis를 이룬다.

## 3-7. Inner Product of Functions

**Inner Product of Functions**
함수의 Inner Product는 다음과 같다.
$\langle u, v \rangle := \int^b_a u(x)v(x)dx$

## 3-8. Orthogonal Projections

큰 Data는 굉장히 High Dimensional하다.
우리는 중요한 정보는 Low Dimension에 있다고 가정하고 고차원에서 저차원으로 Mapping하고 싶은 것이다.
- *정보 누수를 최소한으로 하여 저차원으로 Mapping하는 것이 목표이다.*
딥러닝으로 저차원으로 보내는 방식을 학습하는 것이 지금의 주류 해결책이다.
그 전에는 낮추는 규칙을 사람들이 일일이 정했는데, 이를 Hand-crafted Feature라고 한다.
`Projection = Feature = Enbedding`

**Definition of Projection**

$Def.$ $\mathbf{U}\in\mathbf{V}, \pi:\mathbf{V\rightarrow U} \text{ and } \pi^2 = \pi \circ \pi = \pi$
- 두 번 Mapping 했을 때와 한 번 Mapping 했을 때의 결과가 같아야 한다.
- 즉, 투영된 점은 이미 그 공간 위에 있으니까 다시 투영해도 같은 점이 나온다는 의미이다.

**Projection onto Lines(1D Subspaces)**

$x$를 line에 Orthogonal하게 Projection해야 정보 손실이 가장 적다!
$$\mathbf{P}_\pi = \frac{\mathbf{bb^\top}}{||\mathbf{b}||^2}$$
우리가 아는 그 Projection 식이다.

**Inner Product and Projection**

$\langle\mathbf{x, b}\rangle = ||\pi_b(x)||\times ||b||$는 다음을 생각해보면 어쩌면 당연한 것이다.
$\vec{a}\times\vec{b} = ||a|| \ ||b||\cos\theta$

e.g.
`강의자료 31p. 참고`

**Projection onto General Subspaces**

$$\mathbf{P}_\pi = \mathbf{B(B^\top B)^{-1}B^\top}$$
유도 과정은 1D에서의 유도 과정과 비슷하며, 책 `71p.`에 나와있다.
- 여기서 $\mathbf{(B^\top B)^{-1}B^\top}$를 **Pseudo-Inverse**라고 한다. 왜냐하면 $\mathbf{B}$가 Inverse가 나오는 Square Matrix가 아닐 수도 있기 때문에 먼저 $\mathbf{B^\top B}$로 Square Matrix로 변환해주는 것이다.

e.g.
`강의자료 33p. 참고`

**Gram-Schmidt Otrhogonalization Method (G-S method)**



e.g.
`강의자료 35p. 참고`

## 3-9. Rotations

Remind. Orthogonal Matrix는 길이와 각도가 보존된다.

**Rotation**
Rotation이라는 특수한 Mapping은 Orthogonal Matrix로 나타낼 수 있다.
$$R(\theta) = \begin{pmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{pmatrix}$$

# 4. Matrix Decompositions

☑️ Matrix를 어떻게 요약하느냐: Determinant와 Eigenvalues

☑️ Matrix를 어떻게 Decomposition 하느냐: Cholesky Decomposition, Diagonalization, Singular Value Decomposition

☑️ Matrix를 어떻게 Approzimate 하느냐

Flow Chart
![300](../../Z.%20Docs/img/Pasted%20image%2020250403093038.png)

## 4-1. Determinant and Trace

**Determinant**
$$\begin{vmatrix}a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}\end{vmatrix}$$
Laplace Expansion
- $n\times n$ 행렬의 행렬식을 특정 행 또는 열을 기준으로 전개해서 계산하는 방법
- 다시 말해, 어떤 행이나 열을 선택해서, 그 안의 각 원소에 대해 소행렬식(minor)과 부호(cofactor)를 곱해 전체 행렬식으로 만드는 방식
$$det⁡(A)= \sum_{j=1}^{n} (-1)^{i+j} \cdot a_{ij} \cdot \det(M_{ij})$$
e.g. 4.3 (Laplace Expansion)
`교재 108p. 참고`

이렇듯 행렬이 커지면 Determinant 계산하는 것은 어려워진다. `단순히 계산할게 많아진다.`

**Determinant: Properties**

Determinant의 성질들은 다음과 같다:
`강의자료 10p. 참고`
- Triangular Matrix에서 Determinant는 대각성분을 모두 곱한 것과 같다.
- 어떤 행 또는 열에 대해, 그 행(열)에 다른 행(열)의 배수를 더해도 Determinant는 변하지 않는다.

**Trace**
$$tr(\mathbf{A}):= \sum^n_{i = 1} a_{ii}$$
- $tr(\mathbf{AB}) = tr(\mathbf{BA}), tr(\mathbf{AKL}) = tr(\mathbf{KLA})$ `Cyclic Turmination`
- $tr(\mathbf{xy^{\top}}) = tr(\mathbf{y^{\top}x}) = y^{\top}x$ `Dot Product`
`나머지 특징은 강의자료 12p. 참고`

**Characteristic Polynomial**
$$\begin{align}
p_{\mathbf{A}}(\lambda) &:= \det(\mathbf{A - \lambda I}) \\
&= c_0 + c_1\lambda + c_2\lambda^2 + \cdots + c_{n-1}\lambda^{n - 1} + (-1)^n\lambda^n \\
\text{where } c_0 &= \det(\mathbf{A}) \text{ and } c_{n-1}{ = (-1)^{n-1}}tr(\mathbf{A})
\end{align}$$
- 이게 왜 필요하느냐? Eigenvalue와 Eigenvector를 구할 때 필요하기 때문이다.

## 4-2. Eigevalues and Eigenvectors

**Eigenvalue and Eigenvector**
$$\mathbf{Ax} = \lambda\mathbf{x}$$
- Non-trivially 하게 푼다는 것은, 명확한 해인 $\mathbf{x} = 0$을 보지 않고 풀겠다는 것이다.
- Invertable := det != 0
- $\det(\mathbf{A - \lambda I_n}) = 0 \Leftrightarrow p_{\mathbf{A}}(\lambda) = 0$

e.g.
-> Eigenvector는 Unique하지 않고 무한하게 있다.

**Eigenvalue and Eigenvector: Properties**

다음과 같은 특징들을 가지고 있다.
- Collinear: 같은 직선 위에 있는 벡터들
- Eigenspace는 $\mathbf{A - \lambda I}$의 Kernel이다.
- 기하적으로 해석하면 0이 아닌 고윳값에 해당하는 고유벡터는 선형 변환에 의해 늘어나는 방향을 가리킨다. 여기서 고윳값은 길이가 얼마나 바뀌느냐를 나타내는 Factor이다.
- Eigencalue는 Transformation Matrix에 무관하다.

>> Basis가 바뀌어도 Determinant, Trace, Eigenvalue들은 안 바뀐다.

**Examples for Geometric Interpretation**

Remind:
- Eigenvalue: 얼마나 Vector가 늘어나는지/줄어드는지
- Eigenvector: 어느 방향이 유지되는지
- Determinant: 면적이 얼마나 확대/축소되었는지
	- $\det(A) = 1$: 면적 보존
	- $\det(A) = 0$: 면적 소멸

![200](../../Z.%20Docs/img/Pasted%20image%2020250417161728.png)![265](../../Z.%20Docs/img/Pasted%20image%2020250417161746.png)

(1) $\lambda_1 = \frac12, \lambda_2 = 2$
- $\det(A) = 1$: Area 보존
- 한 방향은 축소되고 한 방향은 늘어난다.

(2) $\lambda_1 = \lambda_2 = 1$
- $\det(A) = 1$: Area 보존
- Shearing: 형태는 왜곡되지만 면적은 유지된다.

(3) Only Complex Eigenvalues
- $\det(A) = 1$: Area 보존

(4) $\lambda_1 = 0, \lambda_2 = 2$
- $\det(A) = 0$이라는 것은 면적이 0으로 축소된다는 것이다.
- 한 Eigenvalue가 0이라는 것은 어떤 방향으로 완전히 납작하게 눌린다는 것이다. `2D -> 1D`
- Projection Matrix라고도 볼 수 있을 것이다.

(5) $\lambda_1 = 0.5, \lambda_2 = 1.5$
- $\det(A) = 0.75$이라는 것은 면적이 75%로 줄어든다는 것이다.
- Shearing + Stretching: 비대칭 적인 Scale이다.

**Eigenvalue and Eigenvector: Properties.Cont**

- Eigenvalue가 겹치지 않는다면 Eigenvector는 Linearly Independent하다.
	- 하지만 n개의 Linearly Independent한 Eigenvector들을 가지고 있다하더라도 n개의 별개의 Eigenvalue를 가지고 있는 것은 아니다. `반례: I`
- Denterminant는 Eigenvalue의 곱으로 나타낼 수 있다. `Trivial 하지는 않다.`
- Trace는 Eigenvalue의 합으로 나타낼 수 있다. `Trivial 하지는 않다.`
- Determinant는 Area Scailing, Trace는 Circumference Scaling이다. `엄밀한 이야기가 아니니 넘어가도 됨`

## 4-3. Cholesky Decomposition

이제부터 본격적으로 Decomposition에 대한 이야기로 넘어가보자.

**LU Decomposition**

LU의 의마는 각각 다음과 같다:
- L: Lower Triangular 
- U: Upper Triangular `Gaussian Elimination`

Gaussian Elimination을 할 때 쓰는 Operation들을 Matrix로 쓸 수 있다.
$$(\mathbf{E}_k\mathbf{E}_{k-1}\cdot\mathbf{E}_1) \mathbf{A} = \mathbf{U} \Rightarrow \mathbf{A} = (\mathbf{E}_1^{-1}\mathbf{E}_2^{-1}\cdots\mathbf{E}_k^{-1}) \mathbf{U} = \mathbf{LU}$$
- Lower Triangular Matrix의 Inverse도 Lower Trianguler Matrix이다. Trivial하진 않지만 그런 성질이 있다.
- 여기서 $\mathbf{A}$는 Square Matrix여야만 한다.

**Cholesky Factorization**

Remind, Positive Definite Matrix $\mathbf{A}$:
- 모든 0이 아닌 Vector $\mathbf{x}$에 대하여 Quandratic Form이 항상 양수인 Matrix
$$\mathbf{x}^{\top}\mathbf{A}\mathbf{x} > 0 \quad \forall\mathbf{x} \in \mathbb{R}^n \backslash \{0\}$$

Symmetric Positive Definite Matrix $\mathbf{A}$가 있을 때, $\mathbf{A} = \mathbf{LL^\top}$로 나타낼 수 있다.
이는 $16 = 4 \times 4$와 같이 제곱근을 구하는 것과 비슷한 것이라고 생각하면 된다.
- $\mathbf{L}$: Cholesky Factor, Unique
- $\mathbf{L}$은 양수의 Diagonal로 이루어진 Lower Triangular Matrix이다.

Cholesky Factorization의 기증은 다음과 같다:
- Covariance Matrix의 Factorization
- Random Variable의 Linear Transformation
- 빠른 Determinant 계산

e.g. 4.10
`교재 120p.`

## 4-4. Eigendecomposition and Diagonalization

**Diagonal Matrix**
Diagonal Matrix $\mathbf{D}$ 에 대해선 $\mathbf{D}^k$도, $\mathbf{D}^{-1}$도, $\det(\mathbf{D})$도 구하기 쉽다. 이는 매우 중요한 성질이다.
- $Def.$ Diagonalizable 하다는 것은 A가 D와 Similar 하다는 것을 의미한다.
	- $\mathbf{D} = \mathbf{P}^{-1}\mathbf{A}\mathbf{P}$
	- Similar는 Basis Change의 개념이다.
- $Def.$ Orthogonally Diagonalizable 하다는 것은 위에서 추가로 P가 Orthogonal 할 때를 의미한다.
	- $\mathbf{D} = \mathbf{P}^{-1}\mathbf{AP} = \mathbf{P}^{\top}\mathbf{AP}$

**Power of Diagonalization**

Diagonalizable한 Matrix를 제곱하는 것은 쉽다.
Determinant도 구하기 쉽다. `Determinant of Inverse`

그럼 $\mathbf{A}$가 Diagonalizable 한지 어떻게 알고, $\mathbf{P}$와 $\mathbf{D}$는 어떻게 구하면 좋을까?

**Diagonalizablity, Algebraic/Geometric Multiplicity**

Eigenvalue가 Colinear할 수 있는 이상,
Algebric Multiplicity와 Geometric Multiplicity의 숫자가 다르게 나올 수도 있다.
$$\alpha_i \neq \zeta_i$$

**Orthogonally Diagonaliable and Symmetric Matrix**

Theorem. Square Matrix A에 대하여 다음이 성립한다:
- Orthogonally Diagonalizable $\Leftrightarrow$ Symmetric

Spectral Theorem. A가 Symmetric하면 다음이 성립한다:
- Eigenvalues $\in \mathbb{R}$
- 서로 다른 Eigenvalue에 대응하는 Eigenvector는 서로 직교한다.
- 전체 Vector Space $\mathbb{R}^n$에 대해 직교 Eigenvector로 이루어진 Basis가 존재한다.

> 결론은 $\mathbf{A}$가 Symmetric하면 그것에 대한 Eigenvector들의 Orthonormal Basis를 구축할 수 있다는 것이다.

$$\mathbf{A} = \mathbf{Q}\Lambda\mathbf{Q}^\top$$
Example 4.8
`강의자료 pdf 31p. 참고`

**Eigendecomposition**

이제 위에서 배운 것들을 Eigendecomposition을 하는 데 쓸 수 있다.

A의 Eigenvector들이 n차원 Basis를 구성한다는 것이다.
$\mathbf{AP}=\mathbf{PD}$: $\mathbf{A}$가 $\mathbf{P}$의 Column Vector들을 Eigenvetor로 갖는다는 뜻이다.

즉 시각적으로 말해보자면 다음과 같다:
- Matrix $\mathbf{A}$는 Basis(=Eigenvector) 뱡향으로만 늘리고 줄이는 변환으로 분해 가능하다.
- 그 방향과 크기를 분리해서 본 것이 바로:
$$\mathbf{A} = \mathbf{P}\underbrace{D}_{\text{늘리기/줄이기}}\mathbf{P}^{-1}$$

## 4-5. Singular Value Decomposition

# 5. Vector Calculus

# Midterm

Recommended Exercises: `교재 pdf 69p. -`

---

~~2.4(d), 2.5(a), 2.6, 2.8(a), 2.9(a,b)~~
~~2.10(a), 2.16(a,c), 2.17, 2.1, 2.3~~
~~2.4, 2.5, 2.6, 2.8, 2.9, 2.10~~
~~2.12, 2.16, 2.17, 2.20 3.2~~
~~3.8, 4.1, 4.7, 4.8,~~
~~3.1, 3.4, 3.5~~
~~4.3, 4.12~~
~~5.2, 5.7, 5.1,~~ 5.5

4.7(d)

---

#### Cheating Sheet

**Vector Subspace의 증명 `2.9`**
어떤 집합 $S \subset \mathbb{R}^n$가 Subspace가 되려면:
1. 0벡터가 포함되어야 한다. $\vec{0} \in S$
2. 덧셈에 대해 닫혀 있어야 한다. $\vec{u}, \vec{v} \in S \Rightarrow \vec{u} + \vec{v} \in S$
3. 스칼라곱에 대해 닫혀 있어야 한다. $\vec{u} \in S, c \in \mathbb{R} \Rightarrow c \vec{u} \in S$

**Linear Mapping의 증명 `2.16`**
다음과 같은 성질을 만족하면 Linear Mapping이라고 얘기할 수 있다:
- $\Phi(\mathbf{x + y}) = \Phi(\mathbf{x}) + \Phi(\mathbf{y})$
- $\Phi(\lambda\mathbf{x}) = \lambda\Phi(\mathbf{x})$

**Kernel과 Image 공간, 그리고 그의 Dimension 구하기 `2.17`**
- $\ker(\Phi)$: $\mathbf{Ax}=0$의 Solution Space
- $\text{Im}(\Phi)$: Linear Mapping Matrix A의 Column Vector들로 Span한 공간.
- $\dim(\text{Im}(\Phi))$의 값은 Linearly Independent한 Column Vector의 개수이다. 즉, Row Echelon Form으로 나타냈을 때의 Pivot Column의 개수이다.

**Abelian Group의 정의 `2.1`**
Set $\mathcal{G}$에서 연산자 $\otimes$에 대해서 다음과 같은 법칙들이 성립하면 Abelian Group이라고 한다:
- Closure
- Associativity
- Neutral Element
- Inverse Element
- Communicativity

**Low-Echelon Form으로 변형한 뒤 Free Variable Column을 추가할 때 `2.5`**
- Pivot을 -1로 하여 추가하면 Free Variable에 해당하는 Row를 그대로 사용하여 Solution을 형성할 수 있다.
- Solution = $b' + \lambda_1\cdot A'_{\lambda_1} + \cdots$

**Square Matrix의 Invertible 판단 `2.8`**
행렬 $\mathbf{A}_{n\times n}$가 Square Matrix일 때 A가 Invertible하려면 다음 중 하나가 성립해야한다:
- $\det(\mathbf{A}) \neq 0$
- $\text{rank}(\mathbf{A}) = n$
- $\ker(\mathbf{A}) = \{ 0 \}$

**Basis Change `2.20`**
- $\mathbf{T}_{B \rightarrow B'} = B'^{-1}B$
- $\mathbf{T}_{B' \rightarrow B} = B^{-1}B'$
- B' -> C'의 우회 Transformation Matrix를 구할 때 Matrix를 곱하는 순서에 주의! `오른쪽에 있는 Matrix를 먼저 곱한다.`

**Inner Product `3.2`**
다음 조건을 만족하는 어떤 Mapping을 우리는 그것을 Inner Product라고 한다:
1. $<\mathbf{u + v, w> = <u, w> + <v, w>}$
2. $\mathbf{<\lambda v, w> = \lambda<v, w>}$
3. $\mathbf{<v, w> = <w, v>}$
4. $\mathbf{<v, v>\ge0}$ and equal iff $\mathbf{v} = 0$

**Gram-Schmidt `3.8`**
- $u_1 = v_1$
- $u_2 = v_2 - \frac{<u_1, v_2>}{<u_1, u_1>}u_1$ ($a - \frac{b\cdot a}{b\cdot b}b$, $b$는 앞에서 계산한 거)
- $P_{\pi} = \frac{bb^{\top}}{||b||^2}$

**Determinant 계산하기 `4.1`**
- Laplace Expansion
- Sarrus Rule: $3 \times 3$ 행렬에서만 성립, `+ + + - - -`

**Diagonalize `4.7`**
- 판별법: $\sum$ Algebric Multiplicity = $\sum$ Geometric Multiplicity = n
	- Algebric Multiplicity: 다항식에서 고유값이 다타나는 중복도
	- Geometric Multiplicity: 해당 고유값에 대한 고유벡터의 개수

**Diagonalization $\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1}$**
- 조건: Square Matrix이고, Diagonalize 가능해야 한다.

**Eigen Decomposition $\mathbf{A} = \mathbf{Q}\Lambda\mathbf{Q}^\top$**
- 조건: Square Matrix이고, 반드시 Diagonalize 가능해야할 필요는 없다.

**Singular Value Decomposition $\mathbf{A} = \mathbf{U}\Sigma\mathbf{V}^{\top}$**
- 조건: 어떤 형태의 행렬이던 가능하다.
- U랑 V는 Column Vector들이 정답과 $\pm$만 다르다면 정답이다. `Basis이므로 방향 상관 없음`
- $V^{\top}$은 Eigenvector Column Vector로 이루어진 $V$를 Transpose한 것임에 유의하자.
- $A^{\top}A$를 하는 것은 오른쪽 특이벡터 행렬인 $V$를 구하기 위함이다. $AA^{\top}$을 하면 왼쪽 특이벡터 행렬 $U$를 구하게 된다.

**Norm과 코시슈바르츠 부등식을 이용한 Angle 계산 `3.4`**
- $||x|| = \sqrt{<x, x>}$
- $-1 \leq \cos\omega = \frac{<x, y>}{||x||\ ||y||} \leq 1$

**Vector x를 Vector Space U에 Projection할 때 `3.5`**
- U의 Basis Vector 모음인 B
- $P_{\pi} = B(B^{\top}B)^{-1}B^{\top}$이긴 한데..
- $B^{\top}(x - B\lambda) = 0$, 즉 내적이 0인 걸 이용한다.
- Projection 된 Vector Space U 상의 Vector는 $B\lambda$이다.

**Eigenspace 구하기 `4.3`**
- Eigenvalue가 $\lambda$인 Eigenspace의 표기: $E_\lambda = \text{span}(u_1, \cdots)$

**Echelon Form, Reduced Echelon Form**
- Echelon Form: Pivot 아래만 0이면 된다.
- Reduced Echelon Form: Pivot이 1이여야만 하고, 위아래가 모두 0이여야 한다.

# 6. Probability and Distributions

## 6.1 Construction of a Probability Space

## 6.2 Discrete and Continuous Probabilities

## 6.3 Sum Rule, Product Rule, and Bayes' Theorem

## 6.4 Summary Statistics and Independence

**Properties**

**Correlation Coefficient: Bounded Dimensionless Metric**
- $|\rho| = 1$: 어마하게 강하게 관계되어있다.
- $|\rho| = 0$ 반대로 0이면 관계되어있지 않다는 뜻이다.

**Expectation, Covariance, Variance**
- Covariance Matrix는 매우 흔하게 등장하는 Tool이기 때문에 익숙해질 필요가 있다.

**Data Matrix and Data Covariance Matrix**

Data: Unknnown Distribution에서 Sampling된 결과
N: 샘플의 개수 `N개의 이미지`
D: 측정 수 `of original features`
- 저차원의 특징을 뽑아냈을 때, 그 특징의 개수가 몇 개냐는 것이다.
- 머신러닝에선 30,000에 달하는 feature들을 2,048개 정도로 줄인다.
- 딥러닝에선 이 measurement가 해석이 안 된다.
iid 조건: Sample들이 독립적으로 뽑혔고 같은 분포에서 나왔다라는 조건이다.
- 모양이 다 다른 주사위를 던진 것은 iid가 아니다.
가로: Sample Index, 세로: Feature Index

(Data) Covariance Matrix:
$$S = \frac{1}{N}XX^\top$$
- 데이터의 공분산(Cavariance) 정보를 담고 있는 행렬

**Covariance Matrix and Data Covariance Matrix**

$S_{ij}$: Feature i와 j사이의 Covariance의 평균값을 나타낸다.

**Properties**

`교재 pdf 참고`

## 6.5 Gaussian Distribution

**Normal(Gaussian) Random Variable**

정규 분포

왜 중요한가?
- Central Limit Theorem (중심극한정리)와 관련이 있기 때문이다. 대충 시행을 여러번 하면 그 분포는 정규 분포를 따른다는 내용이다.
- 딱 두 개, Mean과 Variance와만 관련있기 때문에 매우 간단하다.

$$\mathcal{N}(0, 1)$$
$$\mathcal{N}(\mu, \sigma^2)$$
**Gaussian Random Vector**

Parameter가 두 개이기 때문에 Joint pdf를 갖는다.
$$X \sim \mathcal{N}(\mu, \Sigma) \quad \text{or} \quad p_X(\mathbf{x}) = \mathcal{N}(\mathbf{x} | \mu, \sigma)$$- X가 Normal Distribution을 따른다.

**Marginals and Conditionals of Gaussians**

Marginal: 
Conditional: 

Example 6.6 `pdf 206p.`
	Conditional을 구하는 수식을 쓰면 된다.
	중요한 건 왜 이 수식이 도출됐느냐지, 수식을 적용하여 계산하는 건 쉽다.
	Geometrical: $x_2 = -1$ 선 위에 있는 값들만 보고 분석을 하겠다는 뜻이다. `Conditional Distribution`
	$\mathcal{N}(0, 0.3)$: 적분했는데 어떻게 한 번에 나올까? `Gaussian의 편리성`

**Product of Two Gaussian Densities**

$$
\begin{align}
\mathcal{N}(\mu_0, v_0),\ \mathcal{N}(\mu_1, v_1) \\
\mathcal{N}(\frac{v_1\mu_0 + v_0\mu_1}{v_0 + v_1}, \frac{v_0v_1}{v_0+v_1})
\end{align}
$$
- 외울 필요는 없고, Gaussian Product 역시 Gaussian이라는 사실만 기억하면 된다.

**Sum of Gaussians**

**Mixture of Two Gaussian Densities**

$$f(\mathbf{x}) = \alpha f_1(\mathbf{x}) + (1 - \alpha)f_2(\mathbf{x})$$
- Coefficient의 합이 1인 것을 Mixture관계라고 한다.
- 그리고 그런 Mixture 역시 Gaussian이다.

**Linear Tranformation**

## 6.6 Conjugacy and the Exponential Family

**Conjugate Prior: Motivation**

Likelihood와 Prior가 Model로부터 온다.
Conjugate Prior: 우리가 원하는 것은 Posterior가 Liklihood과 같으려면 Prior가 무엇이어야 하냐는 것이다.
- 계산이 때때로 쉬울 수 있다.
- 하지만 Prior(분포에 대한 믿음)가 제한될 수 있다.

**Conjugate Priors: Definition and Examples**

어떤 분포(Lieklihood)를 선택해야하는지는 우리가 풀고자 하는 Test에 따라서 상이할 수 있다.

| Likelihood  | Prior                 | Posterior             |
| ----------- | --------------------- | --------------------- |
| Poisson     | Gamma                 | Gamma                 |
| Bernoulli   | Beta                  | Beta                  |
| Binomial    | Beta                  | Beta                  |
| Normal      | Normal/inverseGamma   | Normal/inverseGamma   |
| Normal      | Normal/inverseWishart | Normal/inverseWishart |
| Exponential | Gamma                 | Gamma                 |
| Multinomial | Dirichlet             | Dirchlet              |

**Beta Distribution**

**Example: Beta-Binomial Conjugacy**

**Sufficient Statistics**

**Poisson Example**

## 6.7 Change of Variables / Inverse Transform

# 7. Optimization

## 7.1 Optimization Using Gradient Descent

**Unconstrained Optinization and Gradient Algorithms**
$$\min f(x), \quad f(x) : \mathbb{R}^n \mathbb \mapsto {R}, \quad f\in C^1$$
- Gradient-type Algorithms: $x_{k+1} = x_k + \gamma_k d_k, \quad k = 0, 1, 2, \cdots$
- Steepest Gradient Descent: 어떠한 방향으로 가던 간에 이것보단 작을 수 없다.
- $\gamma_k$의 적당한 값을 찾는 것이 어려운 부분이다. 아름다움 수식 같은 게 있는 게 아니라 보통 노가다로 찾는다.

**Example**
$$x_{k+1} = x_k - \alpha \nabla f(x_k)$$
- $\alpha$: Learning Rate

**Taxonomy**
$L(\theta, x)$

Batch Gradient Descent
$$d = \frac1N\sum^{n}_{i = 1}\Delta L(\theta, x_i)$$
- Mini-batch는 Random하게 n개만 뽑아서 한다.
- Stocahstic은 하나만 보는 것

**Stochatic Gradient Descent (SGD)**

Batch Gradient가 사실 가장 정확한거고,
Mini-batch나 Stochastic은 연산량 상 합의를 본 것이다.
- $\gamma_k$: Learning Rate

Gradient가 0이면 아무리 많이 업데이트를 해도 제자리에 멈춰서 탈출할 수가 없다.
하지만 Noisy한 Gradient를 쓰면 탈출할 수 있기에 이를 더 선호하는 경향이 있다.

## 7.2 Constrained Optinization and Lagrange Multipliers

## 7.3 Convex Sets and Functions

## 7.4 Convex Optimization

## 7.5 Convex Conjugate