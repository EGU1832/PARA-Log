기초머신러닝

# 0. Syllabus

AI를 전공하기 위해 반드시 알아야 할 Math Concept들은 다음과 같다.
- Linear Algebra ★
- Analytic Geometry ★
- Matrix Decomposition ★
- Vector Calculus
- Probability and Distributions
- Optimization

그리고 기초적인 ML Problem들은 다음과 같다.
- When Models Meet Data `Linear Mapping`
- Dimensionality Reduction with Principal Component Analysis `차원축소 with PCA`
- Density Estimation with Gaussian Mixture Models `분포 예측 with GMM`
- Claasification with Support Vector Machines `분류 with SVM`

본 강의에서 배우는 ML의 기본 토대 4가지는 다음과 같다.
- Regression
- Dimensionality Reduction
- Density Estimation
- Classification

따라서 강의자료로만 공부하면 부족할수도 있으니, 교재도 보기를 바란다.
`Mathematics for Machine Learning1, Cambridge University Press, Marc Peter`

# 2. Introduction

Notation
- Sets: $\mathcal{A, B, C}$

머신러닝의 알고리즘은 일반 알고리즘들과 접근방식이 다소 다르다.
머신러닝의 알고리즘은 주어진 Data로부터 Valuable Information을 추출하는 것이 주요 목적이다.

ML의 주요 3가지 Concept는 다음과 같다.
1. Data: Data를 생성해내는 어떤 Process에 관한것
2. A Model: ML에 필요한 중요한 특징 중 하나는 내가 가지고 있던 Data Set 뿐만이 아니라 Unseened Data로부터 유의미한 정보값을 추출해낼 수 있는 것이다. Application에 따라 이 Unseen Data의 범위를 정하는 것 또한 중요하다고 할 수 있겠다.
3. Learning: Data에 있는 패턴과 구조를 자동적으로 이해할 수 있게끔 모델의 Parameter를 최적화하는 것이다.
항상 이 3가지가 동등하게 중요하다는 것을 앞으로 ML을 공부하며 잊으면 안될 것이다.

어느 학문이나 그렇겠지만, 근본적인 이해를 위해선 수학적인 Background가 중요하다.

ML에서 알고리즘이란 무엇이냐?
- 예를들어 "ML Algorithm"이라 얘기할 때 Input Data를 기반으로 예측값을 뽑아내는 시스템을 가리킬 때가 있다. 즉, 우리가 생각하는 알고리즘과 조금 상이하다.
- Training Proces를 의미할 때도 있다.
그럼 뭐가 뭔지 어떻게 판단하냐? Context상으로 이해할 수 밖에 없다.

#### Data

본 강의에서, 우리는 Data를 컴퓨터에 쉬이 학습될 수 있도록 이미 수치적으로 표현된 **벡터**라고 생각할 것이다.
```
Data == vector
```

그렇다면 **벡터**란 무엇이라고 생각하면 되느냐?
- CSE View: 숫자의 배열
- PHY View: 방향과 크기를 가지는 화살표
- MAT View: 덧셈과 Scaling을 따르는 객체
그때 그때 Context에 따라 알아서 잘 이해하면된다.

#### Model

좋은 모델이란,
- 실제 사고방식을 얼마나 잘 단순화하여 모방했느냐에 따라 좌우된다.
- 또한 사전학습 없이 Real-World Experiments를 얼마나 잘 예측하는지

#### Learning

Training이란, Model의 Parameter들을 최적화하는 것이다.
- 이는 모델이 Training Data에 대하여 얼마나 잘 예측하는지를 평가하는 Utility 함수를 기반으로 이루어진다.
- 계속 반복해서 말하지만, **Training Data가 아닌 Unseened Data에 대하여 좋은 성능을 내는 것**이 중요하다.

# Part I: Math

# 2. Linear Algebra

## 2-1. Systems of Linear Equations

## 2-2. Matrices

$a_{\text{ Row Index | Column Index }}$

$\mathbf{Ax} = \mathbf{b}$
Column Vector: $\begin{pmatrix} a_{11}\\\vdots\\a_{m1} \end{pmatrix} \mathbf{x_1}$

Column - 세로, Row - 가로

Reshape: $\mathbb{R}^{m\times n} \rightarrow \mathbb{R}^{mn}$

$\begin{pmatrix}1 & 2 & 3 \\ 3 & 2 & 1\end{pmatrix} \times \begin{pmatrix}0 & 2 \\ 1 & -1 \\ 0 & 1\end{pmatrix} = \begin{pmatrix}2&2\\3&5\end{pmatrix}$

Identity Matrix: $\mathbf{I}_n$, 문맥 상 크기가 확실하면 굳이 n을 표기할 필요는 없다.

Matrix Properties
- 결합법칙: $\mathbf{(AB)C} = \mathbf{A(BC)}$
- 분배법칙: $\mathbf{(A+B)C} = \mathbf{AC + BC} \neq \mathbf{CA + CB}$
- 항등원: $\mathbf{I}_m\mathbf{A} = \mathbf{A}\mathbf{I}_n = \mathbf{A}$

Inverse $\mathbf{AA}^{-1} = \mathbf{I}_n = \mathbf{A}^{-1}\mathbf{A}$: **Regular**/Inbertible/**Nonsingular**
- Unique ★
- Matrix $\mathbf{A}$가 $2\times 2$면 $\mathbf{A}^{-1} = \frac{1}{a_{11}a_{22} - a_{12}a_{21}}\begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11}\end{pmatrix} = \frac{1}{\det}\begin{pmatrix}a_{22} & -a_{12} \\ -a_{21} & a_{11}\end{pmatrix}$
Transpose
- $\mathbf{A} = \mathbf{A}^{\top}$이면 **Symmetric**이라 한다.

More Matrix Properties `막 그렇게 당연하지는 않은 것들`
- $(\mathbf{A} + \mathbf{B})^{-1} \neq \mathbf{A}^{-1} + \mathbf{B}^{-1}$
- $\mathbf{A}^{-\top} = (\mathbf{A}^{-1})^{\top} = (\mathbf{A}^{\top})^{-1}$

Matrix의 Scalar Multiplication을 할 때 $\lambda \times \mathbf{A}$처럼 $\times$를 쓰기도 한다.
이건 그냥 직관적으로 결합법칙이나 분배법칙을 사용해도 무관하다.

## 2-3. Solving Systems of Linear Equations

식을 세우고, Gaussian Elimination등을 통해 해를 구하면 된다.

General Solution
![250](../../z.%20Docs/img/Pasted%20image%2020250313093530.png)
- z를 어떻게 Sampling 했느냐에 따라 결과가 다양해진다.

General Solution = Particular Solution + Homogeneous Solution
![350](../../z.%20Docs/img/Pasted%20image%2020250313093709.png)

ML은 진짜로 $\mathbf{Ax = b}$를 푸는 게 다이다.
- 이 Equation을 푸는 것을 컴퓨터 알고리즘으로 만들기 위해 Gauss-Jordan Method등을 정의하는 것이다.

Row-Echelon Form, Reduced Row-Echelon Form
![300](https://dmn92m25mtw4z.cloudfront.net/img_set/la-1-4-x-eq-8/v1/la-1-4-x-eq-8-460w.jpg)
- Pivot은 언제나 그 아래 행의 Pivot의 왼쪽에 있어야 한다.

Calculating Inverse

$$
\begin{pmatrix}
1 & 0 & 2 & 0 & | & 1 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & | & 0 & 1 & 0 & 0\\
1 & 2 & 0 & 1 & | & 0 & 0 & 1 & 0\\
1 & 1 & 1 & 1 & | & 0 & 0 & 0 & 1
\end{pmatrix}
$$
$$
\begin{pmatrix}
-1 & 2 & -2 & 2 \\
? & ? & ? & ? \\
? & ? & ? & ? \\
? & ? & ? & ? \\
\end{pmatrix}
$$

Algorithms for Solving System of Linear Equations

(1) **Pseudo-Inverse** $(\mathbf{A}^{\top}\mathbf{A})^{-1}\mathbf{A}^{\top}$
이게 문제가 뭘까?
Inverse가 들어가있기 때문에 연산량이 어마어마해서 실제로 쓰기가 너무 어렵다는 점이다.
따라서 좋아보이긴 하지만, 실제로 쓰기는 어렵다는 점.

(2) Gaussian Elimination
- 바로 한 번에 구해지지 않기 때문에 Intuitibe and Constructive 방법이라고 한다.
- Cuic Complexity

(3) Optimization
- Gradient se

## 2-4. Vector Spaces

**Group**
- Set 안에서 임의로 뽑은 $x, y$에 대해서 $x \otimes y$가 $\mathcal{G}$ 에 속해야 한다.
- Set 안에서 임의로 뽑은 $x, y$에 대해서 연산 순서가 결과에 영향을 미치면 안된다.
- 항등원이 존재해야한다.
- 역원이 존재해야한다.

**Abelian Group**
- Set $\mathcal{G}$에서 연산자 $\otimes$에 대해서 교환법칙이 성립하면 Abelian Group이라고 한다.

어떤 게 Group, Abelian Group을 만족하는지 판단할 수 있어야 한다.

# 3. Analytic Geometry

