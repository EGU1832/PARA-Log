인공지능:딥러닝

# 0. Syllabus


📗 Book Recommended
- Main Book - A. Zhang, Z. C. Lipton, M. Li and A. J. Smola, "Dive into Deep Learning", Cambridge University Press (2024)
- Probability - 8. D. P. Bertsekas and J. N. Tsitsiklis, "Introduction to Probability", 2nd Ed., Athena Scientific (2008)
- Optimization - 9. D. G. Luenberger, "Linear and Nonlinear Programming", 2nd. Ed., Addison-Wesley (1984)

본 강의에선 코딩 실습은 진행하지 않고 이론 위주로 수업이 진행된다.

**Classical Machine Learning**은 구조화된 데이터와 비교적 단순한 패턴을 학습하는 데 강점이 있지만, 이미지 인식이나 자연어 처리 같은 복잡한 실생활 문제에서는 한계를 드러냈다.
**Deep Learning**은 이런 한계를 극복하기 위해 발전한 기술로, 대량의 데이터와 강력한 연산 자원을 활용해 복잡한 패턴을 학습하고 높은 성능을 내는 모델을 만들 수 있게 한다.

초기에는 주로 CPU를 사용해 머신 러닝 모델을 학습했지만, 계산량이 많아지면서 병렬 연산에 최적화된 GPU가 활용되기 시작했다. 그 속도는 무려 50배 가까이 향상되었다.

과거에는 Raw Data(원본 데이터)를 사람이 직접 정제하고 특징을 추출한 후, 머신러닝 모델이 학습하는 방식이었다. 즉, 데이터에서 의미 있는 Feature(특징)를 사람이 정의하고 가공하는 과정이 필요했다. 하지만 Deep Learning에서는 **End-to-End Learning**이 가능해지면서, 원본 데이터를 최소한의 전처리만 거친 후 모델에 입력하면, 모델이 스스로 유의미한 특징을 학습하도록 바뀌었다.

즉, "Raw Data → Feature Engineering → Learning"에서  
"Raw Data → Learning (모델이 직접 특징을 학습)"으로 변화했다고 볼 수 있다.

이를 통해 딥러닝 모델은 복잡한 패턴을 더 효과적으로 학습할 수 있게 되었으며, 특히 이미지, 음성, 자연어 처리 분야에서 강력한 성능을 보인다.

Transformer, Titans과 같은 딥러닝 알고리즘들도 차차 배울 예정이다.

# 1. Linear Algebra

## 1.1 Eigenvalues and Eigendecomposition

eigen value는 square matrix에 대해서만 정의된다.
$\vec x ^{*}$: Transpose consugate $\vec x^{\top}$

Fact: If $v$ is an eigenvector associated with $\lambda$, so is $av$ with $a\neq 0$.
eigenvector에서 중요한건 size가 아니라 방향이다.

Fact:
$A\vec x = \lambda \vec x$
$(A - \lambda I)\vec x = \vec 0$에서 $(A - \lambda I)$가 Invertible하면
$\vec x = \vec 0$ 즉, eigen vector를 찾을 수 없다.
따라서 Non-Invertible해야 하므로 조건에 따라 $\det(A - \lambda I) = 0$이여야 한다.
$\mathcal{N}(A - \lambda I)$ ($\mathcal{N}$: Null Space)
따라서 Eigen vector는 $\mathcal{N}$의 Element이다.

Null Space의 Demension 만큼 independant한 eigen value를 가질 수 있다.
Demension 1이면 점이므로 하나만 있고
Demension 2면 두 개..
따라서 $nullity(A - \lambda I)$만큼 Independant한 eigen value가 생기는 것이다.

Theorem:
nullity n인 Matrix에서 n개의 서로 다른 eigen value가 나오면 모든 eigen vector는 linearly independent하다.

WLOG: Without Loss of Generality

