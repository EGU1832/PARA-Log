# 실시간 렌더링

# Reflection Refraction Diffuse

Reflect + Transmit (Diffuse / Refract)


빛이 들어와서 일부는 반사되는데 에너지 보존의 법칙에 의해서 1이라는 빛이 들어오면 반사되는 양과 투과되는 빛의 양이 총합 1이여야 할 것이다.
$$light := 1 = \gamma + (1 - \gamma)$$
$\gamma$의 비율은 물체의 성질에 따라 $k_{d\gamma}$ 인자에 의해 조정된다.

Reflect는 말그대로 거울 같이 빛이 반사되어 나가는 정반사이니 넘어가고,
Transmit 부분에 대해서 좀 더 살펴보자.

Transmit - Refraction
Snell의 법칙으로 굴절각 계산

Transmit - Diffuse
Ideal Diffuse object를 생각해보면 다음과 같다.
![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSea0Mp4gy43R9pBzRC430Ma6Be_FLctKBrEg&s)

근데 실제 세계에서 이상적인 모델이 있을리가 없다.
따라서 그래픽스에서는 Micrafacet Model을 이용한다.
![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQoaVOS9LpFc_nnnsMWHX6y4Io-60YbFtoa0g&s)
다음과 같이 빛이 나가는 방향이 달라진다를 측정을 해서 적용시킨다.

Snell은 굴절각을,
Fresnel은 굴절 비율을,
Schlick은 굴절 비율의 근사를.

위 내용을 바탕으로 코드로 구현한 것이다.
```cpp
float FresnelReflectAmount (float n1, float n2, vec3 normal, vec3 incident)
{
	// Schlick aproximation
	float r0 = (n1-n2) / (n1+n2);
	r0 *= r0;
	float cosX = -dot(normal, incident);
	if (n1 > n2)
	{
		float n = n1/n2;
		float sinT2 = n*n*(1.0-cosX*cosX); // 실질적으로 sinT2의 제곱을 계산 중이다.
		// Total internal reflection
		if (sinT2 > 1.0) // 어차피 제곱도 최대 범위는 1이다.
			return 1.0;  // floating 연산을 줄이는 중인 것 같다.
		cosX = sqrt(1.0-sinT2);
	}
	float x = 1.0-cosX;
	float ret = r0+(1.0-r0)*x*x*x*x*x;
	
	// adjust reflect multiplier for object reflectivity
	ret = (OBJECT_REFLECTIVITY + (1.0-OBJECT_REFLECTIVITY) * ret);
	return ret;
}
```

$R + T = 1.0$
$T = 1.0 - R$
$C_{ray} = R \cdot C_{R} + T \cdot C_{T}$

Ray Tracing에서 Diffuse를 계산할 때, 우리는 Phong의 조명 모델을 떠올릴 수 있다.

Phong
Diffuse + Spec

Microfacet Model을 쓰는 대신 단순화하여 Phong의 조명 모델에서 식을 가져오기도 한다.

$k_t\gamma$ 값은 어떻게 줍니까? -> 적당히 줘라

local shading같은 경우는 phong의 local illumination model을 적절히 변형해서 썼으나 간단한 프로그램을 만드는 경우고 요즘은 이렇게 하면 욕 먹는다

요즘은 PBR(Physically Based Rendering)을 주로 쓰는데
Rendering Equation을 완전히 적용하면 좋겠지만 지금은 계산적으로 무리
[Reference - PBR](https://youtu.be/RRE-F57fbXw?si=qK-1RISLn6AUjJBp)

Ray Generation에서
$p(t, e, d) = p(t) = e + td$
$t > 0$ 눈 앞
$t = 0$ 눈
$t < 0$ 눈 뒤

#### `OptiXshading.cu`
> Optix Shader를 간단하게 봐보자.

**`closesthit_metal_raadiance`**: 그림 상의 하늘색 metal 구를 raytracing하는 shader
언제 이 함수를 호출하냐?
물질이 metal인 경우
내가 현재 처리하는 Ray가 Primary Ray일수도 있고 Refelction Ray 일수도 있고..
Closest-hit shader는 아마 본 Raytracing 강의의 마지막에서 다룰 shader 일것이다.

`nDl > 0`(backlight가 아니라 frontlight)이면 shadow ray를 쏘는 것이다.

`0.01f`: $t_{min}$
`Ldist`: $t_{max}$
왜 Ray의 최소 길이인 $t_{min}$을 0.0f로 주지 않냐?
튕겨져 나가기도 전에 물체 표면에서 다시 부딪히기 때문이다. 약간 zitter 같은 것이라고 생각하면 될 것 같다.

`light_attenuation`: 빛이 멀면 멀수록 약화시키는 것

local shading
직접조명 + 간접 조명
$I_{\lambda}(P) = I_{a\lambda}\cdot k_{a\lambda} + S_i \{ I_{l\lambda} \cdot k_{d\lambda}(N\circ L) + I_{l\lambda} \cdot k_{s\lambda}(N \circ H)^n \}$

Payload: Ray마다 붙어다니는 무언가
`result`를 현재 노드의 payload에 담아 보내주면 된다.

`tracRadianceRay` Reflection Ray 쏴가지고 들어오는 빛의 색깔

어? 그럼 Ray를 어디서 멈추지? 우리가 했던 건 depth 변수를 1씩 늘리면서 멈출지 말지를 판단했는데?
`phngShade()`함수를 확인해보자.
`prd` 변수에 `prd.importance`와 `prd.depth`로 존재한다. `prd.importance`는 빛의 약화와 관련된 변수고 `prd.depth`는 빛의 반사 횟수와 관련된 변수이다.

**`closesthit_glass_radiance`**: 그림 상의 속이 빈 유리 구를 raytracing하는 shader
세상 좌표계 WC를 기준으로 o, d, t, N을 갖고 옴.
hit point를 계산
`HitType` Front Face(유리구슬 바깥)을 때렸는지 Back Face(유리구슬 안쪽)를 때렸는지 여부를 체크하는 변수`일것임`
- front_hit_point
- back_hit_point

`beer_attenuation`: Beer's Law (강의자료 5 참고) 특정 파장의 빛이 물질을 통과할 때 흡수되는 양이 그 물질의 농도와 통과한 경로의 길이에 비례한다는 원리
$\text{beer effect} = e^{\sigma_t\cdot ray_t}$
$\sigma_t$가 클수록 불투명하다는 것
`dot(n, ray_dir) > 0`: 투명한 물체의 안 쪽이냐? -> Extinction Effect를 줘야한다.

그럼 이제 Refraction을 해보자.
`refract()`: 굴절 방향과 세기를 Snell과 Fresnel의 법칙에 의해 계산

` prd_radiance.importance * ( 1.0f - reflection ) * luminance( glass.refraction_color * beer_attenuation );`
약해진 정보인 importance + 1 - reflection + beer_attenuation을 luminance를 이용해 gray scale 바꿈 = 기준보다 미미하면 계산하지 마라

$(1 - k_r)L_{refr}\cdot C_{glass, refr}\cdot e^{\sigma_t\cdot ray_t} + k_r \cdot L_{refl} \cdot C_{glass, refl} \cdot e^{\sigma_t\cdot ray_t}$
`뭥...미`


# PBR

우리가 학부 때 배운 모델은 다음과 같다.

/ *Phong BRDF Model*
$$\begin{align}
I_\lambda &= I_{a\lambda} \cdot k_{a\lambda}+I_{l\lambda} \cdot k_{d\lambda} \cdot (N \circ L)+I_{l\lambda} \cdot k_{s\lambda} \cdot (R \circ V)^n \\
&= \text{embient reflection} + \text{diffuse reflection} + \text{specular reflection}
\end{align}$$
/ *Blinn Phong BRDF Model* `with halfway vector`
$$I_\lambda = I_{a\lambda} \cdot k_{a\lambda}+f_{att}(d) \cdot I_{l\lambda} \cdot \{ k_{d\lambda} \cdot (N \circ L)+k_{s\lambda} \cdot (N \circ H)^n \}$$

조잡하기 그지 없는 옛날 모델들이다..

최근엔 다음과 같은 모델을 쓴다.

/ *PBR(Physically Based Rendering)*
reference: Map_Physically Based Rendering Cocos Creator.pdf

![450](../../../../z.%20Docs/img/Pasted%20image%2020240919165321.png)
- Albedo Map
- Roughness Map
- Metallic Map
- Ambient Occlusion `그림 그릴 때 그림자 강조를 위해 쓰는 곱하기나 검정 오버레이 같은거`
- Normal Map
- Emissive Map: 발광
- Stemcil Map (Alpha Channel): 최적화를 위해 멀리 있는 물체를 단순화해서 그리는 것

PBR-Specular Workflow
PBR-Metalic Workflow

/ *Rendering Equation*
$$\begin{align} L_{out}(x, \vec \omega) &= L_e(x, \vec\omega) + L_r(x, \vec\omega) \\ &= L_e(x, \vec\omega) + \int_\Omega f_{BRDF}(x, \vec\omega', \vec\omega)L_{in}(x, \vec\omega')(\vec\omega'\cdot\vec{n})d\vec\omega' \\ &= \text{x가 카메라 방향으로 스스로 내는 빛} + \text{x에서 카메라 방향으로 방사되는 빛}&\end{align}$$
![300](../../../../z.%20Docs/img/Pasted%20image%2020240715152030.png)

Reference
Advanced Global Illumination.pdf - 2.3 Radiometry
	flux, radiance, iradiance
Videos.txt 꼭 시청해보기
	#4, #9

### 📁 Pdf: Slide_Rendering Equation - Image Synthesis - Part 3 - Chapter 1_p

/ *Angle & Solid Angle*
![300](https://qph.cf2.quoracdn.net/main-qimg-0083ec41ebff340b26b633e285b35d31.webp)

Angle(2D) $\theta$
최대 $2\pi$ (반지름이 1인 원의 지름)

/ *Solid Angle(3D)* $\ohm$
최대 $4\pi$ (반지름이 1인 구의 면적)
반구, Hemisphere($\ohm$)의 경우 $2\pi$인 것이다.
$\int_{\ohm}\sin{\theta}\ d\theta\ d\phi = \int_{0}^{2\pi}\int^{\frac{\pi}{2}}_{0}{\sin{\theta}\ d\theta\ d\phi} = 2\pi$

/ *Radiant Flux* $\Phi$
단위 시간 당 어떤 강도로 빛 에너지가 나가고 있는가
단위: Watt $[W = \frac{J}{S}]$

/ *Radiant Intensity* $I$
$I(w) = \frac{d\Phi}{d\omega}$
특정 Solid Angle $d\omega$ 로 나가고 있는 Radiance Flux
단위: Watt per Steradian $[\frac{W}{sr}]$

/ *Irradiance* $E$
$E(x) = \frac{d\Phi}{dA}$
단위 면적 $dA$ 당 들어오는 Radiant Flux, 나가는 것도 있다. `Radiant Exitant/Radiosity`
단위: Watt per square Metre $[\frac{W}{m^2}]$

/ *Radiance* $L$
$L(x, w) = \frac{d^2\Phi}{dA\ \cos{\theta}\ d\omega}$
특정 Solid Angle과 Projected Area로 나가고 있는 Radiance Flux
단위: Watt per Steradian per square Metre $[\frac{W}{sr\ m^2}]$

$\Phi \propto dA\ d\omega$, Flux는 면적과 각도에 비례하다고 볼 수 있는데
`특정 지점에서 특정 방향으로 빛이 얼마나 세게 나가고있는가`
정확히 여기서 $dA$는 다음과 같이 progection된 면적이다.
따라서 $\cos{\theta}$를 곱해주는 것이 에너지가 정확히 수직 방향으로 들어가는 면적이다.
![](../../../../z.%20Docs/img/Pasted%20image%2020240919173726.png)

> 왜 Radiance가 중요할까?
- 한 지점에서 특정 방향으로 $L$이라는 Radiance가 나가면 Radiance의 길이가 어떻게 되던 받아들이는 지점과 방출하는 지점의 값이 똑같을 것이다.
- 이를 이용해 Ray Tracing을 하는 것이다.
- 물론 진공 상태라는 가정 하이고, 연기나 공기가 중간에 포함되면 얘기가 또 달라진다.
- 수학적인 증명방법은 Advanced Global Illumination교재 24p. 참고

**Rendering Equation** `앞서 필기한거랑 살짝 다르긴 한데 일단 보자`

$x$라는 지점에서 $\omega_i$로 빛$L(\omega_i)$이 들어오고, $\omega_o$로 나가는 빛의 세기를 계산해보자.
$$\begin{align} L_{o}(\omega_o) &= L_e(v) + \int_\ohm f_{r}(\omega_i, \omega_o)L_{i}(\omega_i)\cos{\theta}_i\ d\omega_i \\
L_i(l) &= \frac{d^2\Phi}{dA\cos\theta\ d\omega} \\
\frac{d^2\Phi}{dA}= d(\frac{d\Phi}{dA}) &= L_i(l)\cos\theta\ d\omega \\
&= dE(l) \text{(irradiance)}
 \\ \therefore L_o(\omega_o) &= L_e(v) + \int_\ohm{f_r(\omega_i, \omega_o)\ dE(\omega_i)}\end{align}$$
- 따라서 BRDF식을 빼면 Equation에 Irradiance만 남는다.
- irradiance에 $f_r(v, l)$를 곱하고 $\ohm$ 만큼 다 더했더니 Radiance가 나온 것이다. 따라서 $f_r(v, l)$의 단위는 $\frac{\text{Radiance}}{\text{Irradiance}} = \frac{1}{sr}$가 될 것이다.
- $f_r(v, l)$: Bidirectional Reflection Distribution Function (BRDF), 얘를 어떻게 모델링 하느냐에 따라 Phong, PBR 등이 나오는 것이다.

#### BRDF
> $f_{r}(\omega_i, \omega_o)$, Material Property

$\omega$는 방향이므로 파라미터가 두 개이다. 따라서 BRDF 식은 4차원 함수이다.
만약 반구 $\ohm$을 1024개로 sampling한다고 치면 (들어오는 것) $\times$ (나가는 것) 해서 벌써 $2^{20}$이다.
Ideal Diffuse Material: 이상적인 난반사체의 경우 BRDF가 상수가 된다., $f_r(\omega_o, \omega_i) = k_d$

$$\begin{align}\rho_d &= \frac{\int_\ohm\ \{\int_\ohm f_{r}(\omega_i, \omega_o)L_{i}(\omega_i)\cos{\theta}_i\ d\omega_i \}\ \cos\theta_o\ d\omega_o}{\int_\ohm{L(\omega_i)\ \cos\theta_i\ d\omega_i}} \\ \text{Albedo (Diffuse Color)} &= \frac{\int_\ohm L(\omega_o)M(x)}{E(x)}\end{align}$$
Albedo(Diffuse Color)
어떤 한 지점의 들어오는 Flux 대비 얼만큼의 Flux가 나가는가
에너지 보존의 법칙에 의해 무조건 1보다 같거나 작아야 한다.

그렇다면 Ideal Diffuse Material 일때의 Albedo를 계산해보고 실제에 상응하는 값 나오는 것을 관찰해보자.
$$\begin{align}\rho_d &= k_d\int_\ohm\cos\theta_o \ d\omega_o \\ &=k_d\int^{2\pi}_0\int^{\frac{\pi}{2}}_0\cos\theta_o\sin\theta_o\ d\theta\ d\phi \\ &= k_d\int^{2\pi}_0[\frac12\sin^2\theta_o]^{\frac{\pi}{2}}_0\ d\phi \\ &= k_d \int^{2\pi}_0\frac12 \ d\phi = k_d \cdot \pi\end{align}$$
`첫 번째에서 두 번째 줄로 넘어가는 과정이 이해가 안 가면(이중적분) pdf 13p.를 보고오자.`
따라서 Ideal Diffuse Material의 경우 BADF의 값을 Albedo 값으로 나타내면 다음과 같다.
$$k_d = \frac{\rho_d}{\pi}$$
학부때 배운 phong 조명 모델 난반사 부분의 공식의 $k_{d\lambda}$에 해당한다.

/ *BRDF Properties*
BRDF가 식으로써 인정을 받으려면 다음과 같은 조건을 만족해야 한다. `pdf 23p.`
- Positivity: 함수값은 항상 0보다 크거나 같아야한다.
- Helmholtz Reciprocity: $l \rightarrow v = v \rightarrow l$ 즉, Bidirectional 해야한다.
- Energy Conservation: 사방에서 1만큼의 빛이 들어오면 나가는 것은 1보다 작거나 같아야 한다.

그렇다면 Phong의 모델을 BRDF로 해석하여 얼마나 조악한 모델이었는지를 살펴보자.
$$\begin{align}I_\lambda &= I_{l\lambda} \cdot \{ k_{d\lambda} \cdot (N \circ L)+k_{s\lambda} \cdot (R \circ V)^n \} \\ &= \{ k_{d\lambda} + \frac{k_{s\lambda}(R\circ V)^n}{(n\circ L)} \} \ I_{l\lambda} \cdot (N\circ L) \\ &= f_r(\omega_i, \omega_o) \ L(\omega_i)\cdot \cos\theta_i\end{align}$$
Blinn-Phong Model도 $(R\circ V)$가 $(N\circ H)$로 바뀌었을 뿐 별 다를 것도 없다.

### 📁Pdf: Slide_PBR Materials - Image Synthesis - Part 3 - Chapter 3_p

#### BRDF
$$\begin{align}&L(v) = L_e(v) + L_r(v)\\ &L(v) = L_e(v) + \int_\ohm f_{r}(v, l)L(l)\cos{\theta}\ dl \\ &f_r(v, l) = f_d(v, l) + f_z(v,, l)\end{align}$$

#### Cook-Torrance Model

/ *Material Parameter*
1. metallic(metalness) = m
```
m = 1 // metal
0 <= m <= 1
m = 0 // non-metal = dielectrics
```

왜 1과 0사이 값이 존재할까? 물체의 표면에 metal인 부분과 metal이 아닌 부분이 한 픽셀에 관찰될 수 있기 때문이다.

m의 값에 따라 specular 반사를 고려하느냐, 안 하느냐가 결정된다.
$m = 1, f_r(v, l) = f_s(v, l)$
$m = 0, f_r(v, l) = f_d(v, l) + f_s(v, l)$
따라서 실제 코드에서는 m param의 비율을 반영해서 하나의 식으로 합쳐서 쓴다.
$f_r(v, l) = \frac{\rho_d}{\pi}(1 - m) + f_s(v, l)$

2. Roughness
```
0 <= r <= 1
```

3. Reflectance
```
0 <= l <= 1
```

/ *Metal-Roughness Model vs Specular - Glossinness*
둘 중 어느 방법으로 가느냐에 따라 사용할 map들이 달라진다.

#### 📁 Base_s2013_pbs_physics_math_notes

/ Surface Scattering
Fresnel Equation을 상기해보자.
incoming light = reflection + trasmission(refraction)

빛이 들어왔을 때 Fresnel Equation에 의해서 특정 비율만큼은 reflect되고 나머지는 trasmission된다.

![300](../../../../z.%20Docs/img/Pasted%20image%2020240926165445.png)
🔵
그런데 transmission된 빛들 중에서는 내부에서 반사되고 반사되서 물체의 색상을 머금고 다시 표면 위로 탈출하는 빛들이 있다.
이 빛들을 surface-scattered light라고 한다.

이것이 non-metal에서 가정하는 diffuse light이다.

🟡
specular reflection의 경우 metal과 같다.

![300](../../../../z.%20Docs/img/Pasted%20image%2020240926165431.png)
🟠
반면 metal에서는 이런 현상이 없고 transmission된 빛들이 그냥 내부에서 흡수되버린다.

🟡
이제 metal에서 말하는 specular reflection에 대해서 얘기해보자.
이론상으로는 마치 거울처럼 perfect specular이 되어야 하지만 실제로는 반사각과 동일한 방향으로 살짝 퍼져서 빛이 나간다.

이 화살표의 폭이 좁아지는 경우도 있고 퍼지는 경우도 있다.
- 폭이 좁음
- 폭이 넓음
- 폭이 없음: perfect specular
이를 모델링하는 것이 **Microfacet Model**이다.

다시 본 Pdf로 돌아와보자.

/ *Microfacet BRDF*
확률적으로 빛이 들어와서 특정 굴곡을 때리면 어떤 방향으로 빛이 나갈지 각각의 굴곡 각각은 perfect specular reflection으로 나간다고 가정한다.

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQiWsHtE33XAfxD6njyQ9bpOfoZL7_2T3kcxQ&s)
이 굴곡, Microfacet의 정도를 확률적으로 조절하는 것이 Roughness Parameter이다.

실제 식은 다음과 같다.
$$\begin{align}&f_s(v, l) = \frac{F(v, h)\ D(h)\ G(l, v)}{4\ \langle n\cdot l\rangle \langle n\cdot v\rangle} \\ &F: \text{Fresnel reflectance} \\ &D: \text{Normal distribution function (NDF)} \\ &G: \text{Geometry term}\end{align}$$

그럼 Fresnel Reflectance에 대해서 알아보자.
아까 위에서 했던 화살표들.. 시작하기 위해선 우선 얼마나 반사되고 얼마나 들어가는지 알아야 하는 것은 자명하다.
이를 위한 게 Fresnel Reflectance이다.

기본 개념은 전에 했으니, non-metal과 metal로 나누어 parameter의 구체적인 값을 위주로 보자.
/ *Fresnel Reflectance: Non-Metal*
![450](../../../../z.%20Docs/img/Pasted%20image%2020240926171537.png)
![450](../../../../z.%20Docs/img/Pasted%20image%2020240926171645.png)
$F_0$: 수직광일 때의 비율이다.
대부분의 non-metal은 대게 0~4%로, 거의 다 흡수한다는 것을 볼수있다. `다이아몬드는 예외`

/ *Fresnel Reflectance: Metal*
![450](../../../../z.%20Docs/img/Pasted%20image%2020240926171557.png)
![450](../../../../z.%20Docs/img/Pasted%20image%2020240926172429.png)

따라서 모델링을 할 경우 Non-Metal인 경우는 그냥 값을,
Metal인 경우는 크기가 3인 Vector가 필요하다는 것을 알 수 있다. `(Linear, Float)`

잠깐 다음 식을 상기하자.
	$m = 1, f_r(v, l) = f_s(v, l)$
	$m = 0, f_r(v, l) = f_d(v, l) + f_s(v, l)$
보통 Albedo Map을 보면 기본 컬러는
Non-Metal은 $\rho_d$가,
Metal은 $F_0$값이 들어간다.

최종적으로 우리는 예전에 배웠던 Schilick Approximation을 쓴다. `빨라서`
$$F_{schlick}(v, h) = F_0 + (1.0 - F_0)(1.0 - \langle v\cdot h \rangle)^5$$
Non-Metal의 경우 vector $F_0$에 모두 같은 값 (0.04, 0.04, 0.04)이 들어가서 계산된다.

```
vec3 f0 = vec3(0.16 * (reflectance * reflectance));
```
-> 이렇게 하면 (Non-Metal의 경우) 0~16% 사이에서 값 조절이 가능하다. `다이아는 예외`
```
f0 = mix(f0, baseColor, metalic);
```
Linear Interpolation을 해주는 함수이다.
$$F_0 - \text{Base Color} \propto \text{metalic}(0.0 -1.0)$$

이제 다음, $D$ 함수를 살펴보자.
/ *Normal Distribution Function (NDF)*
이 함수는 3개 정도가 있다.
$$\begin{align}&D_{Blinn}(h)\\ &D_{Backmann}(h)\\ &D_{GGX}(h)\end{align}$$
여기서는 GGX를 쓴다.
```
float D_FFX();
```

/ *Geometry Term*
microfacet들끼리 간섭을 일으킨다.
![600](../../../../z.%20Docs/img/Pasted%20image%2020240926174826.png)
- Masking Effect
- Shadowing Effect
$$G_{Cook-Torrance}(l, v) = min(a, ..)$$
여기서는 Smith를 쓴다.
```
float G_Smith();
```

### 📁Pdf: CSE6449_강의자료_RT_2

Reference: 3-summer/Raytracing.md

# Texture Mapping

### 📁Pdf: CSE4070_강의자료08

Texture Mapping
- Filtering
- Shadow Mapping
- Reflection / Refraction Mapping
- Normal Mapping (Bump Mapping)

Rendering
- Rasterization
- Ray Tracing
	- WS
	- Path Tracing
	- Distributed Ray Tracing

참고로 강의자료는 거의 10년 전에 만들어진 것이니 적당히 참고하자..

```
"텍스처 매핑은 컴퓨터 그래픽스의 꽆이다."
/ ihm
```

1. Texture Mapping:
	- 우리가 알고있는 그 텍스처
2. Bump Mapping (Normal Mapping):
	- 표면의 우둘투둘한 부분을 표현하고 싶을 때
	- Ray Tracing에서도 중요한 Map이다.
3. Environmental Maaping:
	- 주변 배경 `cube map`
4. Shadow Mapping:
	- 그림자
5. Displacement Maaping:
	- Normal Map처첨 무지갯빛이다. 근데 용도가 다르다.
	- Normal Map은 물체 자체를 바꾸진 않지만 얘는 물체 자체를 변형해버린다.
	- 틀에 본을 딴다고 생각하면 될거 같다.

여기까지는 전형적인 텍스처 매핑의 종류이고, 더 나아가선 상상력을 발휘해서 마법을 부리는 아티스트들도 있다.

`8p.`
그럼 텍스처 매핑은 어떻게 하는걸까?

![600](../../../../z.%20Docs/img/Pasted%20image%2020241017170323.png)

1. 표면 매개화 (Surface Parameterization)
2. 기하 변환
3. 래스터화
4. 텍스쳐 색깔 계산: 픽셀 위치가 결정되면 Texture Space로 다시 돌아와서 다양한 방법으로 색깔 결정 `NEAREST, LINEAR, MIPMAP`

![450](../../../../z.%20Docs/img/Pasted%20image%2020241024164601.png)
- Texel Generation: `glTexParameter*(*)` `NEAREST, LINEAR, MIPMAP..`
- ~~Texture Application~~: Texel의 색깔을 현재 fragment의 색깔과 어떻게 섞을 것인가? 이건 근데 이제 사용하지 않는다.

`22p.`
`glTexParameter`에 넘겨주는 다양한 인자들이 정의되어있다.
- GL_TEXTURE_WRAP_S:
	- GL_CLAMP: 일정 범위 내로 CLAMP 시킴
	- GL_REPEAT: OpenGL 교재 앞면처럼 이미지를 반복시킴

텍스처 좌표 address node
normalized coordinate

`plus alpha: Texture in CUDA`
	CUDA에서의 Texture Mapping에 대해서 잠깐 들여다보자.
	📁 CUDA C++ Programming Guide Release 12.3, Texture Object API
	현재는 Texture Read만 가능하긴 하다.

texture coordinate attribute라는 물체의 점의 attribute를 기반으로 텍스처를 붙이게된다.

`10p.`
PreImage: 픽셀이란 조그만 창문을 통해서 바라보는 물체의 영역
Texture Image 원본에 접근해서 대응되는 Texel의 Interpolation을 계산한다 <- 이 부분이 계산이 복잡해지는 과정이다!
Pixel 하나에 대응되는 Texel이 한 개일수도 있고 여러 개 일수도 있는데 연산 속도는 같아야 한다는 점이 이를 어렵게 만드는 것이다.

`34p.`
#### Texture Filtering
> 어떻게 하면 픽셀에 대응되는 Preimage영역의 색깔을 가급적 정확하고 빠르게 추정할 수 있을까?

물체 확대 시 Pixel 하나에 대응되는 Texel이 적어지고, -> MAG_FILTER 사용
물체 축소 시 Pixel 하나에 대응되는 Texel이 많아진다. -> MIN_FILTER 사용

그럼 언제 두 필터 사용을 전환할까?
**1 Pixel <-> 1 Texel** 일때!

Ex)  `40p.`
Mgnification: Nearest(각진거)
Minification: Linear(부드러운거) `윈도우 화면 줄이는 것도 Minification에 해당한다!`
*어? 근데 주전자를 돌려보면 주전자의 실루엣 부분에서, 실루엣 부분은 minification이니까 부드러워 보여야할 것 같은데 각져보인다.*
왜그럴까? 이에 대해선 차차 배우면서 알아보자.

`36p.`
일단 Preimage를 정사각형이라고 가정하자. `실제로는 안 그렇다.`
![450](../../../../z.%20Docs/img/Pasted%20image%2020241017172526.png)
앞서 말했다시피 확대, 축소에 따라 대응되는 개수가 차이가 난다.

`22-23p.`로 가면 openGL에서 사용하는 이와 관련된 함수가 나와있다.
`Gen -> Bind -> TexImage2D CPU에서 GPU로 load ->... `

/ *Mipmap Filter*
GL_LINEAR_MIPMAP_LINEAR
이 필터를 주로 쓴다. 혼합 필터같은 경우는 많이 쓰진 않는다.
```cpp
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, ??? ); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, ??? );
```

/ GL_NEAREST와 GL_LINEAR 요약
GL_NEAREST는 그냥 단순히 pixel을 닿는 Texel로 칠해버리는 것이고 `Noisy👎 / Crispy👍`
GL_LINEAR는 Linear Interpolation을 사용하는 것이다. `Soft👍 / Blurred👎`

GL_LINEAR를 써도 충분할거 같은데 왜 Mipmap이 개발되었을까? 다음 질문을 보자.

`어? 근데 주전자를 돌려보면 주전자의 실루엣 부분에서, 실루엣 부분은 minification이니까 부드러워 보여야할 것 같은데 각져보인다.`
다음 의문을 상기하며 질문을 던져보자.

과연 축소 상황이 심해져도 GL_LINEAR가 도움이 될까?
-> 축소 상황이 심해지면 구조적으로 GL_LINEAR 필터도 불충분하다.
왜냐하면 Interpolation을 한다고 했을 때, 중간에 생략되는 Texel들이 많아지기 때문이다.
![200](../../../../z.%20Docs/img/Pasted%20image%2020241017174002.png) (GL_LINEAR는 둘러싼 Texel 4개만 씀!)

그래서 GL_LINEAR_MIPMAP_LINEAR를 쓰는 것이다. `Mipmap`
이제 이 MIPMAP에 대해서 자세히 알아보자.

/ *MIPMAP*
![600](../../../../z.%20Docs/img/Pasted%20image%2020241024170033.png)
L0($1024 \times 1024$) -> L1($512\times 512$) -> L2($256\times 256$) ->...
똑같은 이미지를 평균 내서 더 작은 해상도의 이미지를 만들고 미리 Texture Map에 넣어놓는다.
`43-45p.`
- Box Filter 크기의 추정: 세 추정 방법 사이에 어떤 차이가 있을까?
	- (Square) 딱딱함 --> 부드러움 (Trapezoid)
- Mipmap Level의 결정:
	- `dFdx() dFdy()`: Pixel이 한 칸 움직일 때 Texture 좌표가 얼마나 움직이는가
- Trilinear Interpolation (삼선형 보간):
	- 각각의 Texel이 Size가 PreImage의 정사각형의 크기보다 작거나 큰 경계를 찾는다.
	- 삼선형 보간: $k_0, k_1$에서 각각 Linear Interpolation`(1)`, 정사각형의 크기의 비율로 또 다시 Linear Interpolation`(2)` 하는 것 `GL_(1)_MIPMAP_(2)`

이제 이 MIPMAP을 시각화한 강의자료 `46p.`를 살펴보자.
초록색, 파란색, 빨간색 -> 민트색
L0 -> L1 -> L2 -> L3
`NEAREST_*`: 각각의 Level이 딱딱한 것을 볼 수 있다.
`LINEAR_*`: 각각의 Level이 부드러운 것을 볼 수 있다.
`*_NEAREST`: 다음 단계로 넘어갈 떄 딱딱하게 넘어가는 것을 볼 수 있다.
`*_LINEAR`: 다음 단계로 넘어갈 때 부드럽게 넘어가는 것을 볼 수 있다.

`48p.`
Box Filter 크기의 추정과 관련하여 각기 다른 MIPMAP의 양상을 봐보자. 
**Isotropic**: 길쭉하게 늘어난 이미지를 정사각형으로 추정하겠다.
- 정사각형인 Texture Image만 필요하다.
**Anisotropic**: 길쭉하게 늘어난 이미지를 가로 세로가 다른 직사각형으로 추정하겠다. -> 비스듬히 보게되는 부분이 뿌옇게 보이지 않고 선명하게 보이게된다.
- 강의자료의 그림처럼 각 방향으로 쪼그라드는 Texture Image를 만들어놔야하기 때문에 계산걍과 메모리가 많이 든다.

이제 Shadow Mapping에 대해서 알아보자.
Ray Tracing을 하는 사람들은 웃을 것이다. `그냥 Shadow Ray 쏘면 되잖아..`
하지만 일단은 Rendering에서 매우 중요한 것이므로 아무리 시대가 변화해간다고 하더라도 알아두는 것이 좋을것이다.
Reference는 다음과 같다.
	OpenGL 2 Shading Language Cookbook (2nd ed. Chapter 7)
	본 과목 제공 OpenGL 코드
	OpenGL 4 Shading Language Cookbook 제공 관련 코드
	Learn OpenGL - Graphics Programming Chapter 35
	Leran OpenGL- Graphics Programming 제공 관련 코드
	본 과목 제공 Vulkan 쉐이더

`55p.`

# Shadow Mapping

#### Shadow Mapping
기본적인 Step은 GPT를 참고
Shadow Mapping의 Idea 자체는 간단하다.
광원을 기준으로 Shadow map을 만들고, Camera 시점에서 보는 지면의 점(그림자가 질 곳) 광원에 투영해서 Shadow Map을 참고하여 Depth 비교를 통하여 그림자를 생성한다.
`58p.`에 있는 식을 정확히 이해할수만 있다면 Shadow Mapping은 끝난다.

Shadow Mapping의 과정을 간략히 정리하면 다음과 같다.
1. Pixel을 통해서 보이는 가장 가까운 지점까지의 거리에 대한 Map, 즉 Shadow MAp을 만든다.
2. Shadow Map을 적용해서 카메라 관점에서 Scene을 그린다. 카메라에서 쏘고 Surface에서 반사돼서 빛으로 가는 그 길이가 Depth Map의 값보다 크면 그림자가 생기는 것이다.

Camera를 구성하고 있는 요소는 다음과 같다.
- Viewing Transformation (Extrinsics Parameter)
- Projection Transformation (Intrinsic Parameter)

근데 이제 여기서 다음과 같은 요소를 고려해야된다.
Framebuffer
- FB(Default): Color Buffer, Depth Buffer, Stencil Buffer `Depth Buffer와 Stencil Buffer는 Depth-Stencil Buffer로 합칠 수 있다.`
- FBO: FB와는 달리 Render Target을 메모리의 특정 Texture로 설정해 중간 렌더링 결과를 저장할 수 있도록 한다. 그러니까 쉽게 말해 `아..씨 내가 구현해보고 싶은 게 FB로는 안 될거 같은데...` 일때 쓰는 것이다. 다음과 같이 생겼다. `glDrawBuffer(n, attachments[n])`를 통해서 Link 해줘야 하는데 `Attachment 0, 1, 2..`로 지정해준 것이 `attachment(GL_COLOR_ATTACHMENTS, ...)` 각각의 Index로 Mapping 되는 식이다. 쉽게 말해 번거롭고 귀찮은 과정을 거쳐야하는 것이다. `advanced_lighting_5.1.deffered_shading/deffered_shading.cpp 참고` ![450](https://www.researchgate.net/publication/267830465/figure/fig5/AS:384241901817860@1468621816848/Figura-5-Esquema-representando-a-estrutura-de-um-Frame-Buffer-Object-Green-2005.png)
Renderbuffer Object와 Texture Objects의 차이는 무엇인가?
- TO는 아무데나 접근을 해도 MIN/MAX_FILTER 등으로 알아서 색깔이 추출돼서 나온다.
- RBO는 Data의 배열이기 때문에 중앙에만 접근할 수 있다. 한 번 Rendering 계산이 끝나면 버릴 것이기 때문에 그냥 차라리 빠른 RBO를 쓰자. 라는 느낌이다.

아직 넘어가긴 이르다.
OpenGL Geomerty Pupeline Overview를 다시 상기하고 가자. `📁CSE4100 강의자료 4 참고`



**(1) Shadow Map Generation**

`5.5.0.Tiger_Texture_Shadow_PS_GLSL` 참고
```
Texture Unit
GL_TEXTURE0 + N_NORMAL_TEXTURES_USED
0
1
2 <=> Shadow Map
```
Shadow의 해상도는 `SM_param.width, SM_param.height` 조절

`glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_BORDER);`
범위 바깥의 처리

`glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_COMPARE_FUNC, GL_LEQUAL);`
Texture를 Access 했을 때 결과값은 RGB 값이 아니라 1.0또는 0.0
$(s, t, r, l)$
$D_t$: s, t를 가지고 가지고 온 색깔 값

`textureProj(texture_unit_num, (s, t, r, q))`
Proj가 붙은 것에서 알 수 있듯이 원근 나눗셈을 한 번 해준다.
$$\begin{pmatrix}s\\t\\r\end{pmatrix} = \begin{pmatrix}\frac{S}{Q}\\\frac{T}{Q}\\\frac{R}{Q}\end{pmatrix}$$
저 $s, t$가 $D_t$를 만들고 그것을 $r$과 어떻게 비교할까? 하는 것이 Texture Comparison Function Parameter로 지정해주는 것이다. `GL_LEQUAL 등`

![450](../../../../z.%20Docs/img/Pasted%20image%2020241029174241.png)

**(2) Shadow Map Application**

$$\begin{align}\begin{pmatrix}S\\T\\R\\Q\end{pmatrix} &= \begin{pmatrix}0.5 & 0&0&0.5\\0&0.5&0&0.5\\0&0&0.5&0.5\\0&0&0&1\end{pmatrix}\times M_{P_l}\times M_{V_l}\times \begin{pmatrix}x_w\\y_w\\z_w\\1\end{pmatrix} \\ \begin{pmatrix}s\\t\\r\end{pmatrix} &= \begin{pmatrix}\frac{S}{Q}\\\frac{T}{Q}\\\frac{R}{Q}\end{pmatrix}\end{align}$$
$$\begin{align}S &= 0.5 x_c + 0.5w_c\\
T &= 0.5y_c + 0.5w_c \\ 
R &= 0.5z_c + 0.5w_c\\
Q &= w_c\end{align}$$

$$\begin{align}s = 0.5 \frac{x_c}{w_c} + 0.5 \\
t = 0.5 \frac{y_c}{w_c} + 0.5\\
r = 0.5 \frac{z_c}{w_c} + 0.5\\\end{align}$$

그럼 이제 그림자가 아닌, 표면을 우둘투둘하게 보이게 하는 Normal Mapping에 대해서 알아보자.

#### 📕 OpenGL 4 Shading Language Cookbook
#### 📄 2024.10.31 Paper
#### Normal Mapping
PBR 때 했던 Normal Map이 기억나는가?
홀로그램 같은 Map이 기억이 날 것이다. 그것이 Normal(법선) 정보가 담긴 Normal Map이고 이 텍스처를 사용해 표면의 세부적인 법선 방향을 지정하여 빛이 입사할 때마다 표면의 조명 반응을 조정한다.
일종의 착시 현상 같은 것이다.

![300](https://www.updateme.cloud/download/Stonex/Cubefly/Guide/lib/UV%20Planar1.png) ![300](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSP72A3wFZ0X5PfpKIXLgEGT9WBVobNHIlqYA&s)
각 물체마다 

![300](../../../../z.%20Docs/img/Pasted%20image%2020241031171857.png)

여기서 새로운 좌표계의 개념이 나오게 된다.
/ *Local Space(Coordinate) `= Tangent Space(Coordinate) = SL space(Coordinate)`*
![300](https://www.researchgate.net/publication/47861427/figure/fig8/AS:668606312509456@1536419575526/Normal-blue-tangent-red-and-binormal-green-are-computed-per-vertex-Thus-every.png)
Normal Map에는 위 좌표 중 N의 값이 RGB 형태로 들어있는 것이다.
$$\begin{align}&\begin{pmatrix}n_x \\ n_y \\ n_z\end{pmatrix}\\
n_x = n \cdot T, n_y &s= n\cdot B, n_z = n\cdot N\end{align}$$
Nomal Map을 이미지화해보면 다음과 같다.
![300](https://learnopengl.com/img/advanced-lighting/normal_mapping_ground_normals.png)
이제 여기서 색깔 계산을 위해 필요한 수학은 딱 하나이다.
하려는 것은 Normal Map의 Normal로 원래 Normal을 대체해서 Shading을 하려는 것이고, 
우리가 알아야되는 것은 MC/WC/EC에서 Normal Map으로 주어진 Normal Vector를 어떻게 표현할 것인가 라는 것이다. 이를 $n^*$라고 하자.
그럼 우리는 다음과 같이 식을 세우면 된다. Texture 좌표축에 물체의 Normal의 수선을 내린다고 생각하면된다.
$$\begin{align}n_x = T \cdot n ^* \\ n_y = B \cdot n ^* \\ n_z = N \cdot n ^*\end{align}$$
이제 이를 $A\vec x = \vec b$로 만들어서 역행렬을 곱하면 되는데, 우리가 다루는 건 A는 Orthogonal Matrix니까 그냥 Transpose하면 된다. 따라서 최종적으로는 다음과 같다.
$$\begin{pmatrix}n^*_x \\ n^*_y \\ n^*_z \end{pmatrix} = \begin{pmatrix}T &B & N\end{pmatrix} \begin{pmatrix}n_x \\ n_y \\ n_z\end{pmatrix}$$

이제 Paper📄 133p.의 실제 코드를 보자.
굳이 T B N다 알 필요 없이 T->B->N으로 둘씩 짝지어서 외적하면 나머지 하나를 알 수 있으니
따라서 이 코드에서는 Attribute로 Position, N, Texture 좌표, T가 들어가있다. (B는 N, T로 구할 수 있음)

📆 2024.11.05
![600](../../../../z.%20Docs/img/Pasted%20image%2020241105170106.png)



-----------------------------------------------

/ Z-Fighting(Depth-Fighting)
Precision 오차를 보완하기 위해 $D_t$에 padding offset을 더하는 것
물체를 Offset 만큼 민다고 생각
![300](../../../../z.%20Docs/img/Pasted%20image%2020241031165600.png) ![150](../../../../z.%20Docs/img/Pasted%20image%2020241031170016.png)
오른쪽 그림과 같이 Driver에서는 삼각형의 기울기에 따라 지정해줄 Offset을 다르게 계산하기도 한다.

**Forward Rendering**: Rasterization Algorithm
**Backword Rendering**: Ray Tracing
**Deferred Rendering**: 위 두 렌더링 기법의 단점 보완
`자세한 것은 GPT 참고`

9p. Resterization 시 Linear Interpolation으로 선형보간되어 좌표가 결정된다.
$(s_0, t_0), (s_1, t_1), (s_2, t_2)$

# Deffered Rendering

오늘 우리는 Forward Rendering과 Deffered Rendering의 차이점을 비교해볼 것이다.
Forward Rendering이라는 것은 우리가 잘 아는 pipeline에서 pixel의 색깔을 계산하는 방법이다.
- Vertex-lit Rendering: Shader를 쓰지 않는 Fixed Function Pipeline
- Forward Rendering: Shader를 쓸 수 있는 Pipeline
- Deferred Rendering: d

![200](https://cglearn.eu/images/acg/deferred/deferred-pipeline-1.png) ![400](https://cglearn.eu/images/acg/deferred/deferred-pipeline-2.png)
▶ Forward Rendering, Deferred Rendering

Forward Rendering의 Pipeline을 간단하게 봐보자.
VS -> TS -> GS -> FF -> Raster -> FS -> Raster Operation -> FB
- 광원이 많을 때 FS에서 Ranster Operation으로 넘어갈 때 Floating Point 연산이 낭비된다.

Deferred Rendering은 Forward Rendering의 Pipeline을 그대로 사용하되, 한 번 더 반복한다.
Pass I에서는 정상적으로 렌더링을 하고, G-buffer라는 Data를 생성한다.
Pass II에서 Shading 계산을 하게된다.
`Reference - Leran OpenGL`
차이점은 Pass I의 결과를 FB가 아닌 FBO에 그린다.
FBO에는 Texture Image를 붙일 수 있다고 했다. 다음과 같은 정보를 붙여준다.

/ **G-buffer**
![450](../../../../z.%20Docs/img/Pasted%20image%2020241107172312.png)
- Position: 세상좌표계에서의 x, y, z값
- Normal: 울퉁불퉁한 정도
- Albedo (RGB) + Specular (A): 기본 색깔 + 정반사 정도
- Depth Buffer: 앞에 있는지 뒤에 있는지

정상적인 렌더링이라면 내 눈에 보이는 물체들만 그리게 될 것이다.
뒤에 있는 물체는 Depth Buffer에서 날라간다.

일단 코드를 보며 이해해보자.
deffered에선 min filter도 fl_nearest로 하고 mag filter도 히 nearest로 한다. 이 부분 꽤 중요하니 잘 기억하고 넘어가자.
다음 코드가 FBO에 RGB texture 하나를 붙이는 하나의 블록 과정이다.
```cpp
glGenTextures(1, &gPosition);
glBundTexture(GL_TEXTURE_2D, gPosition);
// RGBA 16it floating point
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA16F, SCR_WIDTH, SCR_HEIGHT, -, GL_RGBA, GL_FLOAT, NULL);
glTexParameteri(GL_TEXTURE_2D, GL_TI+EXTURE_MIN_FILTER, GL_NEAREST);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTIRE_MAG_FILTER, GL_NEAREST);
glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT1, GL_TEXTURE_2D, gNormal, 0);
```

마지막으로 다음과 같이 Attach해준다.
```cpp
unsigned in attachements[3] = { GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1, GL_COLOR_ATTACHMENT2 };
glDrawBuffer(3, attachments);
```

이제 Render Buffer에 Depth Buffer 정보를 저장한다.
Render Buffer는 Pass I에서 한 번 쓰고 버릴거기 때문에 좀 더 복잡하게 생겼지만 Texture와는 다른 형식으로 저장한다. 사실 Texture를 써도 되는데, 여러모로 Write가 더 빠르기에 효율을 위해 이렇게 하는 것 같다. `사실 컴퓨터에서는 GPU가 워낙 빠르기 때문에 뭘 쓰던 상관없는 것 같다. `
```cpp
glGenRenderbuffers(1, &rboDepth);
glBindRenderbuffer(GL_RENDERBUFFER, rboDepth);
glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT, SCR_WIDTH, SCR_HEIGHT);
glFramebufferRenderbuffer(GL_FRAMBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, rboDepth);

// check Frambuffer is complete
...

glBnidFrambuffer(GL_FRAMECUFFER, 0) // System에서 사용하는 기본 Buffer에 최종적으로 Bind
/* Default Setting Complete!! */
```
이제 G-buffer의 세팅이 완료되었다.
이제 다음 Pass에서 색깔 계산 시 이를 사용하게 될 것이다.

이렇게 한 번 더 Shading을 함으로써 전에는 색깔 다 계산 해놓고 `아; 미안 너 뒤에 있었네.. 죽어라!`
하던 게 이제는 Pass I에서 미리 앞인지 뒤인지 알고 가서 그 부분만 계산하기 때문에 floating point 연산을 줄일 수 있다.
각각 다 장단점은 있지만서도 언뜻 봐서는 Deferred Rendering이 더 효율적으로 보인다.

📆 2024.11.12

![600](../../../../z.%20Docs/img/Pasted%20image%2020241112164120.png)

```cpp
//shader condiguration
shaderLightintPass.setInt("gPosition, 0");
// OpenGL에서 Texture를 넘겨주는 방식, 0번 Texture Unit에 붙여준 Texture Unit에 Access 해라.
shaderLightintPass.setInt("gNormal, 1");
shaderLightintPass.setInt("gAlbedo, 2");
```

reference: 📁 CSE4170_강의자료_6.pdf

renderQuad()에선 물체를 그리기 전 필요한 Modeling, Viewing, Perspective 변환을 하지 않고 있다.
그에따라 Vertex Shader의 내용도 살짝 다르다.
원래 Vertex Shader에선 MC에서 `M_M -> M_V -> M_P`를 하여 CC로 보내줘야 하는데
$$\begin{pmatrix}-1 \\ -1 \\ 0 \\ 1\end{pmatrix}_{MC} \rightarrow \begin{pmatrix}-1 \\ -1\\0\\1\end{pmatrix}_{CC} \rightarrow PD\rightarrow\begin{pmatrix}-1\\-1\\0\\1\end{pmatrix}_{NDC}\rightarrow NDC \rightarrow RAS$$
$[0.0, 1.0]\times [0.0, 1.0]$에 해당하는 WDC에서 (s, t)에 해당하는 Texture 좌표의 값을 가져와서 Shading을 수행한다.

근데 CSE4070에서 배운대로 원래 원근투영에선 CC에서 w가 1이 될 수가 없을텐데, 무슨 일일까?
여기선 원근 투영이 아니라, 직교 투영(Orthogonal Projection)을 사용하고 있는 것이다.
![300](https://i.ytimg.com/vi/dUJzpz8J9xw/maxresdefault.jpg)

M_V 이게 단위 행렬이라는 뜻은 카메라가 원점에 있다는 것이다. (WC = CC)
M_M 따라서 이것도 단위행렬이 된다.

```vert
TexCoods = aTexCoords;
gl_Position = vec4(aPos, 1.0);
```
	??

```cpp
//diffuse
...
//specular
...
//attenuation
...
```

이제 Shading 계산이 끝났다.

/ **Forward Rendering(Without, With Early Depth Test) vs Deffered Rendering**
근데 이게 전통적인 Forward Rendering보다 뭐가 좋은걸까?
(+) 딱 G-buffer를 만든 다음 눈 앞에 보이는 물체에 대해서만 고비용의 Shading 계산을 진행한다.

우리가 알고있는 전통적인 Rendering Pipeline을 보자. `강의자료 6 98p.`
Pixel의 최종 RGBA값과 Depth Z값, 그리고 Position 값이 들어오면 그 다음에 Blending이 일어나서 최종 pixel 값이 계산된다. 즉, Depth Test가 끝단에서 일어난다.

따라서 이 Depth Test를 앞쪽으로 넘기자라는 Idea가 나왔는데, 이가 바로 Early Depth Test이다.`Fragment Processing 전에 이 Depth Test를 하자.`
지금은 이 방법이 정형화되어서 인자로 지정해줘서 이 기능을 켜기만 하면 된다.
(-) 그런데 이 기능을 켜도 Fragment는 화면상에 랜덤으로 들어오기 때문에 뒤에있는 것부터 들어오게 되면 어차피 다 통과되게 되어버린다.
Deffered Shading은 이 단점까지 다 보완한 방법인것이다.

/ **A Larger Number of Lights**
![600](../../../../z.%20Docs/img/Pasted%20image%2020241112174115.png)
여기서 더 나아가면, 극한의 효율을 추구하는 게임업계에선 특정 범위까지만 영향을 미치는 지역광원은 아예 계산을 안하고 넘어간다.
그 구체적인 방법은 다음과 같다. `reference: Learn OpenGL 360p.`
$$\frac{5}{256} = \frac{i_{max}}{\text{Attenuation}}$$


$\begin{cases}\frac{1}{k_{0i}+k_{1i}||VP_{pli}||+k_{2i}||VP_{pli}||^2},\ P_{pli}'s\ w \neq 0 \text{ (point light)} \\ 1.0,\ P_{pli}'s\ w = 0 \text{ (directional light)}\end{cases}$

광원마다 광원의 좌표, 광원의 rgb값, 그리고 위와 같은 광원의 att식이 있다고 했을 때, 
광원의 att값이 사용자가 설정한 값보다 작아지는 distance를 계산해서 이 distance보다 멀면 아예 이 광원은 무시한다.

실제 그 Distance를 계산하는 코드는 다음과 같다. `360p. 근해 공식 참고`
```cpp
float radius = (-linear + sqrt(linear * linear - 4 *))..
```

그리고 Fragment Shader에서 다음과 같이 거리 검사를 하고 광원을 무시한다.
```cpp
for(int i = 0; i < NR_LIGHTS; ++i) {
	if (distance < lights[i].Radius) {
		// Process Shading
	}
	else {
		// Do not Process Shading
	}
}
```

그런데 여기가 최적화의 끝일까? 아니다.
게임 업계를 무시하지 마라.. `돈이 달린 코딩은 다르다.`

/ **SIMD with QUDA**
reference: 📁 CSEG5483_강의자료_1 12p.
우리가 CSEG5483에서 주구장창 배운 것처럼 Shader를 SIMD하게 돌려 더욱 더 최적화한다.
QUDA가 등장하게 된 계기가 그래픽 프로그래밍이라고 할 수 있겠다.

(!) 그런데 실제 코드를 보면 for문도 if문의 일종이고 안에서 distance 검사를 하는 것도 if문이라 if문이 중첩되어 SIMD 형태로 돌리기가 매우 곤란해진다.
따라서 사람들은 다시 이 `if (distance < lights[i].Radius)`를 없앨까..? 를 고민한다.
이와 관련한 게 바로바로 Stencil Test이다.


-------------------------------

📆 24.11.14 Presentation Day

🟡
Low Poly Model? `FBX file 변환 및 Import`
.ppm -> .jpg or window로 바로 확인
Ramping
연산 최적화: AABB 검사

🟡
Blinn-Phong + (Spot + Light Attenuation) `Struct Spot, Attenuation`
Sphere 굴절률 2.4
stbi_set_flip_vertically_on_load(1)
OpenGL은 Texture가 아래서부터 저장이 된다. (CSE4170 Project1 에서 확인했었음)

교수님 曰:
	Pixel에다 Ray를 최소 256개씩 쏴서 Rendering을 해야지 이미지가 더 깨끗하게 나온다.

🟡

🟡
Triangular Mesh에 Bounding Box 추가
Reflect Ray
Refract Ray: 전반사 확인, 내부/외부 확인

교수님 曰:
	색유리 그림자 생성을 하려면 Path Tracing을 사용해야 함
	해상도?

🟡
교수님 曰:
	- Whitted Style의 한계: 유리의 그림자, 따라서 적당히 유리의 그림자는 밝게 해서 보완하는 방식으로 한다.
	- Multisampling: 영화 같은 데선 Ray를 256개 씩 쏘기도 한다. 최소 16개는 쏴야 최소 기준을 만족할 수 있을 것이다.
	- Pixel에 따라 Addptive하게 쏘기도 한다. `어떤 Pixel은 4개, 어떤 Pixel은 16개..`

🟡
교수님 曰:
	Relfection으로 반사된 Ray에서도 Shadow Ray를 쏴야 Shadow 계산 가능

🟡
교수님 曰:
	Self Intersection: 점박이

🟡
교수님 曰:
	- Triangular Mesh 계산 시 t값이 매우 예민하기 때문에 Epsilon 값을 실험을 통해서 잘 알아내는 것이 중요하다.
	- 반투명한 유리구에서 ray가 통과했을 때 Transparecy를 계산하는 공식이 있다.
	- Photon Mapping 기법

🟡
교수님 曰:


------------------------------------

# Stencil Test

reference: 📁 OpenGL_4.4_Pipeline.pdf, 📁 2403CSE4170_강의자료_5.pdf

`CSE4170 강의자료 5 86p.` 4사분면을 보면 다음을 볼 수 있다.
```
Stencil Test -> Depth Buffer Test -> Blending(RGBA)
```
Stencil Test는 초보자 단계에선 아직 쓸 일이 별로 없는, 상당히 전문적인 Test이다.
Depth Buffer Test에서 `glDepthFunc(GL_LESS)`는 Franment의 Depth값이 Depth보다 작으면 통과 시키라는 뜻이다.
Blending은 CSE4170의 추가 과제를 하며 해보았을 것이다.

[Framebuffer - OpenGL Wiki](https://www.khronos.org/opengl/wiki/framebuffer)
위 사이트를 봐보면, COLOR Buffer는 여러개 붙일 수 있지만, DEPTH Buffer와 STENCIL Buffer는 하나씩만 붙일 수 있다.
둘을 하나로 합쳐서 붙일수도 있다. `GL_DEPTH_STENCIL_ATTACHMENT`

[Stencil Test - OpenGL Wiki](https://www.khronos.org/opengl/wiki/Stencil_Test)
```
glStencilFunc(GLenum func, GLint ref, GLuint mask)
```
`glDepthFunc()`처럼, Test에 들어오는 Fragment를 상황에 따라서 어떻게 할 것인지를 결정하는 것이다.
참고로 Mask는 8bit이다.
`GL_LESS`로 예를 들자면 다음과 같이 연산되는 것이다.
```
GL_LESS
Passes if (ref & mask) < (stencil & mask)
```

다음과 같은 함수도 있다. `이렇게까지 한다고..? 싶은 함수다.`
```
glStencilOpSeparate(GLenum face, GLenum sfail, GLenum dpfail, GLenum dppass)
```
- `sfail`: Stencil Test에서 죽는 경우
- `dpfail`: Depth Test에서 죽는 경우
- `dppass`: 둘 다 통과해서 Blending까지 가는 경우

📄`5.advanced_lighting_deferred_shading_volu/8.2.deferred_shading.fs`를 봐보자.
`/ A Larger Number of Lights`에서 했던 얘기를 상기해보자.

[Tutorial 37 - Deferred Shading - Part 3](https://www.ogldev.org/www/tutorial37/tutorial37.html)
다음 사이트를 보며 Step by Step으로 위 과정을 어떻게 하는지 자세히 살펴보자.
바로 여기서 Stencil Test가 쓰인다.

![200](https://www.ogldev.org/www/tutorial37/light_volume.jpg) ![202](../../../../z.%20Docs/img/Pasted%20image%2020241119171947.png)  ![199](../../../../z.%20Docs/img/Pasted%20image%2020241119173449.png)

실제로 코드는 다음과 같이 작동한다. `**는 문법이 아니라 강조표시`
```cpp
virtual void RenderSceneCB()  
{  
    CalcFPS();  
  
    m_scale += 0.05f;  
  
    m_pGameCamera->OnRender();  
  
    **m_gbuffer.StartFrame();**  
  
    DSGeometryPass();  
  
    // We need stencil to be enabled in the stencil pass to get the stencil buffer  
    // updated and we also need it in the light pass because we render the light  
    // only if the stencil passes.  
    **glEnable(GL_STENCIL_TEST);  
  
    for (unsigned int i = 0 ; i < ARRAY_SIZE_IN_ELEMENTS(m_pointLight); i++) {  
        DSStencilPass(i);  
        DSPointLightPass(i);  
    }  
  
    // The directional light does not need a stencil test because its volume  
    // is unlimited and the final pass simply copies the texture.  
    glDisable(GL_STENCIL_TEST);**  
  
    DSDirectionalLightPass();  
  
    **DSFinalPass();**  
  
    RenderFPS();  
  
    glutSwapBuffers();  
}

void DSGeometryPass()  
{  
    m_DSGeomPassTech.Enable();  
  
    **m_gbuffer.BindForGeomPass();**  
  
    // Only the geometry pass updates the depth buffer  
    glDepthMask(GL_TRUE);  
  
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);  
  
    glEnable(GL_DEPTH_TEST);  
  
    Pipeline p;  
    p.SetCamera(m_pGameCamera->GetPos(), m_pGameCamera->GetTarget(), m_pGameCamera->GetUp());  
    p.SetPerspectiveProj(m_persProjInfo);  
    p.Rotate(0.0f, m_scale, 0.0f);  
  
    for (unsigned int i = 0 ; i < ARRAY_SIZE_IN_ELEMENTS(m_boxPositions) ; i++) {  
        p.WorldPos(m_boxPositions[i]);  
        m_DSGeomPassTech.SetWVP(p.GetWVPTrans());  
        m_DSGeomPassTech.SetWorldMatrix(p.GetWorldTrans());  
        m_box.Render();  
    }  
  
    // When we get here the depth buffer is already populated and the stencil pass  
    // depends on it, but it does not write to it.  
    glDepthMask(GL_FALSE);  // ★ 위 과정을 완료한 다음 Depth Buffer를 Read-only로 바꾼다.
}

void DSStencilPass(unsigned int PointLightIndex)  
{  
    m_nullTech.Enable();  
  
    // Disable color/depth write and enable stencil  
    m_gbuffer.BindForStencilPass();  
    glEnable(GL_DEPTH_TEST);  
  
    glDisable(GL_CULL_FACE);  
  
    glClear(GL_STENCIL_BUFFER_BIT);  // 0으로 초기화
  
    // We need the stencil test to be enabled but we want it  
    // to succeed always. Only the depth test matters.  
    glStencilFunc(GL_ALWAYS, 0, 0);  // 항상 통과

    // ★ 이게 주요 부분이다.
    glStencilOpSeparate(GL_BACK, GL_KEEP, GL_INCR_WRAP, GL_KEEP);  
    glStencilOpSeparate(GL_FRONT, GL_KEEP, GL_DECR_WRAP, GL_KEEP); 
    // Stencil Test, 그냥 내비둠 (GL_KEEP)
    // Depth Test, Back Face에선 1 중가시키고 Front Face에선 1 감소시킨다. (그림 참고)
  
    Pipeline p;  
    p.WorldPos(m_pointLight[PointLightIndex].Position);  
    float BBoxScale = CalcPointLightBSphere(m_pointLight[PointLightIndex].Color,  
        m_pointLight[PointLightIndex].DiffuseIntensity);  // 각각의 구 준비
    p.Scale(BBoxScale, BBoxScale, BBoxScale);  // 구마다 미치는 범위가 다름 (Scale)
    p.SetCamera(m_pGameCamera->GetPos(), m_pGameCamera->GetTarget(), m_pGameCamera->GetUp());  
    p.SetPerspectiveProj(m_persProjInfo);  
  
    m_nullTech.SetWVP(p.GetWVPTrans());  
    m_bsphere.Render();  
}

void DSPointLightPass(unsigned int PointLightIndex)  
{  
    m_gbuffer.BindForLightPass();  
  
    m_DSPointLightPassTech.Enable();  
    m_DSPointLightPassTech.SetEyeWorldPos(m_pGameCamera->GetPos());  
  
    glStencilFunc(GL_NOTEQUAL, 0, 0xFF);
    // 그림 3에서 세팅한 ref(0) 값과 비교해서 같으면 죽여라
    // 이제 광원 범위 안에서만 Shading이 일어난다. (Setup 완료)

    glDisable(GL_DEPTH_TEST);  
    glEnable(GL_BLEND);  // Blending 기능 켜라 (Default 꺼져있음)
    glBlendEquation(GL_FUNC_ADD);  
    glBlendFunc(GL_ONE, GL_ONE);  
    // CSE4170 강의자료 6 117, 122p. 참고
    // Source Factor (1, 1, 1, 1), Destination Factor (1, 1, 1, 1)
    // -> 각각의 광원에 대해서 Shading 된 Color가 누적된다.
  
    glEnable(GL_CULL_FACE);  
    glCullFace(GL_FRONT);
    // Front Face 제거, 안 하면 색이 타버린다. FF에서도 Shading이 한 번, BF에서도 Shading이 한 번 일어나기 때문이다.
    // Front Face를 제거하는 이유는 카메라 위치 때문이다 (이 부분 잘 모르겠다)
      
    Pipeline p;  
    p.WorldPos(m_pointLight[PointLightIndex].Position);  
    float BBoxScale = CalcPointLightBSphere(m_pointLight[PointLightIndex].Color,  
                                            m_pointLight[PointLightIndex].DiffuseIntensity);  
    p.Scale(BBoxScale, BBoxScale, BBoxScale);  
    p.SetCamera(m_pGameCamera->GetPos(), m_pGameCamera->GetTarget(), m_pGameCamera->GetUp());  
    p.SetPerspectiveProj(m_persProjInfo);  
    m_DSPointLightPassTech.SetWVP(p.GetWVPTrans());  
    m_DSPointLightPassTech.SetPointLight(m_pointLight[PointLightIndex]);  
    m_bsphere.Render();  // 이제 구를 그리기
    glCullFace(GL_BACK);  
  
    glDisable(GL_BLEND);  
}

virtual void RenderSceneCB()  
{  
    CalcFPS();  
  
    m_scale += 0.05f;  
  
    m_pGameCamera->OnRender();  
  
    **m_gbuffer.StartFrame();**  
  
    DSGeometryPass();  
  
    // We need stencil to be enabled in the stencil pass to get the stencil buffer  
    // updated and we also need it in the light pass because we render the light  
    // only if the stencil passes.  
    **glEnable(GL_STENCIL_TEST);  
  
    for (unsigned int i = 0 ; i < ARRAY_SIZE_IN_ELEMENTS(m_pointLight); i++) {  
        DSStencilPass(i);  
        DSPointLightPass(i);  
    }  
  
    // The directional light does not need a stencil test because its volume  
    // is unlimited and the final pass simply copies the texture.  
    glDisable(GL_STENCIL_TEST);**  
  
    DSDirectionalLightPass();
    // Directional Light는 위 기법이 적용 안돼서 걍 전체를 계산한다.
  
    **DSFinalPass();**  
  
    RenderFPS();  
  
    glutSwapBuffers();  
}
```

# Optix

필요 사양: RTX GPU
reference: 📁 optix_quide_241022_A4.pdf
[NVIDIA-Turing-Architecture-Whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf)

이제 본격적으로 CPU가 아닌 GPU로 돌아가는 Ray Tracer를 만들어보자.

먼저 NVDIA GPU의 역사를 Brief하게 살펴보자.
```
Turing `RT Cores` -> Ampere -> Ada(Lovelace) -> Blackwell
```

RT Core가 여기서 빠질 수는 없다.

`📁 NVIDIA Turing Architecture white paper.pdf 26p.`
Graphics Pipeline을 살펴보자.
Vertex Shader, Geometry Shader, Fragment Shader + Compute Shader
이것 끼리는 Graphic Memory를 Share하기 때문에 빠르지만 VS, GS, FS는 CUDA와 Data를 주고받는데 상당한 시간이 걸린다.
따라서 CUDA와 빠르게 소통하려면 Compute Shader로 병렬 계산을 해야한다.

다시 Optix Guide `6p.`로 돌아와서,

`📁 RT_강의자료_2.pdf 23p.`
/ **BVH**
Bounding Volume이란 Triangles와 Ray의 교차계산 속도 최적화를 위해 Object를 AABB로 둘러쌓아 놓은 것이다. `다른 모양도 가능하지만 최근엔 거의 다 AABB를 씀`
그리고 이를 Tree구조로 만들어 계층구조를 만든 것이 Bounding Volume Hierarchy이다.

먼저 Ray를 나타내는 Built-In Variable을 상기하고 가자.
- e
- d
- tmin, tmax

![350](../../../../z.%20Docs/img/Pasted%20image%2020241121171009.png)
`tmin`과 `tmax`는 위 그림처럼 늘어들었다, 줄어들었다 하며 BVH Traversal에 매우 중요하게 쓰인다.

BVH Traversal
```sudo
BVH_traversal(ray, node) {
	if (ray does not hit node’s bv) return NULL;
	if (node is a leaf node) {
		intersect ray with every primitive in leaf;
		return the closest hit if any, or NULL otherwise;
	}
	else {
		hit_l= BVH_traversal(ray, left child node of node);
		hit_r= BVH_traversal(ray, right child node of node);
		return the closer of hit_land hit_r;
	}
}
```

kd-Tree Traversal
```cpp
void Traverse() {
	( t_near, t_far ) = ( Epsilon, ray.t_max );
	( t_near, t_far ) = Clip(t_near,t_far);
	node = rootNode; if (t_near > t_far) return; // ray misses bounding box of object while (1) { while (!node.IsLeaf()) { // traverse ’til next leaf d = (node.split- ray.org[node.dim]) / ray.dir[node.dim]; if (d <= t_near) { // case one, d <= t_near <= t_far-> cull front side node = BackSideSon(node);
	} else if (d >= t_far) {
	// case two, t_near <= t_far <= d -> cull back side node = FrontSideSon(node); } else { // case three: traverse both sides in turn stack.push(BackSideSon(node),d,t_far); ( node, t_far ) = ( FrontSideSon(node), d ); } } // have a leaf now IntersectAllTrianglesInLeaf(node); if (t_far <= ray.t_closesthit) return; // early ray termination if (stack is empty) return; // noting else to traverse any more...
	( node, t_near, t_far ) = stack.pop(); } }
```

이제 다시 Optix Manual로 돌아와서 이제 다음 자료를 보며 Ray Tracing Pipeline에 대한 개념을 정확히 잡고 넘어가자.

![300](../../../../z.%20Docs/img/Pasted%20image%2020241121172029.png)
![](../../../../z.%20Docs/img/Pasted%20image%2020241121171457.png)

Traversal을 할 때 Leaf Node에 도달했을 때 교차 계산을 하게 되는 Geometry Primitive의 종류는 다음과같다.
삼각형, 구, 육면체, 기하학적 도형, 심지어 연기도 가능하다. `연기는 그를 감싸는 AABB의 형태`
- Q) 연기 같은 경우는 어떻게 탐색하나? A) 우리가 프로그램을 짜야하는데 아주 살짝만 얘기하자면 프렉탈을 이용한다.

이 Primitive들과 교차계한을 해주는 것이 **Intersection Shader**이다.
- ★ 위 Primitive 중에서 "삼각형"만 Intersection Shader가 Default로 제공된다. `Built-in, 위 그림의 초록색에 해당하는 부분`

**No Intersection**: AABB와는 교차했는데 그 안의 Primitive와는 교차하지 않는 경우
**Closest Hit?**: 지금까지 찾은 교차점 중에 가장 가까이 있는가? 바로 여기서 tmin, tmax가 쓰인다.
**Opaque?**: 복잡한 Geometry를 덮고있는 Primitive에 Ray가 도달했을 때 가리키는 곳에 실제로 Geometry가 있는지 판단하는 것
![350](../../../../z.%20Docs/img/Pasted%20image%2020241121173235.png)
**Any-Hit Shader**는 Opaque하지 않을 때만 쓰인다.

이제 비로소 Traversal이 끝나면, 그제야 **Closet-Hit Shader**와 **Miss-Shader**가 쓰인다.
- Closest-Hit Shader는 실제 Shading 계산을 행해주는 것이고
- Miss-Shader는 맞지 않았으니까 배경색으로 칠해주면 된다.

이제 예제 코드를 봐보자.
`📄 optixWhitted.sln`
Optix에선 Intersection Shader라는 것을 지정해주기 위해 꼭 `__intersection__`이라는 어미를 넣어야 한다.
```
extrn "C" __global__void __intersetcion__sphere_shell()
```

Intersection이 쉬울 것 같은가?
당장 두꺼운 유리 겹으로 되어있는 구를 생각해보자.
벌써부터 Intersection이 복잡해진다. 실제 코드도 상당히 더러운 것을 볼 수 있다.

이제 본격적으로 다음 사진을 나타내는 프로그램을 살펴보며 OptiX에 대해서 배워보자.

![450](https://sites.williams.edu/scientephic/files/2015/10/Nvidia_Optix_CUDA_GPU_Raytracing_Whitted_Hi-Res_01.jpg)

OptiX는 NVIDIA가 개발한 GPU 가속 광선 추적 엔진으로, 광선 추적 알고리즘의 구현을 단순화하고 성능을 극대화하기 위해 설계된 플랫폼이다.
Turner Whitted의 1980년 Rendering 처럼 반사, 굴절, 그림자를 포함한 물리적으로 정확한 광선 추적 장면을 구현하는 데 강력한 도구로 사용된다.

![](../../../../z.%20Docs/img/Pasted%20image%2020241121171457.png)
앞에서 했던 위 Pipeline을 머릿속에 확실히 넣고 가자.

[NVIDIA OptiX 8.1](https://raytracing-docs.nvidia.com/optix8/index.html)

CUDA Core랑 RT Core 둘 다 쓴다.
- CUDA Core: SIMD 형태의 Inst 가속
- RT Core: VBH 가속

#### `createContext()`
> 최초의 Context 생성

`📄 Whitted.cpp`
```
1003  sbt: Shader bundung Table
1004  묶어낼 픽셀들의 해상도 정보, width
1005  묶어낼 픽셀들의 해상도 정보, height
1007  Depth
```

`📄 camera.cu`
raygen shader가 정의되어있다. 

`optixGetLaunchIndex()`: `[i][j][k] := (x, y, z=1 (Depth) )`

[Device API - optixTrace](https://raytracing-docs.nvidia.com/optix8/api/group__optix__device__api.html#ga11c7984d825b2a597e26a2a902386bbc)
`optixTrace(params.handle, ray_origin ray_direction, params.scene_epsilon, 1e16f, OprixVisibilityMask(1), OPTIX_RAY_FLAG_NONE, whitted::RAY_TYPE_RADIANCE, shitted::RAU_TUPE_COUNT, whitted::RAY_TYPE_RADIANCE, float3_as_args(prd.result), reinterpret_cast<insigned int&>(prd.improtance), reinterpret_cast<unsigned int&>(prd.depth));`
- `params.handle`: BVH에 대한 Pointer
- 
- `whitted::RAY_TYPE_COUNT`
- `whitted:RAY_TYPE_RADIANCE`: 허공을 뚫고 지나갔을 때 

보면 Parameter가 상당히 많은데, 앞부분의 Param들은 고정된 Attribute 값이고 아래의 Param은 값이 고정되지 않는 Ray의 Payload에 해당한다.
- `reinterpret_cast<unsigned int&>(prd.depth)`: Payload, Ray로 계산된 정보를 받아오는 것, Attribute(orig, dir 등)와 헷갈리지 말자.
- `prd.result`: Payload, RGBA 결과값을 받아 픽셀에 색을 칠할 용도
- `importance`: Payload, Ray를 언제 끊어버릴 것인가

일단 `params.handle`와 관련하여 BVH를 어떻게 만들지부터 시작하자.

/ **Making BVH (Accelaration Sturcture)**
가장 처음으로 장면에 대해서 BVH를 만들어야 한다.

Vulkan에선 다음과 같고,
- TLAS(Top-Level Acceleration Structure)
- BLAS(Top-Level Acceleration Structure)
OptiX에선 다음과 같다.
- GAS(Geometry Acceleration Structure)
- IAS(Instance Acceleration Structure)

Vulkan
![450](../../../../z.%20Docs/img/Pasted%20image%2020241126171046.png)
BLAS하나를 갖가지 Modeling 변환을 통해 TLAS_1, TLAS_2, TLAS_3..을 만든다.
BLAS를 세상에다 배치할 때 어떤 변환을 통해 TLAS로 배치할지에 대한 정보가 각각의 TLAS에 붙어있다.

OptiX
![450](https://raytracing-docs.nvidia.com/optix8/guide/pages/img/traversables_graph.jpg)
다음과 같이 BVH를 단순하게 Two-Level로 하는 Vulkan보다 다소 복잡하게 구성할 수 있다.

OptiX에서 Acceleraton Structure를 제공하는 Geometry들은 다음과 같다.
```OptiX8.1 Programming Guide 5. Acceleration Structures
The following build input types are supported:

Instance acceleration structures
OPTIX_BUILD_INPUT_TYPE_INSTANCES
OPTIX_BUILD_INPUT_TYPE_INSTANCE_POINTERS

A geometry acceleration structure containing built-in triangles
OPTIX_BUILD_INPUT_TYPE_TRIANGLES // 삼각형

A geometry acceleration structure containing built-in curve primitives
OPTIX_BUILD_INPUT_TYPE_CURVES // 커브(머리카락..?)

A geometry acceleration structure containing built-in spheres
OPTIX_BUILD_INPUT_TYPE_SPHERES // 구

A geometry acceleration structure containing custom primitives
OPTIX_BUILD_INPUT_TYPE_CUSTOM_PRIMITIVES // 내 맘대로 커스텀
```
우리가 볼 예제 코드에선 다른 건 안 쓰고 `OPTIX_BUILD_INPUT_TYPE_CUSTOM_PRIMITIVES` 즉, 커스텀 Acceleration Structure를 쓰고있다. 이 Structure는 다음과 같이 만들어진다.
- 부가적인 정보없이 AABB만 형성한다. `x_min, x_max, y_min, y_max, z_min, z_max`
- 충돌 시 AABB 안에서의 충돌 여부는 우리가 직접 짠 Intersection Shader가 필요하다.

#### `createGeometry()`
> Acceleration Sturcture 구축

이제 본격적으로 봐볼까?
`📄 Whitted.cpp`
```
917  cuda를 최초로 호출하면 자동적으로 Context를 만들어준다. cudaFree(NULL)은 아무 의미 없음
925  OptiX Contex를 만들 때 먼저 Cuda Context를 만들고 이를 같이 전달해야한다.
1111  여기서 Acceleration Structure를 만든다.

createGeometry()
388 AABB Type의 Array를 만들고 있다 OBJ_COUNT는 전체 Scene을 구성하는 물체의 개수, 여기선 3개이다. sphere_bound()는 구에 대한 min max를 계산해주는 함수이다.
393-399  cuda 함수를 써서 CPU에서 GPU, 즉Device로 보내주고 있다.

// Setup AABB build Input
405  0번 물체(불투명 구)에 대해서는 anyhit shader 부르지 마라. (Pipeline 상기)
407  1번 물체(glass 구)에 대해서는 anyhit shader를 한 번만 불러라.
409  2번 물체에(불투명 바닥) 대해서는 anyhit shader 부르지 마라.

...

// "뭐"에 대해서 Acceleration Sturcture를 만들어라 = Build Input
423  아까 본 Acceleration Structure Type 중 내맘대로 타임
내 맘대로 타입에 어떤 정보를 넣어야 하는지는 OptixBuildInputCustomPrimitiveArrau의 정의를 보자.
- d_aabb: AABB 포인터
- numSbtRecords: Intersection Shader, Closest Shader, Anyhit Shader를 각각의 물체마다 붙여주고 말고 해야하는데, 일단 여기서는 각각의 물체마다 따로따로 세 종류를 쓰겠다고 지정하고 있다. 하나만 쓰고 싶으면 NULL로 지정해주면 된다.
Primary Ray Shadow Ray
- d_sbt_index: 물체 순서대로 어떤 index의 sbtRecord를 쓸지 지정해주는 것

```

위 d_sbt_index를 어떻게 
$$
\begin{align}
\text{sbt-index}& = \text{sbt-instance-offset}\\
    &+ (\text{sbt-geometry-acceleration-structure-index} \times \text{sbt-stride-from-trace-call}) \\
    &+ \text{sbt-offset-from-trace-call}
\end{align}
$$

이 코드를 예로 들면 Glass Sphere에 Primary Ray가 부딪혔다고 치면 다음과 같은 값이 들어가 있다.
$$\text{sbt-index} = 0 + 1 \times 2 + 0$$
각각의 항에 어떤 값이 들어갈 지는 `optixTrace()` 함수에서 지정해줄 수 있다..

이해가 잘 가지 않으면 다음 Programming Guide를 보자.
[7.3 - Acceleration structures](https://raytracing-docs.nvidia.com/optix8/guide/index.html#shader_binding_table#accelstruct-sbt)
![600](../../../../z.%20Docs/img/Pasted%20image%2020241128173204.png)
위 표는 우리가 하는 예제 코드와 달리 Build Input이 하나가 아닌 경우이다.

Shader Bingding Table의 Content는 다음과 같다.
```
SBT
[0]
PR: Color Shader { IS, AS, CS }  // { Intersection Shader, Anyhit Shader, Closest Shader }
SR: Shadow Shader { IS, AS< CS }

[1]
PR: Color Shader { IS, AS, CS }
SR: Shadow Shader { IS, AS, CS }

[2]
PR: Color Shader { IS, AS, CS }
SR: Shadow Shader { IS, AS, CS }
```
어떤 물체는 `SBT[0]`으로 계산하고, 어떤 물체는 `SBT[1]`로 계산하고.. 하라는 것이다.

Ray Type `whitted::RAY_TYPE_RADIANCE`
- Primary Ray: Ray가 때렸을 때 생성하는 색깔을 알아내기 위한 Ray
- Shadow Ray: 광원을 향해 때려서 그림자 생성 여부를 알아내는 Ray

각각의 shader를 가리키는 포인터는 OptiX에서 다음과 같이 정의되어있다.
- raygenRecored
- missRecordBase
- hitgroupRecoredBase

매 프레임마다 OptiXLaunnch가 들어가게 된다. 이를 이루고 있는 주요 요소에 대한 설명은 다음과 같다.
- pipeline: 현재 쓸 Raytracing Pipeline
- stream: CUDA Stream 지정
- sbt: Shader Binding Table

`📄 Whitted.cpp`
```
435  압축기능 (삼각형이 3000만개다.. 이럴 때 쓰는거지 이 예제는 그냥 넣은 것 뿐)

440  buildGas()

// buildGas()
358  output Data를 압축했을 때 예상되는 사이즈
366  암축 후의 사이즈가 확실히 차이가 날 경우에만 압축해라
```


우리는 이제 겨우 OptiX에서 쓸 Ray Tracing Pipeline을 어떻게 구축할 것인지에 대해서 이야기 해볼 것이다.
그것은 위 Ray Tracing Pipeline 그림에서 보았던 Acceleration Structure Traversla Loop가 작동할 수 있도록 Pipeline을 구축하라는 것이다.

#### 📗 HW3
Learn OpenGL Code 8.1, 8.2
구에 해당하는 다면체 모델이 필요함. `삼각형으로 이루어진 구`
- Trangle info: x, y, z, x, y, z, x, y, z /
- normal: ...
Instancing
: IAS(Instance Accleration Structure), GAS하나로 M변환만 통해서 무한 복제 (Crowd)

다시 코드로 돌아와서,
#### `createPipeline()`
> Ray Tracing Pipeline 구축

optixPipelineCreate()
각 물체들을 다루기 위한 Shader Program들을 미리 컴파일 시켜서 pipeline compile options와 pipeline link optoin를 지정해준 후 `optixPipelineCreate()`로 Pipeline 구축
- Shader Array: Shader Program이 들어가 있는 배열
- Compile Option: Traverse하는 Geometry 종류에 따라 Optimize
- Link Option: Ray의 Max 반사 Depth, hmaxTraceDepth가 바로 여기서 지정된다.

LaunchParams params 어디 가면 있고 사이즈는 몇이라고 알려준다.

BVH
- Build: 처음부터 다시 다 빌드한다.
- Update: Physical Simulation 시 Vertex에 변형이 일어나면 Tree 구조는 거의 그대로 유지한 채 부분적으로 변형 가능하도록 한다.
하지만 계속 Update할수록 Traversal 비용이 커지기 때문에 Update하다 Build하고 다시 계속 Update하는 식으로 한다.

`optixDynamicGeometry.cpp`를 참고하면 된다.
```
446  처믕에 만들 때 Build 한 번 한다. ALLOW_UPDATE Option을 켜줘야 한다.
421  매 Frame 당 변화가 일어나면 Update한다.
```

📗 HW4
	Update하는 동적인 물체: Ben 같은 걸 삽입

UVW := XY-Z

Launch Parameter`param`를 매 Frame Setup 한 뒤 launchSubFrame을 호출한다.

Supersampling: 픽셀 당 Ray를 여러 번 쏘겠다.
- Regular: 각 픽셀당 격자무늬로 잘라서 Ray를 보낸 다음 Weight를 주는 것
- Stochastic: 픽셀에 Random 위치에 Ray를 보낸다.
우리 눈은 규칙적인 alias에 더 민감하기 때문에 Stochastic이 더 자연스러운 방법으로 선호된다.
![300](https://pbr-book.org/4ed/Sampling_and_Reconstruction/pha08f16.svg)  ![150](../../../../z.%20Docs/img/Pasted%20image%2020241205172215.png)
▶ Jitter, Poisson Disk, Stratified Jitter

`camera.cu`
```
69  들어온 거 그냥 평균 내면서 Accumulation Buffer에 누적 하고있다.
```
- `lerp(A, B, t)` = $(1 - t)A + tB$
- $\frac{nA + B}{n+1} = (1-\frac{1}{n+1})A + \frac{1}{n+1}B = (1-t)A + tB$

/ Polygon Model
먼저 다음 코드를 보자.
[optix7course/example07_firstRealModel at master · ingowald/optix7course](https://github.com/ingowald/optix7course/tree/master/example07_firstRealModel)
`SampleRendere.cpp`
GAS 만들 때 각각의 모델에 대해서 For Loop이 도는데 Vertex Buffer랑 Index Buffer 읽어들이고
- Vertex Buffer: `x y z, x y z, ...`
- Index Buffer: `3 5 7, 3 6 11, ...`
`121-129` Vertex Buffer, Index Buffer Setting
`135` SBT Entry가 하나이므로 다른 Parameter 설정을 안해줘도 된다. `=0`

그럼 Polygon Model은 어떻게 할까?
SBTRecord: Shader Binding Table의 각각의 원소

[Optix 7 - Shader binding table](https://velog.io/@sunbei00/Optix-7-Shader-binding-table)

#### `createSBT()`
> Shader Binding Table 준비

Shader Binding Table(SBT)

```cpp
tyoedef struct OptixShaderBindingTable {
	CUdeviceptr raygenRecord;
	
	cudeviceptr missRecordBase;
	unsigned int missRecordStrideInBytes;
	unsigned int missRecordCount;
	
	// ★ Hit group (Intersection Shader, Any-hit Shader, Closest-hit shader)
	cudeviceptr hitgroupRecordBase;
	unsigned int hitgroupRecordStrideInBytes;
	unsigned int hitgroupRecordCount;
};

// ---------------------------------------
//   reco0 |  reco1  |  reco2  |  reco3..
// --------------------------------------- RecordCount
// { Stride }
```

```
// Ray type = { 0, 1 } = { Primary Ray, Shadow Ray }
// GAS = { Metal Sphere, Glass Spere, Floor }

Metal Sphere    0
				1
Glass Sphere    0
				1
Floor           0
				1
```

Shader Binding Table Record
```
// Header = 32byte

header `optixSbtRecordPackHeader(state.???group, ...)`
IS: __intersection__ spere
SH: __slosesthit__metal_radiance
AH: nullptr

data `???group_records[sbt_table_idx].data`
// 아무거나 타입의 다양한 추가 정보들 (길이, 높이 등)
```
- 각각의 Record에 대해 Header Packing`header` -> Geometry Setting`data`이 이루어진다.

```
// Device Memory
------+------+------+-------
------+------+------+-------
Launch Params      SBT
```

```
60  optixReportIntersection(t, 0, float_as_args(n), __float_as_uint(a1), __float_as_uint(a2))
// t값에 따라 유리 구 안에서 어떤 종류의 굴절/반사가 일어나는지 분류
```
- Intersection Shader에서는 해당 Intersection이 어떤 것인지 알아낼 정보들을 전달하는 것이 중요하다.
- 이 Intersection Shader가 상황에 따라선 앞서 배웠던 것처럼 Any Hit Shader를 부를 수도 있다.
- Closest-Hit Shader

```cpp
// Floor의 체크무늬를 만든다.

float2 texcoord = make_float2(__uint_as_float(optixGetAttribute_3()), __uint_as_float(optixGetAttribute, 4()));
float2 t = textcoord * checker.inv)checker_size;
tx = floorf(t.x);
ty = floorf(t.y);

int which_check = (static_cast<int>(t.x) + static_cast<int>(t.y)) & 1;

if (which_check) {
	Kd = checker.Kd1;
	Ka = checkerKa1;
	Ks = checker.Ks1;
	Kr = checker.Kr1;
	phong_exp = checker.phong_exp1;
}

// 4 5 6 7 8 9
// 3 4 5 6 7 8
// 2 3 4 5 6 7
// 1 2 3 4 5 6
// 0 1 2 3 4 5
// 와 1로 Bit-wise연산을 한다.

// 뭔가 고능해보이는 코드다..

..
world_normal // IAS - 모델링 좌표계에서 세상 좌표계로 다시 보내주고 있다.
faceforward(world_normal, )
```

Primary Ray (importance = 1.0, depth = 0)를 처음에 쏜다.
- `importance`: `new_importance = prd_importance * luminance(p_Kr)`
- `depth`: 빛이 반사된 횟수
여기서 p_Kr이란 RGB값을 합하여 Gray Scale을 적용한 것으로, 빛이 반사된 세기, 즉 광도를 나타낸다고 생각하면 되겠다.
빛이 반사될 때마다 빛의 Importance와 depth가 업데이트 된다.
이 importance와 depth를 가지고 Ray를 또 쏠지 말지 결정한다.

결과는 다음과 같이 계산한다.
`reculs += p_Kr * traceRay(origin, direction, depth, new_importance)`

Miss 했다는 것은 새로운 버전의 OptiX에 탑재되어있는 feature를 이용하여 Background Color와 비교하여 알아낸다.

다음 코드는 무엇을 의미할까?
```cpp
const vool do_refine = fbsf(root1) > (10.0f * radius);
```
![200](../../../../z.%20Docs/img/Pasted%20image%2020241212164924.png) 다음 그림의 두 번째 같은 경우이다.

/ Metal Sphere 반사
$t\in [t_{min}, t_{max})$
optixReportIntersection에서 때린 걸 찾고 `t_min`과 `t_max`가 들어온다.
hitKind 정보와 함께 Ray에 대한 Attribute를 통해서 Report한다.
- Any-Hit 호출 O
	- optixIgnoreIntersection: `t_max`가 물체의 위치로 업데이트를 함 `1`
	- optixTerminateRay `2`
	- NULL `아무것도 안 부름`, True Return(아무것도 안 했다는 뜻) `3`
- Any-hit 호출 X
	- NULL `아무것도 안 부름`, True Return(아무것도 안 했다는 뜻) `4`

/ Glass Sphere 반사
![300](../../../../z.%20Docs/img/Pasted%20image%2020241212173140.png)
① OO (Outside Outside)
② IO (Inside Outside)
③ II (Inside Inside)
④ OI (Outside Inside)

`bear_attuenuation`: Absorption 효과, 유리의 가장자리로 갈 수록 진해보이는 효과

/ Triangle Mesh `📄 optixMeshViewer`
- `📄 whitted.cu`
- t, hitKind, attribute (($\alpha, \beta$), baryCentric Coord)
- 삼각형을 때리면 몇 번째 삼각형을 때렸는지 Index로 받아올 수 있다.
- 그렇게 삼각형의 꼭짓점 `p0, p1, p2`를 받아와 $p = (1 - \alpha - \beta)\cdot p_0 + \alpha\cdot p_1 + \beta\cdot p_2$를 계산한다.


# Volume Rendering Equation

$$\begin{align}\text{Absorption} + \text{Out-Scattering} &+ \text{Emission} + \text{In-Scattering} \\
= \text{Extinction} &+ \text{Emission} + \text{In-Scattering}\end{align}$$
/ Absorption
$$\begin{align}L(S^*) &= L_o \cdot e^{-\int^{S^*}_0\sigma_a(s)ds}\\&=L_o\cdot T(S^*)\end{align}$$
/ Out-Scattering
$$L(S^*) = L_o\cdot e^{-\int^{S^*}_0\sigma_s(s)ds}$$
/ Extinction

$$L(S^*) = L_o\cdot e^{-\int^{S^*}_0\sigma_t(s)ds}$$
/ Emission

$$L(S^*) = L_o + \int^{S^*}_0g(s)ds$$
/ In-Scattering

$$L(S^*) = L_o + \int^{S^*}_0s(s)ds$$
