# 수치컴퓨팅및응용

#### Introduction

수학적 증명 보다는 Programming $ Assignment 위주
[교재: Numerical Algorithms](https://people.csail.mit.edu/jsolomon/share/book/numerical_book.pdf)
실습 환경: Conda
언어: IPython

#### Overview

- Introduction
- Linear System I: LU Factorization
- Linear System II: Conditioning and Least Square
- Linear System III: Regularization and QR Factorization
- Effeicient Numercial Code
- Eigen Decomposition and Singular Value Decomposition
- Non-Linear Systems
- Optimization and Auto-Differentiation 
- Neural Network
- Interpolation
- Integration
- Monte-Carlo Integration
- Inverse Graphics

#### Numerical Algorithms
> 어떤 함수나 방정식의 해를 컴퓨터를 이용해 수치적으로 근사해서 근삿값을 구하는 알고리즘에 대한 연구를 하는 학문이다.
> Discrete Algorithm과 어떻게 보면 반대라고 생각할 수 있다.
- Big Floating-Point Data
- Three=dimensional Scanning
- Other Scientific Applications `Vehical Control, Thermal Control`

#### 목적
실제 세계의 문제에 어떻게 접근할까? 를 배운다고 생각하면 된다.
복잡한 문제에 접근하고, 수치적 모델로 축약, 알맞은 수치 알고리즘 선택, 동작 할지 말지 예상, 실행 까지의 단계를 배울 것이다.

# 1. Introduction

#### Background
> 배우기에 앞서 필요한 표기에 대해서 알아보고 가자.

**Preliminaries**
$\mathbb{N} = \{1, 2, 3, ...\}$
$\mathbb{Z} = \{..., -2, -1, 0, 1, 2, ...\}$
$\mathbb{Q} = \{a/b:\ a, b \in \mathbb{z}, b \neq 0\}$
$\mathbb{R, Q}\subset \mathbb{R}, \pi, \sqrt{2}\in \mathbb{R}$
$\mathbb{C} = a+bi:\ a, b \in \mathbb{R} \text{where} i \equiv \sqrt{-1}$


**Vector**

**Linear Systems in Matrix form Ax = b**
방정식을 행렬의 형태로 정형화 시킨 것
선대를 상기하자.

**Geometrical Interpretation(GI) of Ax = b**

**Changing the Viewpoint**
현재의 위치에서 매핑된 점 b'가 있을 때 그 점이 b와 얼마나 가깝냐가 현재 알고리즘의 정확도를 따지는 척도가 된다. 이를 줄여나가는 것이 최적화의 과정인 것이다.

# 1. Numerics and Error Analysis

```cpp
float x = 1.0;
float y = x / 3.0;
if ( x == y * 3.0 ) cout << "they are equal.";
else cout << "they are not equal.";
```

둘 중 뭐가 나올지 알 수 없다.
컴퓨터에서의 실수 표현은 우리가 생각하는 것과는 다르다.
실수를 근사한 형태로 다루기 때문에 에러가 계속 쌓이고, 임계치를 넘어가게 되면 오류가 발생하게 된다.
기초 GPU 프로그래밍에서 배운 floating point equation을 상기하자.

`float 말고 double로 하면 오류가 줄어들까..?`

> Mathematically Correct != Numerically Sound

그렇다면 어떻게 코드를 올바르게 만들까? `limit.h`
다음과 같이 "같다"를 재 정의해준다.
```cpp
float x = 1.0f;
float y = x / 3.0f;
if( abs(x - y*3.0f) < numeric_limits::epsilon())
	cout << "they are equal.";
else cout << "they are not equal.";
```

`numeric_limit<float>::epsilon()`를 잘 다루는 것이 중요할 것이다.

그렇다면 실수를 우리가 어떻게 표현할까?

#### Fixed-Point
> 어떤 유한한 이진수의 나열 사이에 소수점을 딱 찍는다고 생각
> 원시적인 실수 모델이라고 생각하면 된다.

이러한 Representation은 특정 목적에 따라 쓰이게 된다.

더할 때는 문제가 없는데, 문제는 곱하기에서 발생한다.
26p.를 보자.

Scientific Calculation에서는 수치가 굉장히 크거나 작아지기 때문에, 이러한 숫자를 커버하게 될 수록 많은 비트가 필요하게 되고, 이는 비효율로 이어지게 된다.
Fixed Point는 아주 크거나 작은 수의 계산에 적합하지 않은 것이다.

#### Floating-Point
> Scientific Notation을 기반으로 하며,
> Fixed-Point의 단점을 보완한다.

$\pm (d_0 + d_1 \cdot \beta ^{-1} + d_2 \cdot \beta ^{-2} + \cdot\cdot\cdot + d_{p-1} \cdot \beta ^{1-p})\times \beta^b$ 

유효 숫자내에서는 정확한 계산이 가능하며
계산 가능한 숫자의 범위가 확장된다.

$110 = 1.10 \times 2^2$
$0.110 = 1.10 \times 2^{-1}$

$\beta ^{n+1}-\beta^{n}$의 차이를 $2^{p}$로 해당 exponent 사이의 간격이라고 생각하면 된다. 이가 커질 수록 가까운 수의 차이가 커지는 것이다.
따라서 필요한 것이 **Rounding Rule**이다.

$\pm 1.01001_2 \times 2^{10}$에서 사실상 정수 부분은 필요 없다는 것을 알 수 있다.

**Floating Data Type**
![500](../../../../z.%20Docs/img/Pasted%20image%2020240902124931.png)
Single Precision보다 Double Precision이 더 정확하다.
GPU의 floating point part를 상기하자.

**FP Calculation Cautions**
1. 비슷한 숫자 사이의 뺄셈
2. 아주 큰 수와 아주 작은 수와의 덧셈/뺄셈
3. 아주 작은 수로의 나눗셈
4. 기타

#### Source of Errors
> 수치해석학에서 발생할 수 있는 다양한 에러를 정의해보자.
- **Rounding**: 수의 체계의 한계 때문에 현재 숫자를 표현할 수 있는 수로 표현할 때 발생하는 오차
- **Discretization**: 미분 같은 계산 시 정확한 계산이 아니라 근사하게 되는 것
- **Modeling**: 모델 자체가 부정확할 때
- **Emporical Constant**: Constant가 에러를 지니고 있을 때
- **User Input**: 모델이 정확해도 Input 자체가 부정확 할때 `센서가 부정확하던가`

#### Absolute & Relative Error
- **Absolute Error**: 실제 값과의 값 차이 $2.00 \pm 0.1$
- **Relative Error**: 실제 값과의 비율 차이 $2.00 \pm 5\%$

34p.를 보며 위 오류에 대해 예를 들며 알아보자.
경우에 따라 둘 중에 어떤 에러를 선택할지가 달라질 것이다.

#### Indirect Measures of Success

$f(x_{est})$ 즉, 추정하는 해를 넣은 함숫값이 0에 얼마나 가까운지를 찾는 것
$x^2$과 $x^{\frac12}$라는 함수가 있다고 치면 같은 함숫값에 대해서
$f_1(x) = \sqrt{\epsilon}$
$f_2(x) = \epsilon^2$
같은 함수를 적용해도 함수의 형태에 따라 추정할 수 있는 해의 정확도에 차이가 나타난다.

이와 관련된 개념을 알아보자.

#### Conditioning

**Backword Error**
$|f(x_{est}) - f(x^*)|$
함숫값의 차이로 해를 알아간다

**Forward Error**
$|x_{set} - x^*|$
해의 차이로 해를 알아간다.

backword Erorr로 하는 수 밖에 없을 것이다.

**Well Conditioned**
> Backword Error가 작을 때 Forward Error도 작다고 알 수 있는 경우이다.
> Small Backword Error -> Samll Forward Error

$f(x + \epsilon) = f(x) + f'(x) \cdot \epsilon + \frac12 f''(x)\cdot \epsilon^2 + ?\epsilon^3$

**Condition Number**
> 해당 문제를 잘 풀 수 있을지에 대한 수치적 지표
> Condition Number가 작을 수록 문제가 Well-Conditioned인 것이다.

$$\frac{\text{forward error}} {\text{backword error}} = \frac{|(x+\epsilon)| - x}{|f(x+\epsilon)-f(x)|} \approx \frac{|\epsilon|}{|\epsilon f'(x)|} = \frac{1}{|f'(x^*)|}$$

![300](../../../../z.%20Docs/img/Pasted%20image%2020240902143351.png)

#### Example

Example: Computing the Length of Vector
```cpp
double normSuqared = 0;
for (int i = 0; i < n; i++)
	normSquared += x[i] * x[i];
return sqrt(normSquared);
```

Example: Computing the length of Vector (Improved)
```cpp
double maxElement = epsilon;

for (int i = 0; i < n; i++)
	maxElement = max(maxElement, fabs(x[i]));
for (int i = 0; i < n; i++) {
	double scaled = x[i] / maxElement;
	normSquared += scaled * scaled;
}
return sqrt(normSquared) * maxElement;
```

벡터의 일부 요소가 매우 큰 값이고, 다른 요소가 매우 작은 값이라면, 작은 값들의 제곱은 부동 소수정 연산에서 정밀도 문제로 인해 무시될 수 있다.
이를 위해 **정규화**라는 것을 수행한 것이 Advanced된 코드이다.
쉽게 말해 계산하는 범위를 작게 만듦으로써 수치적 안정성을 높이는 것이다.

벡터의 각 요소를 최대 요소로 정규화하여 계산함으로써 부동 소수점 연산의 정밀도 한계로 인한 오차를 줄이고, 보다 안정적이고 정확한 결과를 얻을 수 있도록 하는 것이다.
직접 계산하는 것보다 작은 값이 무시되는 문제를 줄여 더 정확한 벡터의 길이를 계산할 수 있다

과제 시 설명 추가하기

#### Python
> d

```python
A = [1, 2, 3]
B = A

B = A.copy()

# 둘은 다르다.
```

```shell
// 가상환경 만들기
> conda create -n "name" python=3.10

// 가상환경 들어가기
> conda activate test

// 가상환경에서 ipython 실행하기
> ipython

// 나가기
> exit

// Jupyter Notebook 실행
> \numerical
> jupyter notebook
```

# 2. LU Decomposition

## 2.1 Linear System

선형대수학의 기본적인 내용이다.

$A\vec{x} = \vec{b}$
$A\in \mathbb{R}^{m\times n}, \vec x \in \mathbb{R}^n, \vec{b}\in\mathbb{R}^m$

**Casses for Lineare System** `3p.`
1. One Solution
2. Infinite Solution
	- 점들이 $y = x$로 매핑된다.
	- 투영 될 점들이 dependent하다 -> 투영 된 점들의 차원이 줄어든다
3. No Solution
	- $y = x$로 매핑되는데 생뚱맞는 $(-1, 1)$을 표현하라 그러는 것과 마찬가지

그럼 왜 두 개의 Solution은 없는 것일까?
만약 해가 한 개 초과면 두 개만 있는게 아니라 분명히 다른 해들도 존재하므로 두 개의 해가 아니라 무한한 해가 존재하게 되는 것이다. `자세한 증명은 pdf 4p. 참고`

Linear System을 풀 수 있을지 말 지 판단할 때 $A$ 뿐만 아니라 $\vec{b}$도 잘 봐야한다.

$A$ Matrix의 모양에 따라 경우를 또 분류해보자.
![600](../../../../z.%20Docs/img/Pasted%20image%2020240909122944.png)
1. m > n (Tall Matrix): Solution이 존재하지 않을 수 있다.
2. n < m (Wide Matrix): Solution이 무한할 수 있다.

## 2.2 Solving Linear System

실제로 numpy에서 해를 구할 때 역행렬을 곱하여 해를 구하지는 않는다.
따라서 다른 방법으로 Linear System의 해를 구한다.
이에 대해서 설명해보겠다.

#### LU Decomposition
> 행렬을 하삼각행렬 $L$과 상삼각행렬 $U$의 곱으로 표현하는 수치해석학의 기술

#### Row Operations

우리가 손으로 풀 때는 단순하게 그냥 적으며 넘어가는 것들을 컴퓨터에서 계산시키려면 좀 더 자세하고 체계적인 과정 정립이 필요하다.
다음은 그런 과정의 정립에 대해서 설명하는 것이다.
주의할 접은 Matrix로 나타내는 계산 방법과 실제로 컴퓨터에서 수행되는 계산의 방법이 다르다는 것을 머리에 넣어놓고 가자.

1. **Row Permutation** $O(n)$
	- $\sigma(i)$: $i$번째 행으로 바꿔치기 할 것이다.
	- 보통 Identity 행렬을 곱하는 것으로 permutation을 나타내는데 이렇게 하면 행렬곱이 수행되는 것이므로 시간복잡도가 $O(nmk)$로 상당히 커지기 때문에 실제로 컴퓨터에서 행해지는 계산은 swap, 즉 $O(n)$이다.
2. **Row Scaling**
	- 대각선이 scaling할 숫자인 matrix를 곱해주는 것과 같다.
3. **Elimination** $E \equiv I + c e_je_i^{\top}$
	- row i에 c를 곱해서 row j에 더해주고 싶다는 연산을 $E$로 표현하기로 약속하자.
	- row i를 추출하고, c를 곱하여 scale한 다음, row j에 더해주면된다.

**Gaussian Elimination**
> `numpy`에 다 구현이 되어있다.
> 시간복잡도는 $O(n^3)$이다.

일단 Pivot을 잡고 Row Sacling부터 시작한다.
그다음 전 과정은 다음과 같다.
```
Forward Substitution -> Upper Triangular Form 
-> Backward Substitution -> Identity Matrix
```

**Pivoting**
> Pivot 위치에 너무 작은 값이 오면 계산 시 별로 좋지 않기 때문에 행, 더 나아가서는 열도 바꿔주는 연산
- Partial Pivoting: 행의 순서만 바꿔주는 것
- Full Pivoting: 행도 바꾸고 열도 바꿔주는 것, 열을 바꾼다는 것은 연산이 다음과 같이 바뀐다는 것이다.
	$A\begin{pmatrix}x \\ y \\ z\end{pmatrix} \rightarrow A\begin{pmatrix}x \\ z \\ y\end{pmatrix}$

이제 LU Decomposition에 대해서 알아보자.

#### LU DeComposition
L: Lower Triagular Matrix
U: Upper Triangular Matrix

Lower Triagular Matrix로 표현하기만 하면 Inverse를 계산하기 쉽다.
k개의 foward substitution matrices를 Inverse를 취해서 모두 곱하면 다시 L이 나온다.

1. Factorize A into two triangular matrices L and U s.t A = LU
2. Solve $L\vec{y} = \vec{b}$
3. Solve $U\vec{x} = \vec{y}$
$$\begin{align}&A\vec{x} = \vec{b} \\ &L(U\vec{x}) = \vec{b} \\ &L\vec{y} = \vec{b}\ (\because U\vec{x} = \vec{y})\end{align}$$

# 3. Least Square and Conditioning

reference: MAT2110 Note - 34p. At Least Square

#### Parametric Regression

observation을 따르는 Model을 찾는다.

어떤 현상에 대해서 여러번 관측을 하고 independent cariable의 값을 달리하면서 어떻게 값이 바뀌었는지 observation 값 y를 관찰하여 (x, y)의 pair를 모아 Matrix로 표현을 할 수가 있다.

$X^{T} \vec{a} = \vec{y} \Leftrightarrow A\vec{x} = \vec{b}$

예시로 Polynomial Regression이 있다.

Regression "in the wild"
현실은 완벽하지 않다. 현실에 도사리고 있는 Error는 다음과 같이 정리할 수 있다.
- Model Error: Model이 Observation을 표현하기에 너무 복잡하다. `Over fitting의 예시를 보자.`
	![300](../../../../z.%20Docs/img/Pasted%20image%2020240923123006.png)
	- 다음과 같이 점을 모두 정확히 지나는 복잡한 Model `실선`을 제시하는 것 보다 경향을 나타내는 단순한 Model `점선`을 제시하는 것이 낫다는 것이다.
- Measurement Error: 측정 오류

우리는 $Ax = b$의 정확한 값을 구하는게 아니라, 근사하는 형태로 값을 찾아나갈 것이다.
Over fitting을 생각해보자.
$$A\vec{x} \approx \vec{b}$$

그럼 어떻게 구할 것이냐? 그 방법에 대해서 설명해보겠다.

#### Least Squares

$A\vec{x}\approx \vec{b}$라는 것은, $A\vec{x}$와 $\vec{b}$의 거리가 가깝다는 것으로, 이를 이용할 것이다.
$||\vec{a}|| = \sqrt{\vec{a}\vec{a}} = \sqrt{\vec{a}^{\top}\vec{a}}$
$$A\vec{x} \approx \vec{b} \Leftrightarrow A^{\top}A\vec{x} = A^{\top}\vec{b}$$

#### Pertubation Analysis
> $A$ $\vec{b}$에 변화가 일어났을 때 $\vec{x}$ 가 어떻게 바뀌는지?

**Vector Norm**
vector norm의 정의는 나중에 rounding error 할 때 응용하게 될 것이다.

**Matrix Norm**
Frobenius Norm이라는 항이 있는데, 이거 말고 다음과 같이 Spectral Norm: Induced two-norm이라는 것을 새롭게 정의를 할 것이다.
$$||A|| = max\{ ||A\vec{x}||: ||\vec{x}|| = 1 \}$$

19p.
$(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$

20p.
$\frac{||\vec{x}(\epsilon) - \vec{x}(0)||}{||\vec{x}(0)||}$
변화값이 x 크기에 비해 얼마만큼 바뀌었는지 알고자 하는 것
증명은 pdf 69p. 참고

파란색 네모: A b의 변화가 크면 당연히 변화값도 커지게 된다.
노란색 네모: A Matrix의 꼴에 따라서도 condition number가 결정이 된다.
$$\vec{x}:\kappa \equiv ||A^{-1}||\ ||A|| = \text{cond} A$$
$\text{cond}A$는 $A$가 등방향이 아니라 (Identity Matrix) 다른 방향으로 커지게 되면 값이 늘어나는 경향을 보일 것이고, 이 값이 커지게 되면 오차가 커지게 된다.

21p.
Q) A Matrix에 Diagonal Matrix 를 곱하거나 하는 변형을 가해서 $\text{cond}A$를 줄일 수 있을까(오차를 줄일 수 있을까)?
A) $\text{cond}A$의 식을 보면 알 수 있듯이 Scaling을 해도 Inverse에서 상충되기 때문에 의미가 없다.

$A^{-1}\vec{x}$는 $A^{-1}$를 구하는 것보다 쉽다.

**QR Ractorization**

# 5. Efficient Numeric Code

## 5.1 Efficient Code for Single-Thread Computing

Register-Level Blocking: 계산의 중복성을 높여서 read 횟수를 줄인다

Unrolling and Scheduling: j<1000, 즉 1000번의 if문을 체크한다. Unrolling을 해주면 500번의 if만 해주면 된다.

Scalar Replacement: 왼쪽은 메모리 Access를 많이 하게되므로 scalar 값으로 바꿔서 Register Access를 대신 늘리자

Precomputation of Constants: 미리 계산 해놓고 시작

## 5.2 Efficient Code for Matrix-Matrix Multiplication

Naive Implimentation:
생각보다 n이 작을 때 계산하는 경우가 많으므로, 시간복잡도는 그리 크지 않지만 Memory Access 관점에서 보면 좋은 코드는 아니다.

Cache Miss Counting
한 원소를 접군하면 8개의 float 변수들이 따라온다.
과연 얼마나 많은 Cache Miss가 일어날까?
A에서는 그럼 8번째마다 Miss가 일어난다는 것이다. $\frac{N}{8}$
B에서는 세로로 읽어야되므로 N번의 Miss가 일어난다.
`>>`총 $\frac{9N}{8}$번 Cache Miss가 일어난다.

어? 그럼 Block Size만 커지면 되는거 아냐?
아니다. Restrict한 조건 한정이다.

## 5.3 Numeric Code Optimization

CUBLAS
CULAPACK
라이브러리 다 있다

CUBLAS는 Platform Dependent하다.

Broadcasting Examples
오른쪽부터 매칭하면서 똑같거나 둘 중 하나가 1이면 된다.
오른쪽부터 차원을 맞춰나간다.

View and Copy
Array에서 값을 가져올 때 View해서 그 부분을 Edit했을 때 같이 수정되지 않도록 Copy하자.

Reshape & Transposing
Reshape: 
Transposing: 

Space Managment is Important
Code 1은 할당을 계속하고있다.
Code 2는 미리 할당을 해놓고 10개씩 집어넣고있다.
`>>` Memory를 그때 그때마다 할당하지 말고 미리 쓸만큼 Dynamic Allocation을 해놓고 시작하자.

# 6. Eigenvectors

Statiscal Problems
교재의 증명 참고

$||X^{\top}\hat v||^2 = \hat v^{\top}XX^{\top}\hat v$

## 6.1 Eigenvectors

Shearing: 사각형이 찌그러짐

Complex Matrix: 복잡한 이동 표현 가능

어떤 벡터에 대하여 이런 Matrix를 곱해도 그냥 Scale만 될 뿐인데, 이를 Eigenvector라고 한다.

#### Eigen Vector
> 행렬 $A$에 대해, 변환 후에도 방향이 변하지 않는 벡터
> For some vector $\vec x$ and $A\in \mathbb{R}^{n\times n}, A\vec x = \lambda \vec x$ for scalar $\lambda$.
> Eigen vector $\vec x$, Eigen value $\lambda$

The Quadratic Form
Max(Quadratic Form의 Value) = Max($A^{\top}A$의 Eigen Value)

Eigen Problems
Constraint 함수와 Object함수를 미분했을 때의 vector의 방향이 같으면 (gradiant가 일치하면) 그 경계위에서의 점이 Critical Point이다.
Object - Constraint = $\vec x^{\top}S\vec x -\lambda(||x||_2^2 - 1)$ `15p.`

/ *Lagrange Multiplier Method*

Geometric Example

Non Square Metrix라 차원이 떨어져도 Eigen Vector로 같은 방법을 적용 가능하다.

## 6.2 Conputing Eigen Vectors

이제 우리는 Eigen Vector의 기하적 성질에 대해서 알아보았다.
그럼 이 Eiegn Vector는 어떻게 구할까?

Power Iteration

Reference: MAT2110 Note 44p. $\S\ 5.1$

하나의 Eigen Vector를 구하는 방법
- 제일 큰 Eigen Value를 갖는 Eigen Vector는 A를 무식하게 곱함으로써 알 수 있다.
- 그러니까 A를 곱해도 더이상 A가 바뀌지 않을 때
- $\lambda$도 $\lambda^k$로 같이 커지기 때문에 이를 처리해주는 부분이 또 필요하다. -> Normalize를 통해 크기를 일정하게 유지한다.
- 즉 강의자료 21p.에서 오른쪽 알고리즘을 골라야한다는 것이다.

$A^{-1}$를 구하고 싶다. 식을 재정렬하면 $A\vec x=\vec b$와 같은 form을 얻을 수 있으므로 Gausian Elimination을 하든 해서 구하면 된다.
즉 22p. 오른쪽 알고리즘과 같이 LU Decomposition이 되어있으면 더 편하게 구할 수 있다.

즉 Power Iteration은 별거 없고 A를 열심히 곱하자!라는 것이다.
두 번째 알고리즘은 Inverse를 구할 때 LU Decomposition또는 Gausian Elimination으로 바꿔서 단순화해서 풀자는 것이다.

/ *Shifting*
아무리 곱해도 수렴을 하지 않을 때 가속화하기 위해서 우리가 취할 수 있는 전략이다.
$\sigma$를 잘 추정해서 $A-\sigma I_{n\times n}$에 대하여 Eigen Vector를 구하자.
추정하는 방법을 모른다는 것이 단점인데, 이 부분에 대해서 25p.를 보자.

24p.에서 Ill Conditioning이라는 의미는 Inverse Iteration의 경우 분모에 0에 가까운 Value가 오기때문에 $\frac{c}{0}$꼴이 나기 때문에 그렇다는 것이다.

다음과 같이 Least-Square Approximation을 적용한 식을 풀어보면 우리는 $\sigma$를 추정할 수 있다.
$$\arg \min \limits_{\lambda}||A\vec c_0-\lambda\vec v_0||^2=\frac{\vec v_0^{\top}A\vec v_0}{||\vec v_0||^2_2}$$

`27p.` 그런데 $\vec v_0 \cdot \vec x_1=0$일때 즉, $\vec v_0 = c_1\vec x_1 + c_2 \vec x_2 + ...$ 에서 $c_1 = 0$인 vector가 들어왔을 때 Power Iteration을 취하면 어떻게 될까?
-> 두 번째 Eigen 벡터가 나온다.
즉, 처음에 뽑을 때 잘 뽑아야 한다는 뜻이다.

Computing Multiple Eigen Vectors
여러개의 Eigen Vector를 구하는 방법은 위의 상황을 참고하여 구한 Eigen Vector를 빼고 또 다음 Eigen Vector를 구하는 과정을 반복하면 된다.

📁 교재 Proposition 6.4
A가 대칭, 즉 Symmetric Matrix일때, Eigen Value가 다르면 Eigen Vector가 다르다.


QR Iteration

The Limit of QR Iterations
A와 Q가 같은 Eigen Vector를 가지고 있다.
$Q\vec x=\pm\vec x$

Eigenvector Structure of $A^k$
$\therefore Q_k$는 수렴한다.

----------------------

📆 2024.10.28

# 7. Singular Value Decomposition

`3p.`
$$\begin{align}A^{\top}A = S = S^{\top} \\
S\vec x = u\vec x \\
S\vec y = \gamma \vec y \\ 
u\vec x \cdot \vec y = S\vec x \cdot \vec y = \vec x \cdot S^{\top}\vec y \\ 
\vec x \cdot S\vec y = \gamma \vec x\cdot \vec y \\ 
(u - \gamma)(\vec x\cdot \vec y)\end{align}$$
$A^{\top}A$의 고유값이 ~이고
linear transformed 또한 수직이다.

`5p.`
/ *Singular Value Decomposition*

`Chat GPT: SVD(Singular Value Decomposition)`
### SVD
**SVD(Singular Value Decomposition, 특이값 분해)**는 임의의 행렬을 세 개의 특수한 행렬의 곱으로 분해하는 선형 대수 기법입니다. 이 방법은 행렬의 구조를 파악하거나, 차원 축소, 노이즈 제거, 데이터 압축 등에 널리 사용됩니다. 특히 데이터 분석, 컴퓨터 비전, 머신러닝의 여러 분야에서 유용합니다.

#### SVD의 정의

임의의 \( m \times n \) 행렬 \( \mathbf{A} \)는 아래와 같이 분해됩니다:

$$
\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^T
$$

- **$\mathbf{U}$ (왼쪽 직교 행렬)**: $m \times m$ 크기의 직교 행렬로, **A의 열 방향** 주성분을 나타냅니다.
- **$\mathbf{\Sigma}$ (대각 행렬)**: $m \times n$ 크기의 대각 행렬로, 행렬 $\mathbf{A}$의 **특이값**을 포함합니다. 이 값들은 행렬 $\mathbf{A}$가 가지는 변동의 크기를 나타내며, $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$ 순서로 정렬됩니다.
- **$\mathbf{V}^T$ (오른쪽 직교 행렬)**: $n \times n$ 크기의 직교 행렬로, **A의 행 방향** 주성분을 나타냅니다.

이 분해를 통해 행렬을 $m \times r$ 크기와 $r \times n$ 크기의 두 행렬 곱으로 나타낼 수 있습니다. 여기서 $r$은 행렬 $\mathbf{A}$의 **랭크**입니다.

#### SVD의 직관적 이해

SVD는 원래의 행렬을 회전(변환) 및 스케일링하여 데이터의 특성을 최대한 보존하면서 재구성하는 작업으로 볼 수 있습니다. 이를 통해 데이터의 주요 패턴이나 방향을 유지하면서 차원을 줄이거나, 데이터의 잡음을 제거할 수 있습니다.

#### SVD의 계산 과정

1. **특이값**은 $\mathbf{A} \mathbf{A}^T$와 $\mathbf{A}^T \mathbf{A}$의 고유값의 제곱근으로 계산됩니다.
2. **왼쪽 고유벡터 $\mathbf{U}$는 $\mathbf{A} \mathbf{A}^T$의 고유벡터로부터 얻어집니다.
3. **오른쪽 고유벡터 $\mathbf{V}$**는 $\mathbf{A}^T \mathbf{A}$의 고유벡터로부터 얻어집니다.

#### SVD의 주요 응용

1. **차원 축소**: 특이값이 큰 값에 해당하는 주요 성분을 선택하고, 작은 특이값에 해당하는 성분을 제거해 데이터의 차원을 줄일 수 있습니다. 차원 축소 후에도 데이터의 핵심 정보는 대체로 유지됩니다.
2. **잡음 제거**: 데이터 행렬에서 특이값이 작은 성분들은 주로 잡음에 해당하므로 이를 제거하면 잡음이 줄어든 데이터 복원이 가능합니다.
3. **데이터 압축**: 데이터의 특이값 중 일부를 선택하여 저장하면 원본 데이터보다 용량을 크게 줄일 수 있습니다.
4. **선형 시스템 해석**: 과적합된 시스템이나 과소적합된 시스템을 해결할 수 있으며, 특히 최소 제곱해를 계산할 때 유용합니다.

#### SVD와 차원 축소

특이값 분해는 Principal Component Analysis(PCA)와 밀접한 관계가 있습니다. 차원 축소 관점에서 SVD는 행렬의 정보를 주요 성분으로 압축하여, 계산 비용과 데이터 용량을 줄이면서도 정보 손실을 최소화하는 효과를 제공합니다.


$\lambda$의 $\vec u_i$ 벡터에서 $\vec u_i$라는 것도 어떤 MAtrix($A^{\top}A$)의 Eigen Matrix였다.

`6p.`
앞에서의 $\vec u_i$는 Transformed 된 벡터이므로 unit vector가 아니지만,
여기서부터는 $\vec u_i, \vec v_i$를 unit vector라고 보고 증명을 보자.

`7p.`
$U, V$를 만들 수 있다.
각각의 Matrix의 Component는 $k$ 차원 space의 한 축을 담당하고 있는 벡터들이다.

`8p.`
$\vec e_i$ i번째 열을 뽑아내는 것
$\lambda_ㅑ$를 넣어준다.
$A\vec v_i$가 정의에 의해서 $\sqrt{\lambda_i}\vec u_i$가 된다.
$U^{\top} : \vec u_1^{\top}, \vec u_2^{\top}, \vec u_3^{\top}...$ (서로 Orthogonal하다.)

`9p.`
-> Singular Value를 포함하고 있는 Singular Value Matrix가 나온다.
이것이 Singlular Value Decomposition이다.

`10p.`
전체 과정의 시각화
$$\begin{align}A =& U\Sigma V^{\top} \\ =& \Sigma^k_{i = 1}\sigma_iu_iv_i^{\top} \\ =& \Sigma^k_{i = 1}\Sigma^k_{j = 1}u_i\sigma_jv_j^{\top}\end{align}$$(아차피 나머진 다 0이 돼서 마지막 줄로 표현하지 않아도 상관 없음)
-> $A$라는 Matrix가 3개로 쪼개지는 것을 볼 수 있다.

`11p.`
n차원 위의 한 점을 eigen vector들의 linear combination으로 표현할 수 있고 
이 linear combination을 target point $\vec y$에도 그대로 적용할 수 있다.

`12p.`
이를 그림으로 살펴보면 다음과 같다.
헷갈리면 이 그림을 먼저 보고 증명을 시작하는 것도 나쁘지 않을 것 같다.
그럼 이 Decomposition을 하면 무엇이 간단해지는가?
Coordinate이 최종적으로 그대로 옮겨지면서 Interpretation이 간단해진다. (?)

`13p.`
$V: A^{\top}A$의 고유벡터들
$U:$ 원래 구해놨던 eigen vector에 $A$를 곱하고 Normalization 한다.

`14p.`
Singular Matrix를 굉장히 다양한 형태로 Decomposition할 수 있으며,
앞선 예는 Full SVD이다.
Compact SVD: 실제로 python에서 Data를 저장하는 형식

`15p.`
-> 단순히 Projection과 Scaling 만으로 $A\vec x = \vec b$ 문제를 풀 수 있다!

`16p.`
Short Matrix는 특정 해를 구하기 어렵다.
이에 관해서 Short/Tall 상관없이 통합된 식을 만들어보면 강의자료의 식과 같다는 것이다. (SVD 사용)
silent camera 사진 참고

`18p.`
A가 Square Matrix:
A가 Tall Matrix(Overdetermined):
A가 Wide Matrix(Underdetermined): 
결국 모든 경우에 Pseudo Inverse로 풀 수 있구나 정도로 이해하고 증명을 보면된다.

`19p.`
/ *Rank-k Approximations*
주어진 행렬을 더 작은 순위의 근사 행렬로 표현하는 방법으로, 대케 데이터 압축이나 노이즈 감소와 같은 문제를 해결하기 위해 사용되며 SVD를 기반으로 한 방법이 널리 사용된다.

`20p.`
Rank-k Approximations에 대한 증명
Eckart-Young Theorem
이걸 가지고 영상처리에도 쓰고, 그래픽스에도 쓰고 한다.

`21p.`
8%의 공간만으로 이미지를 얼추 표현할 수 있다. (노이즈가 있긴 하지만)

`22p.`
Frobenius Norm: 
Induced Norm: Singular Value의 최대값이 Strach가 제일 많이된 길이이다.
Condition Number: SVD한 다음에 Comdition을 계산했을 때 Ax=b의 Condition number를 $\sigma$의 비율로 나타낼 수 있다.
-> 앞에서 배운 것들이 SVD와 연결되는 것을 알 수 잇다.

`23p.`
/ *Priciple Component Analysis*
데이터의 차원을 줄이기 위한 통계적 기법으로, 데이터 내의 중요한 패턴이나 변동성을 최대한 보존하면서 데이터를 더 간단한 구조로 변환하는 데 사용된다.
데이터의 주성분을 추출하여 차원 축소와 데이터 압축에 유용하며, 잡음을 제거하거나 시각화할 때 널리 활용된다.

`24p.`
임의의 Orthogonal Matrix로 길이를 제나
원래 vector의 길이를 재나 똑같다.

`25p.`

